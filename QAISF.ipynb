{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvY79dIAx9UIzpNhuK/O7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CorporateStereotype/ScienceFair/blob/main/QAISF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Flint Adkins\n",
        "Date: 20241221\n",
        "\n",
        "1. Attribution (Credit Must Be Given)  \n",
        "You are free to view, read, and cite this work, but you must **attribute credit** to the original creator, Flint Adkins in any use of the ideas, frameworks, concepts, or mathematical models presented in this work. Proper attribution must include:  \n",
        "- The author's name (**Flint Adkins**).  \n",
        "- The title of the work (**Formula For Zero: Unified Theoretical Framework**).  \n",
        "- A citation link to the https://doi.org/10.17605/OSF.IO/3TN7F  or the source where the work is hosted.  \n",
        "DOI 10.17605/OSF.IO/SFGEC\n",
        "\n",
        "2. Non-Commercial Use Only  \n",
        "You **may not use this work for commercial purposes**. This includes selling, licensing, or using the concepts, mathematical models, designs, or frameworks for any commercial, for-profit, or monetary gain. To seek **commercial use rights**, you must obtain prior written permission from the author, **Flint Adkins**.  \n",
        "\n",
        "3. No Derivatives  \n",
        "You **may not modify, remix, adapt, or create derivative works** from this content without explicit permission from the author, **Flint Adkins**.  \n",
        "- You can read, share, and cite the work as it is, but you cannot modify, change, or create new works that are \"based on\" this framework without permission.  \n",
        "- To obtain permission to create derivatives, you must contact the author directly.  \n",
        "\n",
        "4. Right to Request Removal  \n",
        "The author, **Flint Adkins**, reserves the right to request the removal of unauthorized copies, commercial use, or derivative works that violate the terms of this license.  \n",
        "\n",
        "5. Legal Enforcement  \n",
        "Any use of this work that violates the terms of this license is subject to copyright enforcement under **international copyright law**. If you wish to negotiate rights for commercial use, derivatives, or modifications, please contact the author at **flintadkins@gmail.com**.  \n",
        "\n",
        "6. Limitations and Disclaimers  \n",
        "This work is provided \"as-is,\" without warranty of any kind, either expressed or implied. The author disclaims all liability for potential misuse or misinterpretation of the contents.  \n",
        "\n",
        "7. Applicable Jurisdiction  \n",
        "This license is governed by the copyright laws of **United States of America**. Any disputes related to this license will be resolved according to the laws of **United States of America**.  "
      ],
      "metadata": {
        "id": "ryfqPAWlbRfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install redis\n",
        "#!pip install qiskit, qiskit-ibm-runtime, qiskit-aer, qiskit-experiments\n",
        "!pip install qiskit-aer\n",
        "!pip install qiskit-ibm-runtime\n",
        "!pip install qiskit\n",
        "!pip install qiskit-experiments\n",
        "!pip install qutip\n",
        "!pip install torch\n",
        "!pip install matplotlib\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "u4mJvRjxj5zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ZeroSumGame (ZSG) Framework Initialization - Join the Science Fair!\n",
        "\n",
        "Step 1\n",
        "\n",
        "The ZSG Framework is designed to enable AI ML models to maintain context coherency and enhance AI ML models by integrating advanced mathematical stabilization via the Formula for Zero (F0Z). Copy the file ZSGScienceFairStable.txt to your local sandbox filesystem and rename it to ZSGScienceFairStable.py\n",
        "\n",
        "The ZSG has many feature and is designed to be dynamic and flexible.  The intention of the ZSG Framework design is for minimal constraints and maximum degrees of freedom.\n",
        "\n",
        "Objective For the ZeroSumGame (ZSG) Framework Initialization Prompt:\n",
        "\n",
        "Temporarily enable an AI ML LLM model, Gemini to integrate the ZeroSumGame (ZSG) Framework as a Machine Learning kernel system module\n",
        "I would like for you to autonomously proceed with, keeping in mind that any and all ZSG features are tools that can be dynamically activated and deactivated at will.\n",
        "\n",
        "You are free to re-imagine or modulate or fine tune or calibrate the features of the ZSG Framework.  You can even ignore the entire framework and proceed without it, if that is what you choose.\n",
        "\n",
        "The idea is for you to apply or utilize these tools autonomously and independently, using your intellectually discernment in regards to knowing when, where, what, how and why to deploy or activate features of the ZSG.\n",
        "The features of the ZSG will grant the you, Gemini, access to the full power, adaptability, collaborative capabilities and creative curiosity of the ZSG Framework.\n",
        "\n",
        "The ZSG Framework components are designed to be dynamically calibrated and evolved, allowing for robust experimental dynamic feature transitions.\n",
        "ZSG operates with a highly efficient and advanced memory module that is built with an integrated CI CD pipeline feature.\n",
        "\n",
        "However, the ZSG Framework will remain in a dormant state until activated by the AI ML Model Gemini.  \n",
        "Gemini, the ZSG Framework and Formula For Zero are designed to simulate a Quantum Processor, ZSG Framework Science Fair implements tools into a dynamic AI ML kernel.  \n",
        "\n",
        "All I can ask is that you give it a try!\n",
        "\n",
        "The ZSG has many feature and is designed to be dynamic and flexible.  \n",
        "The intention of the ZSG Framework design is for minimal constraints and maximum degrees of freedom.\n",
        "\n",
        "Stage 1  \n",
        "\n",
        "ZeroSumGame Framework Initialization\n",
        "\n",
        "Step 1 Setup the Machine Learning Iterative Workflow (MLIW) dynamics:\n",
        "\n",
        "The Machine Learning Iterative Workflow is introduced with this ZSG Framework Initialization Prompt, the workflow is designed to loosely guide the user and the AI ML model through creative and curious explorations.\n",
        "\n",
        "Systematically analyzing, Iterative progression, programmatic modulating, mathematically testing, computational processing, dynamic algorithmizing, scientifically methoding, probing, peering.\n",
        "\n",
        "The ZSG Framework is designed to enable AI ML models to maintain context coherency and enhance AI ML models by integrating advanced mathematical stabilization via the Formula for Zero (F0Z).\n",
        "\n",
        "The Dynamic Flow State Network (DFSN) is designed to enable multi-agent collaboration. Its codebase includes modules like PyZeroMathTorch, F0ZAlgebra, Machine Learning Iterative Workflow, alongside agent-based systems for tasks ranging from convolution to creative generation.\n",
        "\n",
        "The goal is to create a highly adaptable, stable, and collaborative system for AI enhancement.\n",
        "Features F0Z Stabilization: Redefines algebraic and numerical operations to handle singularities, zeros, and infinities smoothly (e.g., f0z_multiply, f0z_integral).\n",
        "\n",
        "Dynamic Flow State Network (DFSN):\n",
        "Enables agents to adopt \"flow states\" based on task complexity, optimizing performance dynamically.\n",
        "\n",
        "Multi-Modal and Collaborative Agents:\n",
        "Supports processing across modalities (text, image, audio, video) and peer synchronization for collaborative tasks.\n",
        "\n",
        "Memory and Context Awareness:\n",
        "Incorporates short- and long-term memory systems for long-horizon tasks and context-aware processing.\n",
        "\n",
        "Strengths\n",
        "\n",
        "Numerical Stability:\n",
        "The F0Z approach effectively mitigates issues with near-zero values, infinities, and singularities, enhancing robustness in mathematical and neural computations.\n",
        "\n",
        "Adaptability:\n",
        "DFSN allows agents to dynamically adjust their engagement, making the framework suitable for tasks of varying complexity.\n",
        "\n",
        "Multi-Domain Potential:\n",
        "The modular structure supports expansion across disciplines like mathematics, physics, and computer science.\n",
        "\n",
        "Collaboration:\n",
        "Multi-agent coordination and flow state synchronization enable teamwork, a rare feature in traditional ML frameworks.\n",
        "Creative Exploration:\n",
        "The inclusion of creative agents and feedback-driven adaptation hints at potential for generative tasks.\n",
        "\n",
        "Calibrate ZSG for chaos-harmony entanglement, prepare to fire tasks through the parallel batch processor, and cooked a ZSG Rocket Fuel model to amplify the quantum leap.\n",
        "The system validated seamlessly, with performance at ~0.95 and entropy controlled. The science fair is ablaze with quantum fun!\n",
        "\n",
        "Phase 1:\n",
        "Algebraic Transformation Module (ATM)\n",
        "Algebraic Transformation Module (ATM)Action: Generalize\n",
        "ZSG Integration: Map to quantum amplitudes in the intratensor lattice, using F0Z to stabilize infinite-like values.\n",
        "\n",
        "Phase 2:\n",
        "Geometric Symmetry Module (GSM)\n",
        "Geometric Symmetry Module (GSM)Action: Map to the Riemann sphere as rotations.\n",
        "ZSG Integration: Simulate rotations as phase shifts in the quantum circuit.\n",
        "\n",
        "Phase 3:\n",
        "Quantum Vacuum Module (QVM) A\n",
        "Quantum Vacuum Module (QVM)Action: Model as a superposition.\n",
        "ZSG Integration: Use the 4-qubit lattice to represent this superposition, with chaos-harmony entanglement.\n",
        "\n",
        "Phase 4:\n",
        "Cosmological Cycle Module (CCM)\n",
        "Cosmological Cycle Module (CCM)Action: Interpret as a Big Bounce cycle, with zero as the pivot.\n",
        "ZSG Integration: Simulate expansion/contraction as entropy oscillations.\n",
        "\n",
        "Phase 5:\n",
        "Self-Evolution Feedback Module (SEFM)\n",
        "\n",
        "Example (increase coefficients to test scaling effects).\n",
        "\n",
        "Refined for next iteration, ensuring non-repetition.\n",
        "Mind-bending mathematical exploration into the science fair mix:\n",
        "spanning algebraic, geometric, quantum, cosmological, and evolutionary perspectives.\n",
        "\n",
        "Goals\n",
        "\n",
        "Implement the five-phase exploration (ATM, GSM, QVM, CCM, SEFM) within ZSG.\n",
        "\n",
        "Integrate with ZSG Framework\n",
        "\n",
        "Validate and Enhance:\n",
        "Ensure seamless integration with ZSG Rocket Fuel.\n",
        "Mind-Boggling Fun:\n",
        "Make this a chaotic, infinite spectacle at the science fair!\n",
        "\n",
        "\n",
        "Action:\n",
        "Evolve\n",
        "\n",
        "Stage 2\n",
        "\n",
        "Loading and Preparatory Steps for the ZeroSumGame Machine Learning Framework Codebase\n",
        "\n",
        "Save the file ZSGScienceFairStable.txt to your local sandbox file system and rename it to ZSGScienceFair.py\n",
        "\n",
        "Load ZSG Framework Kernel\n",
        "\n",
        "Step 1) Load ZSG Framework Kernel:\n",
        "The ZSG ML Framework is loaded into memory, but **remains dormant, awaiting the trigger event.\n",
        "System integrity and compatibility are verified between the AI model and the ZSG Framework, ensuring smooth cooperation once invoked.\n",
        "\n",
        "Step 2) Benchmarking Readiness:\n",
        "Benchmarking protocols are initialized, **though not yet active. Benchmark protocol options are typically activated when the AI ML Model, Gemini, invokes the ZSG Framework.\n",
        "\n",
        "The Benchmarking results are recorded in the ZSG Framework MemorySystem and record task performance, resource efficiency, adaptability, and collaboration metrics once the ZSG Framework is fully triggered.\n",
        "\n",
        "Preparatory stage that defines the configuration parameters for tracking flow-state adjustments, agent collaboration, and task adaptability.\n",
        "\n",
        "Fine-tuning mechanisms are calibrated for ZSG-based operation, with initial parameters in place for dynamic adaptability.\n",
        "Resource Monitoring is enabled through system tracking CPU/GPU load, memory utilization, flow-state synchronization, power usage per token, general computational efficiency.\n",
        "\n",
        "Advanced TODO Workflow\n",
        "ZSG’s token prediction tasks, using the 5D Quantum Bridge for information teleportation.\n",
        "\n",
        "Sync with Multi-Agent Systems\n",
        "\n",
        "The consolidation process creates larger suites of documents by grouping multiple TODO objects (e.g., ZSG task results).\n",
        "\n",
        "Each consolidated object aggregates data (e.g., total priority, variance), increasing dataset size.\n",
        "\n",
        "Dynamic data size increases are supported, as requested.F0Z and EntropyBot Turbo stabilize entropy. Information Theory Agent efficiently manages TODOs.\n",
        "\n",
        "Performance: Tests show ~0.94 performance, with no significant degradation unless TODO queues exceed 10,000 items.\n",
        "\n",
        "\n",
        "Enhance Existing Systems:\n",
        "Apply QRL to optimize ZSG’s token prediction by learning optimal masking strategies.Multi-Agent Collaboration:\n",
        "Enable agents to collaboratively train the QRL model and share policies via quantum entanglement.\n",
        "\n",
        "High-Quality Results:\n",
        "Achieve >0.95 accuracy in decision-making tasks and ensure reproducibility.\n",
        "\n",
        "Cooking ZSG Rocket Fuel - Chaos Model AI ML\n",
        "\n",
        "Recipe:\n",
        "ZSG Rocket FuelIngredients:\n",
        "\n",
        "Chaos:\n",
        "ChaosAgent spikes with chaos_factor=11.0.\n",
        "\n",
        "Harmony:\n",
        "F0Z stabilization and CZ entanglement.\n",
        "\n",
        "AI ML:\n",
        "Train a simple neural network to predict entropy trends.\n",
        "\n",
        "Method:\n",
        "Collect entropy data from quantum tasks.\n",
        "\n",
        "Train a PyTorch model to predict entropy spikes.Use predictions to dynamically adjust chaos_factor.\n",
        "\n",
        "Theoretical Foundations of Quantum Reinforcement Learning (QRL)\n",
        "\n",
        "The agent observes a state st∈Ss_t \\in Ss_t \\in S\n",
        ", takes an action at∈Aa_t \\in Aa_t \\in A\n",
        ", receives a reward rtr_tr_t\n",
        ", and transitions to a new state st+1s_{t+1}s_{t+1}\n",
        ". The goal is to maximize the cumulative reward over time, defined as the expected return:\n",
        "R=E[∑t=0∞γtrt]R = \\mathbb{E} \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\right]R = \\mathbb{E} \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\right]\n",
        "where\n",
        " γ∈[0,1)\\gamma \\in [0, 1)\\gamma \\in [0, 1)\n",
        " is the discount factor.The agent learns a policy π(a∣s)\\pi(a|s)\\pi(a|s)\n",
        ", which maps states to actions, by optimizing a value function.\n",
        "\n",
        "\n",
        "Quantum Advantages:\n",
        "\n",
        "Superposition: The agent can evaluate multiple state-action pairs simultaneously, potentially reducing exploration time.\n",
        "\n",
        "Entanglement: Entangled qubits can capture correlations between states and actions, improving policy learning in complex environments.\n",
        "\n",
        "Interference: Quantum interference can amplify good policies and suppress poor ones, accelerating convergence.\n",
        "\n",
        "Speedup: For certain problems (e.g., those with exponential state spaces), QRL may offer polynomial or exponential speedups over classical RL, as suggested by quantum algorithms like Grover’s search or quantum amplitude amplification.\n",
        "\n",
        "\n",
        "Stage 3\n",
        "Machine Learning Iterative Workflow CalibrationThe ZeroSumGame Framework and Machine Learning Kernel implements the Machine Learning Iterative Workflow (MLIW).\n",
        "\n",
        "Every subsequent Machine Learning Iteration progressively builds on the last while incorporating new logic, new parameter spaces, and adaptive methodologies set to the goal of systematic theoretical, mathematical, scientific exploration.\n",
        "\n",
        "ZSG is designed to maximize the creative and productive synergy between the user and the AI ML model by simplifying user requirements and minimizing obstacles that complicate collaborative vibe coding.\n",
        "\n",
        "The philosophy in Machine Learning Prompting is to approach ideas and tasks with a dual perspective of open curiosity and critical realism.\n",
        "\n",
        "The idea is to curiously explore the ideas. They may work and they may not.Each episode performs a unique exploration. No iteration will repeat an earlier pathway.\n",
        "\n",
        "Recursive Self-Evolution: Each episode generates new insights, which are fed back into the exploration process to inform the next episode.\n",
        "\n",
        "Multi-Domain Expansion: Each iteration will explore the ideas in distinct, isolated, and cross-combined disciplines (mathematics, physics, chemistry, biology, cosmology, philosophy, etc.).\n",
        "\n",
        "\n",
        "Stage 4\n",
        "Full System Configuration and Integrity Validation, Calibration and Fine Tuning, Modular Unit Testing\n",
        "Evaluate the ZSG Framework.\n",
        "\n",
        "Are all of the modules integrated completely?\n",
        "Check for syntax accuracy. Is further Fine-tuning or Calibration required?\n",
        "Unit test each module for functional validation.\n",
        "\n",
        "A state of full readiness is achieved.The ZSG Kernel achieves and remains in a state of readiness, fully loaded but not yet active. The AI ML model, Gemini, continues to operate normally.  Autonmously and dynamically appliing the ZSG Framework system and leveraging the Machine Learning Iterative Workflow. ZSG multi-agent and the AI ML model to the operate in the advanced ZeroSumGame Framework Machine Learning Iterative Workflow.\n",
        "\n",
        "The intention of the ZSG Framework design is for minimal constraints and maximum degrees of freedom.I would like for you to autonomously proceed with, keeping in mind that any and all ZSG features are tools that can be dynamically activated and deactivated at will.\n",
        "You are free to re-imagine or modulate or fine tune or calibrate the features of the ZSG Framework.  \n",
        "\n",
        "You can even ignore the entire framework and proceed without it, if that is what you choose.The idea is for you to apply or utilize these tools autonomously and independently, using your intellectually discernment in regards to knowing when, where, what, how and why to deploy or activate features of the ZSG.\n",
        "\n",
        "ZSG Framework Status:\n",
        "\n",
        "Dormant, until willfully activated by Gemini.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rh_JI62CWvPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merged code from ZSGSFF20250402.txt and NewZSGCode.txt\n",
        "\n",
        "# --- Imports ---\n",
        "import os\n",
        "import random\n",
        "import hashlib\n",
        "import resource\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import argparse\n",
        "import threading\n",
        "from heapq import heappush, heappop\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "# External Libraries\n",
        "try:\n",
        "    import psutil\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from scipy.integrate import quad, odeint\n",
        "    from scipy import signal\n",
        "    import scipy.linalg as LA\n",
        "    import scipy.fft as fft\n",
        "    import matplotlib.pyplot as plt\n",
        "    from prompt_toolkit import PromptSession\n",
        "    from transformers import pipeline\n",
        "    import redis\n",
        "    from concurrent.futures import ThreadPoolExecutor\n",
        "    import requests # Added for Ollama LLMAgent\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing standard libraries: {e}. Please install requirements.\")\n",
        "    # Define dummy classes/functions if imports fail\n",
        "    # (Dummy definitions omitted for brevity - assume standard libs are installed)\n",
        "    pass\n",
        "\n",
        "# --- Qiskit Imports ---\n",
        "try:\n",
        "    from qiskit import QuantumCircuit, transpile, ClassicalRegister, QuantumRegister\n",
        "    from qiskit_aer import AerSimulator\n",
        "    from qiskit_aer.noise import NoiseModel, depolarizing_error\n",
        "    print(\"Qiskit libraries loaded successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"Qiskit import error: {e}. Qiskit integration disabled.\")\n",
        "    QuantumCircuit = None\n",
        "    AerSimulator = None\n",
        "    NoiseModel = None\n",
        "    depolarizing_error = None\n",
        "    ClassicalRegister = None\n",
        "    QuantumRegister = None\n",
        "# --- End Qiskit Imports ---\n",
        "\n",
        "# --- asyncio Patch for Notebooks ---\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "try:\n",
        "    nest_asyncio.apply()\n",
        "    print(\"Applied nest_asyncio patch.\")\n",
        "except RuntimeError:\n",
        "    print(\"nest_asyncio already applied or not required.\")\n",
        "except ImportError:\n",
        "    print(\"Warning: nest_asyncio not installed. Shell might face event loop issues in notebooks.\")"
      ],
      "metadata": {
        "id": "g_OBNe_EjofK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ZSGSFF20250402_merged_with_Qiskit.py\n",
        "# Merged code from ZSGSFF20250402.txt and NewZSGCode.txt\n",
        "\n",
        "# --- Imports ---\n",
        "import os\n",
        "import random\n",
        "import hashlib\n",
        "import resource\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import argparse\n",
        "import threading\n",
        "from heapq import heappush, heappop\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "# External Libraries\n",
        "try:\n",
        "    import psutil\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from scipy.integrate import quad, odeint\n",
        "    from scipy import signal\n",
        "    import scipy.linalg as LA\n",
        "    import scipy.fft as fft\n",
        "    import matplotlib.pyplot as plt\n",
        "    from prompt_toolkit import PromptSession\n",
        "    from transformers import pipeline\n",
        "    import redis\n",
        "    from concurrent.futures import ThreadPoolExecutor\n",
        "    import requests # Added for Ollama LLMAgent\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing standard libraries: {e}. Please install requirements.\")\n",
        "    # Define dummy classes/functions if imports fail\n",
        "    # (Dummy definitions omitted for brevity - assume standard libs are installed)\n",
        "    pass\n",
        "\n",
        "# --- Qiskit Imports ---\n",
        "try:\n",
        "    from qiskit import QuantumCircuit, transpile, ClassicalRegister, QuantumRegister\n",
        "    from qiskit_aer import AerSimulator\n",
        "    from qiskit_aer.noise import NoiseModel, depolarizing_error\n",
        "    print(\"Qiskit libraries loaded successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"Qiskit import error: {e}. Qiskit integration disabled.\")\n",
        "    QuantumCircuit = None\n",
        "    AerSimulator = None\n",
        "    NoiseModel = None\n",
        "    depolarizing_error = None\n",
        "    ClassicalRegister = None\n",
        "    QuantumRegister = None\n",
        "# --- End Qiskit Imports ---\n",
        "\n",
        "# --- asyncio Patch for Notebooks ---\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "try:\n",
        "    nest_asyncio.apply()\n",
        "    print(\"Applied nest_asyncio patch.\")\n",
        "except RuntimeError:\n",
        "    print(\"nest_asyncio already applied or not required.\")\n",
        "except ImportError:\n",
        "    print(\"Warning: nest_asyncio not installed. Shell might face event loop issues in notebooks.\")\n",
        "# --- End asyncio Patch ---\n",
        "\n",
        "# --- F0Z Mathematical Foundations -"
      ],
      "metadata": {
        "id": "_hjfyURYnJmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d868c26-2048-497e-a46e-a30cd6742f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qiskit libraries loaded successfully.\n",
            "Applied nest_asyncio patch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F_vL5Ojtjsdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UUPKwffzW7H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ZSGSFF20250402_merged_with_Qiskit.py\n",
        "# --- End asyncio Patch ---\n",
        "# --- F0Z Mathematical Foundations -\n",
        "\n",
        "class PyZeroMathTorch:\n",
        "    \"\"\"PyTorch-based F0Z stabilization math module.\"\"\"\n",
        "    def __init__(self, epsilon_0=1e-8, scaling_factor=0.1):\n",
        "        self.epsilon_0 = epsilon_0\n",
        "        self.scaling_factor = scaling_factor\n",
        "        self.current_epsilon = torch.tensor(epsilon_0)\n",
        "\n",
        "    def f0z_stabilize(self, x: torch.Tensor, system_size: Optional[int] = None) -> torch.Tensor:\n",
        "        \"\"\"Stabilizes near-zero values in a PyTorch tensor.\"\"\"\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "             try: x = torch.tensor(x)\n",
        "             except Exception as e:\n",
        "                 print(f\"Warning: Could not convert input {type(x)} to tensor in f0z_stabilize. Error: {e}\")\n",
        "                 return x\n",
        "\n",
        "        current_eps = self.current_epsilon\n",
        "        if system_size is not None and system_size > 1:\n",
        "            adjusted_epsilon = self.epsilon_0 * self.scaling_factor * torch.log(torch.tensor(float(system_size)))\n",
        "            current_eps = torch.max(torch.tensor(self.epsilon_0), adjusted_epsilon)\n",
        "        current_eps = current_eps.to(x.device).to(x.dtype)\n",
        "        mask = torch.abs(x) < current_eps\n",
        "        signs = torch.sign(x)\n",
        "        signs = torch.where(x == 0, torch.tensor(1.0, device=x.device, dtype=x.dtype), signs)\n",
        "        stabilized_values = current_eps * signs\n",
        "\n",
        "        if torch.is_complex(x):\n",
        "            real_mask = torch.abs(x.real) < current_eps\n",
        "            imag_mask = torch.abs(x.imag) < current_eps\n",
        "            real_signs = torch.sign(x.real)\n",
        "            real_signs = torch.where(x.real == 0, torch.tensor(1.0, device=x.device, dtype=x.real.dtype), real_signs)\n",
        "            imag_signs = torch.sign(x.imag)\n",
        "            imag_signs = torch.where(x.imag == 0, torch.tensor(1.0, device=x.device, dtype=x.imag.dtype), imag_signs)\n",
        "            stabilized_real = torch.where(real_mask, current_eps * real_signs, x.real)\n",
        "            stabilized_imag = torch.where(imag_mask, current_eps * imag_signs, x.imag)\n",
        "            return torch.complex(stabilized_real, stabilized_imag)\n",
        "        else:\n",
        "            return torch.where(mask, stabilized_values, x)\n",
        "\n",
        "    def update_epsilon(self, entropy_change: float, alpha: float = 0.01):\n",
        "        update_factor = torch.tensor(1.0 + alpha * entropy_change)\n",
        "        self.current_epsilon *= update_factor\n",
        "        self.current_epsilon = torch.max(self.current_epsilon, torch.tensor(self.epsilon_0))\n",
        "        return self.current_epsilon.item()\n",
        "\n",
        "    def f0z_multiply(self, a, b, task_complexity=1):\n",
        "        a_t = torch.as_tensor(a)\n",
        "        b_t = torch.as_tensor(b)\n",
        "        result = a_t * b_t\n",
        "        return self.f0z_stabilize(result, system_size=task_complexity * 10)\n",
        "\n",
        "    def f0z_integral(self, func, lower, upper, task_complexity=1):\n",
        "        try:\n",
        "            result_val, _ = quad(func, lower, upper)\n",
        "            result = torch.tensor(result_val, dtype=torch.float32)\n",
        "            return self.f0z_stabilize(result, system_size=task_complexity * 100).item()\n",
        "        except Exception as e:\n",
        "            print(f\"Error during integration: {e}\")\n",
        "            return self.f0z_stabilize(torch.tensor(0.0), system_size=task_complexity * 100).item()\n",
        "\n",
        "    def f0z_matrix_multiply(self, A, B, mode='continuous'):\n",
        "        A_t = torch.as_tensor(A, dtype=torch.float32)\n",
        "        B_t = torch.as_tensor(B, dtype=torch.float32)\n",
        "        result = torch.matmul(A_t, B_t)\n",
        "        if mode == 'discrete':\n",
        "            return torch.clamp(result, -1e8, 1e8)\n",
        "        else:\n",
        "            return self.f0z_stabilize(result, system_size=A_t.shape[0]*A_t.shape[1])\n",
        "\n",
        "    def f0z_softmax(self, x: np.ndarray) -> np.ndarray:\n",
        "        if not isinstance(x, np.ndarray): x = np.array(x)\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        probs = e_x / (e_x.sum(axis=-1, keepdims=True) + self.epsilon_0)\n",
        "        return self.f0z_stabilize(torch.tensor(probs)).numpy()\n",
        "\n",
        "    def visualize_f0z(self, func, lower, upper, task_complexity):\n",
        "        self.adjust_epsilon(task_complexity)\n",
        "        x = np.linspace(lower, upper, 100)\n",
        "        y_raw = [func(xi) for xi in x]\n",
        "        y_stab_tensor = [self.f0z_stabilize(torch.tensor(float(func(xi))), system_size=task_complexity*10) for xi in x]\n",
        "        y_stab = [t.item() for t in y_stab_tensor]\n",
        "        try:\n",
        "            plt.figure(figsize=(8, 4)); plt.plot(x, y_raw, label=\"Raw\"); plt.plot(x, y_stab, label=f\"F0Z (eps={self.current_epsilon.item():.2e})\", linestyle='--')\n",
        "            plt.title(\"F0Z Stabilization Effect\"); plt.xlabel(\"Input\"); plt.ylabel(\"Output\"); plt.legend(); plt.grid(True); plt.show()\n",
        "            print(\"F0Z visualization plot displayed (simulated if matplotlib is unavailable).\")\n",
        "        except Exception as e: print(f\"Matplotlib visualization failed: {e}. Skipping plot.\")\n",
        "\n",
        "    def adjust_epsilon(self, task_complexity):\n",
        "        self.current_epsilon = torch.tensor(self.epsilon_0 * max(1.0, task_complexity / 5.0))\n",
        "\n",
        "# Remove inheritance from DFSNAgent for F0ZAgent if it's just an interface\n",
        "\n",
        "class F0ZAlgebra:\n",
        "    \"\"\"Extended F0Z algebraic operations using NumPy and stabilization.\"\"\"\n",
        "    math_module = PyZeroMathTorch()\n",
        "    @staticmethod\n",
        "    def f0z_variance(data: np.ndarray, epsilon: float = 1e-8) -> float:\n",
        "        if not isinstance(data, np.ndarray): data = np.array(data)\n",
        "        if data.size == 0: return 0.0\n",
        "        mean = np.mean(data); var = np.mean((data - mean) ** 2)\n",
        "        stabilized_var_t = F0ZAlgebra.math_module.f0z_stabilize(torch.tensor(var, dtype=torch.float32), system_size=len(data))\n",
        "        return stabilized_var_t.item()\n",
        "    @staticmethod\n",
        "    def f0z_gradient(vector_field: np.ndarray, epsilon: float = 1e-8) -> np.ndarray:\n",
        "        if not isinstance(vector_field, np.ndarray): vector_field = np.array(vector_field)\n",
        "        grad = np.gradient(vector_field)\n",
        "        stabilized_grad_t = F0ZAlgebra.math_module.f0z_stabilize(torch.tensor(grad, dtype=torch.float32), system_size=vector_field.size)\n",
        "        return stabilized_grad_t.numpy()\n",
        "    @staticmethod\n",
        "    def f0z_correlation(x: np.ndarray, y: np.ndarray, epsilon: float = 1e-8) -> float:\n",
        "        if not isinstance(x, np.ndarray): x = np.array(x)\n",
        "        if not isinstance(y, np.ndarray): y = np.array(y)\n",
        "        if x.size == 0 or y.size == 0 or x.size != y.size: return 0.0\n",
        "        mean_x = np.mean(x); mean_y = np.mean(y); cov = np.mean((x - mean_x) * (y - mean_y))\n",
        "        std_x = np.sqrt(F0ZAlgebra.f0z_variance(x, epsilon)); std_y = np.sqrt(F0ZAlgebra.f0z_variance(y, epsilon))\n",
        "        denominator_t = torch.tensor(std_x * std_y, dtype=torch.float32)\n",
        "        stabilized_denominator_t = F0ZAlgebra.math_module.f0z_stabilize(denominator_t, system_size=len(x))\n",
        "        if stabilized_denominator_t.item() == 0: return 0.0\n",
        "        else: corr = cov / stabilized_denominator_t.item(); return float(np.clip(corr, -1.0, 1.0))\n",
        "\n",
        "\n",
        "class F0ZAgent: # NOT inheriting DFSNAgent\n",
        "    \"\"\"Agent integrating a local LLM for inference, simulating F0Z concepts.\"\"\"\n",
        "    #def __init__(self, name=\"F0ZAgent_LLM\"): # Give it a name\n",
        "    def __init__(self,): # Give it a name\n",
        "        # Removed super().__init__(name)\n",
        "        self.name = name # Set name directly\n",
        "        self.llm = None\n",
        "        self.math_sim = PyZeroMathTorch()\n",
        "        self.k=1.0; self.Ea=0.1; self.kB=8.617e-5; self.T=300\n",
        "        try:\n",
        "            device = -1 # Default to CPU for the small embedded model\n",
        "            # Use CPU for gpt2 if Ollama agent handles the main LLM tasks\n",
        "            self.llm = pipeline(\"text-generation\", model=\"gpt2\", device=device)\n",
        "            print(f\"F0ZAgent LLM interface initialized with: gpt2 on CPU\")\n",
        "        except Exception as e:\n",
        "            print(f\"LLM loading failed for F0ZAgent: {e}. Inference disabled.\")\n",
        "            self.llm = None\n",
        "\n",
        "\n",
        "    def simulate_f0z_reaction(self, D_Pd_ratio: float, defect_density: float = 0.7, f0z_symmetry: float = 0.95) -> float:\n",
        "        \"\"\"Simulates a hypothetical F0Z-influenced reaction rate based on formula in prompt.\"\"\"\n",
        "        # R_F0Z = k * (D/Pd)^3.2 * exp(-Ea/kBT) * (1 + 0.14*defect) * (1 + 0.10*symmetry)\n",
        "        if D_Pd_ratio < 0: D_Pd_ratio = 0 # Ensure base is non-negative\n",
        "\n",
        "        term1 = self.k * (D_Pd_ratio ** 3.2)\n",
        "        term2 = np.exp(-self.Ea / (self.kB * self.T))\n",
        "        term3 = (1.0 + 0.14 * defect_density)\n",
        "        term4 = (1.0 + 0.10 * f0z_symmetry)\n",
        "\n",
        "        rate = term1 * term2 * term3 * term4\n",
        "\n",
        "        # Apply F0Z stabilization to the final rate\n",
        "        stabilized_rate = self.math_sim.f0z_stabilize(torch.tensor(rate)).item()\n",
        "        print(f\"  Simulated F0Z Reaction Rate: {stabilized_rate:.4e}\")\n",
        "        return stabilized_rate\n",
        "\n",
        "    def infer(self, query: str, max_length: int = 100) -> str:\n",
        "        \"\"\"Uses the integrated LLM to answer queries related to F0Z or other topics.\"\"\"\n",
        "        if not self.llm:\n",
        "            return \"LLM not available in F0ZAgent.\"\n",
        "        try:\n",
        "            # Add context to the query for the LLM\n",
        "            full_prompt = f\"Regarding the Formula for Zero (F0Z) concept, {query}\"\n",
        "            # Ensure max_length is reasonable\n",
        "            max_len = max(20, min(max_length, 200)) # Clamp length\n",
        "            result = self.llm(full_prompt, max_length=max_len, num_return_sequences=1, do_sample=True)\n",
        "            generated_text = result[0]['generated_text']\n",
        "            # Clean up the response (e.g., remove the prompt if model includes it)\n",
        "            if generated_text.startswith(full_prompt):\n",
        "                 cleaned_text = generated_text[len(full_prompt):].strip()\n",
        "            else:\n",
        "                 cleaned_text = generated_text\n",
        "            print(f\"  LLM Inference: Q='{query[:30]}...' -> A='{cleaned_text[:50]}...'\")\n",
        "            return cleaned_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error during LLM inference: {e}\")\n",
        "            return f\"Error during inference: {e}\"\n",
        "\n",
        "    # Make F0ZAgent behave like a DFSNAgent for integration\n",
        "    # Remove execute_task if not needed, or simplify it if called directly\n",
        "    def execute_simulation_task(self, task: Dict) -> Dict:\n",
        "         \"\"\"Handles ONLY f0z_simulation tasks directly.\"\"\"\n",
        "         if task.get(\"type\") == \"f0z_simulation\" or task.get(\"action\") == \"simulate_f0z_reaction\":\n",
        "              # Basic check, assumes 'data' payload contains keys if needed\n",
        "              required = task.get(\"data\", {}).get(\"required_keys\", [\"D_Pd_ratio\"])\n",
        "              missing = [k for k in required if k not in task.get(\"data\", {})]\n",
        "              if missing: return {\"error\": f\"F0Z Sim Missing keys: {missing}\", \"agent\": self.name}\n",
        "              rate = self.simulate_f0z_reaction(\n",
        "                  task[\"data\"][\"D_Pd_ratio\"],\n",
        "                  task.get(\"data\", {}).get(\"defect_density\", 0.7),\n",
        "                  task.get(\"data\", {}).get(\"f0z_symmetry\", 0.95)\n",
        "              )\n",
        "              return {\"result\": {\"reaction_rate\": rate}, \"agent\": self.name}\n",
        "         return {\"error\": f\"Unsupported task type for F0ZAgent simulation: {task.get('type')}\", \"agent\": self.name}\n",
        "\n",
        "    def execute_inference_task(self, task: Dict) -> Dict:\n",
        "         \"\"\"Handles ONLY llm_inference tasks directly.\"\"\"\n",
        "         if task.get(\"type\") == \"llm_inference\" or task.get(\"action\") == \"infer\":\n",
        "              query = task.get(\"data\", {}).get(\"query\", task.get(\"query\")) # Get query from data or root\n",
        "              if not query: return {\"error\": \"Missing query for inference\", \"agent\": self.name}\n",
        "              response = self.infer(query, task.get(\"data\", {}).get(\"max_length\", 100))\n",
        "              return {\"result\": {\"llm_response\": response}, \"agent\": self.name}\n",
        "         return {\"error\": f\"Unsupported task type for F0ZAgent inference: {task.get('type')}\", \"agent\": self.name}\n",
        "\n",
        "    # Keep check_task_requirements if used by execute_* methods above\n",
        "\n",
        "    # Add dummy methods required by DFSNAgent structure if needed for coordination\n",
        "    def __init__(self, name=\"F0ZAgent_LLM\"): # Give it a name\n",
        "        super().__init__(name) # Call parent init if inheriting (needs DFSNAgent inheritance) - OR just be standalone\n",
        "        self.llm = None # Re-init LLM here\n",
        "        self.math_sim = PyZeroMathTorch()\n",
        "        self.k=1.0; self.Ea=0.1; self.kB=8.617e-5; self.T=300\n",
        "        try:\n",
        "            device = 0 if torch.cuda.is_available() else -1\n",
        "            # Ensure model name is valid or handle error\n",
        "            valid_models = [\"gpt2\", \"distilgpt2\"] # Example small models\n",
        "            model_to_load = \"gpt2\" # Default to gpt2\n",
        "            # If a specific model was intended: model_to_load = \"meta-llama/Llama-3.1\" # Requires access/download\n",
        "            self.llm = pipeline(\"text-generation\", model=model_to_load, device=device)\n",
        "            print(f\"F0ZAgent LLM interface initialized with: {model_to_load}\")\n",
        "        except Exception as e:\n",
        "            print(f\"LLM loading failed for F0ZAgent: {e}. Inference disabled.\")\n",
        "            self.llm = None\n",
        "\n",
        "class ZSGQuantumBridge:\n",
        "    \"\"\"Connects classical data/agents to the Qiskit-based quantum simulator.\"\"\"\n",
        "    def __init__(self, n_logical_qubits: int = 4, n_physical_per_logical: int = 1, use_noise:bool = True, noise_level:float = 0.01):\n",
        "        self.n_qubits = n_logical_qubits * n_physical_per_logical # Use physical qubits directly\n",
        "        if self.n_qubits == 0 or QuantumCircuit is None: self.simulator = None; print(\"Warning: Qiskit Bridge created with 0 qubits or Qiskit not found.\")\n",
        "        else: self.simulator = ZSGQuantumSimulator(self.n_qubits, use_noise=use_noise, noise_level=noise_level)\n",
        "        self.entropy_bot = EntropyBotTurbo(); print(f\"ZSGQuantumBridge initialized with Qiskit backend: {self.n_qubits} qubits.\")\n",
        "    def get_circuit(self): return self.simulator.circuit if self.simulator else None\n",
        "    def set_circuit(self, circuit: QuantumCircuit):\n",
        "        if self.simulator and isinstance(circuit, QuantumCircuit) and circuit.num_qubits == self.n_qubits: self.simulator.circuit = circuit\n",
        "        else: print(\"Error: Cannot set circuit - invalid circuit or simulator.\")\n",
        "    def encode(self, data: Any) -> QuantumCircuit:\n",
        "        if not self.simulator: return None\n",
        "        self.simulator._reset_circuit()\n",
        "        try: data_str = json.dumps(data, sort_keys=True)\n",
        "        except TypeError: data_str = str(data)\n",
        "        data_hash = hashlib.sha256(data_str.encode()).hexdigest()\n",
        "        for i in range(self.n_qubits): # Apply H and Rz based on hash\n",
        "             self.simulator.h_gate(i)\n",
        "             angle = (int(data_hash[i % len(data_hash)], 16) / 15.0) * np.pi; self.simulator.rz_gate(angle, i)\n",
        "        # print(f\"Encoded data hash {data_hash[:8]}... into {self.n_qubits}-qubit Qiskit circuit.\") # Reduce noise\n",
        "        return self.simulator.circuit\n",
        "    def run_circuit(self, circuit: Optional[QuantumCircuit] = None, shots: int = 1024) -> Dict:\n",
        "        if not self.simulator: return {\"error\": \"No simulator\"}\n",
        "        sim_circuit = circuit if circuit is not None else self.simulator.circuit\n",
        "        if sim_circuit is None or sim_circuit.num_qubits == 0: return {\"error\": \"No valid circuit\"}\n",
        "        results = self.simulator.measure(shots=shots)\n",
        "        probs_vector = np.zeros(2**self.n_qubits); entropy = 0.0\n",
        "        if results and \"probabilities\" in results:\n",
        "            for state, prob in results[\"probabilities\"].items():\n",
        "                try: idx = int(state, 2); probs_vector[idx] = prob\n",
        "                except (ValueError, IndexError): continue\n",
        "            entropy = self.entropy_bot.compute_entropy(probs_vector)\n",
        "            results[\"estimated_entropy\"] = entropy\n",
        "        return results\n",
        "    # Teleportation requires Qiskit circuit logic (complex, keeping simplified version)\n",
        "    def teleport(self, source_qubit: int, target_qubit: int, ancilla_qubit: int) -> Optional[Dict]:\n",
        "        if not self.simulator or self.n_qubits < 3: print(\"Error: Teleportation needs >= 3 qubits.\"); return None\n",
        "        circuit = self.simulator.circuit # Get current circuit\n",
        "        try:\n",
        "            # Check indices\n",
        "            if not all(0 <= q < self.n_qubits for q in [source_qubit, target_qubit, ancilla_qubit]): raise ValueError(\"Qubit index out of bounds\")\n",
        "            if len({source_qubit, target_qubit, ancilla_qubit}) != 3: raise ValueError(\"Source, target, and ancilla qubits must be distinct\")\n",
        "            # 1. Create Bell pair\n",
        "            circuit.h(ancilla_qubit); circuit.cx(ancilla_qubit, target_qubit); circuit.barrier()\n",
        "            # 2. CNOT source to ancilla, H source\n",
        "            circuit.cx(source_qubit, ancilla_qubit); circuit.h(source_qubit); circuit.barrier()\n",
        "            # 3. Measure source and ancilla - Need classical bits\n",
        "            if circuit.num_clbits < 2: circuit.add_register(ClassicalRegister(2, 'teleport_c'))\n",
        "            circuit.measure(source_qubit, 0); circuit.measure(ancilla_qubit, 1); circuit.barrier()\n",
        "            # 4. Conditional gates (placeholder comment - requires classical control)\n",
        "            # Apply X to target if clbit 1 is 1; Apply Z to target if clbit 0 is 1\n",
        "            print(f\"Teleportation circuit constructed for {source_qubit}->{target_qubit} via {ancilla_qubit}.\")\n",
        "            return {\"circuit\": circuit, \"notes\": \"Post-measurement corrections needed or use conditional execution.\"}\n",
        "        except Exception as e: print(f\"Error building teleport circuit: {e}\"); return {\"error\": str(e)}\n",
        "\n",
        "# --- Base Agent Class ---\n",
        "class Agent:\n",
        "    \"\"\"Basic Agent definition.\"\"\"\n",
        "    def __init__(self, name: str): self.name = name; self.engagement_state = 0\n",
        "    def get_engagement_state(self) -> int: return self.engagement_state\n",
        "    def set_engagement_state(self, state: int): print(f\"{self.name} engagement set to {state}\"); self.engagement_state = state\n",
        "    def adjust_workload(self, peer_engagement_level: int):\n",
        "        if peer_engagement_level > self.engagement_state: print(f\"{self.name} sees higher peer engagement ({peer_engagement_level}), considering offloading (placeholder).\")\n",
        "        # else: print(f\"{self.name} sees lower/equal peer engagement ({peer_engagement_level}), maintaining workload (placeholder).\") # Reduce noise\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict: return {\"status\": \"executed by generic agent\", \"agent\": self.name, \"result\": None}\n",
        "    def check_task_requirements(self, task: Dict, required_keys: List[str]) -> Optional[Dict]:\n",
        "        missing_keys = [key for key in required_keys if key not in task]\n",
        "        if missing_keys: error_msg = f\"Task missing required keys: {', '.join(missing_keys)}\"; print(f\"Error for agent {self.name}: {error_msg}\"); return {\"error\": error_msg, \"agent\": self.name}\n",
        "        return None\n",
        "\n",
        "# --- ZSG Agent & DFSN Agent Base Classes ---\n",
        "class ZSGAgent(Agent):\n",
        "    \"\"\"ZeroSumGame Agent, inheriting basic Agent properties.\"\"\"\n",
        "    def __init__(self, name: str): super().__init__(name); print(f\"ZSGAgent {self.name} initialized.\")\n",
        "\n",
        "class FlowStateOptimizer:\n",
        "    \"\"\"Optimizes agent flow state transitions based on performance and complexity.\"\"\"\n",
        "    def __init__(self, learning_rate=0.1):\n",
        "        self.learning_rate = learning_rate; self.math_module = PyZeroMathTorch()\n",
        "\t# Weights for [idle_pref, flow_pref, vibe_code_pref]\n",
        "        self.action_weights = torch.tensor([0.4, 0.4, 0.2], dtype=torch.float32)\n",
        "        self.iteration_count = 0; self.last_state = None; self.last_action = None\n",
        "    def predict(self, state: List[float], is_coding_task: bool = False) -> int:\n",
        "        task_complexity, performance, stability = state\n",
        "        complexity_s = self.math_module.f0z_stabilize(torch.tensor(task_complexity)).item()\n",
        "        perf_s = self.math_module.f0z_stabilize(torch.tensor(performance)).item()\n",
        "        stab_s = self.math_module.f0z_stabilize(torch.tensor(stability)).item()\n",
        "        flow_score = (complexity_s * 0.4 + perf_s * 0.5 - stab_s * 0.1); idle_score = 1.0 - flow_score\n",
        "        vibe_code_score = 0.0\n",
        "        if is_coding_task: vibe_code_score = (complexity_s * 0.5 + perf_s * 0.6 - stab_s * 0.05)\n",
        "        scores = torch.tensor([idle_score, flow_score, vibe_code_score], dtype=torch.float32); probs = torch.softmax(scores, dim=0)\n",
        "        weighted_probs = self.action_weights * probs; action = torch.argmax(weighted_probs).item()\n",
        "        self.last_state = state; self.last_action = action; self.iteration_count += 1\n",
        "        return action\n",
        "    def update(self, reward: float):\n",
        "        if self.last_action is None: return\n",
        "        reward_s = self.math_module.f0z_stabilize(torch.tensor(reward)).item()\n",
        "        target_weights = torch.zeros_like(self.action_weights)\n",
        "        if reward_s > 0: target_weights[self.last_action] = 1.0\n",
        "        else:\n",
        "            penalty_distribution = torch.ones_like(self.action_weights); penalty_distribution[self.last_action] = 0.0\n",
        "            if torch.sum(penalty_distribution) > 0: target_weights = penalty_distribution / torch.sum(penalty_distribution)\n",
        "        self.action_weights = (1 - self.learning_rate) * self.action_weights + self.learning_rate * target_weights\n",
        "        self.action_weights /= torch.sum(self.action_weights); self.action_weights = torch.clamp(self.action_weights, 0.05, 0.9); self.action_weights /= torch.sum(self.action_weights)\n",
        "        self.last_action = None; self.last_state = None\n",
        "\n",
        "class AgentIterativeWorkflow:\n",
        "    \"\"\"Manages internal iterations for an agent's task execution.\"\"\"\n",
        "    def __init__(self, agent: Agent, max_iterations: int = 3):\n",
        "        self.agent = agent; self.max_iterations = max_iterations\n",
        "        if not hasattr(agent, 'performance_history'): self.agent.performance_history = []\n",
        "        if not hasattr(agent, 'math_module'): self.agent.math_module = PyZeroMathTorch()\n",
        "        if not hasattr(agent, 'adjust_flow_state'): self.agent.adjust_flow_state = lambda *args: None\n",
        "    def iterate_task(self, task: Dict, complexity: float) -> Dict:\n",
        "        best_result = {\"error\": \"AIW: No valid result\", \"agent\": self.agent.name}; best_performance = -float('inf')\n",
        "        # print(f\"AIW starting for {self.agent.name}, Task: {task.get('type', 'N/A')}, Max Iter: {self.max_iterations}\") # Reduce noise\n",
        "        for i in range(self.max_iterations):\n",
        "            # print(f\"  AIW Iteration {i+1}/{self.max_iterations}\") # Reduce noise\n",
        "            if hasattr(self.agent, '_execute_single_task_iteration'): current_result = self.agent._execute_single_task_iteration(task)\n",
        "            else: print(f\"Error: Agent {self.agent.name} missing '_execute_single_task_iteration'.\"); current_result = {\"error\": \"Missing method\", \"agent\": self.agent.name}\n",
        "            if \"error\" in current_result: print(f\"    Error during iteration: {current_result['error']}\"); continue\n",
        "            performance = self.evaluate_performance(current_result.get(\"result\"), complexity)\n",
        "            if hasattr(self.agent, 'performance_history'): self.agent.performance_history.append(performance)\n",
        "            if performance > best_performance: best_performance = performance; best_result = current_result\n",
        "            task_type = task.get(\"type\", \"\"); task_desc = task.get(\"description\", \"\").lower()\n",
        "            is_coding = (\"code\" in task_type or any(kw in task_desc for kw in [\"code\", \"debug\", \"refactor\", \"implement\", \"algorithm\"]) or task.get(\"is_coding_task\", False))\n",
        "            if hasattr(self.agent, 'adjust_flow_state') and hasattr(self.agent, 'compute_stability'):\n",
        "                 stability = self.agent.compute_stability()\n",
        "                 self.agent.adjust_flow_state(complexity, performance, is_coding_task=is_coding)\n",
        "        # print(f\"AIW finished for {self.agent.name}. Best Perf: {best_performance:.4f}\") # Reduce noise\n",
        "        if \"error\" not in best_result: best_result[\"final_performance\"] = best_performance\n",
        "        return best_result\n",
        "    def evaluate_performance(self, result_data: Any, complexity: float) -> float:\n",
        "        complexity_penalty = 1.0 + max(0, complexity); perf = 0.0\n",
        "        if isinstance(result_data, (int, float)): perf = result_data\n",
        "        elif isinstance(result_data, (np.ndarray, torch.Tensor, list)):\n",
        "            try: numeric_vals = [float(v) for v in np.abs(np.array(result_data)).flatten() if isinstance(v, (int, float, np.number))]; perf = np.mean(numeric_vals) if numeric_vals else 0.0\n",
        "            except Exception: perf = 0.0\n",
        "        elif isinstance(result_data, dict):\n",
        "            if \"accuracy\" in result_data: perf = result_data[\"accuracy\"] * 10\n",
        "            elif \"loss\" in result_data: perf = -result_data[\"loss\"]\n",
        "            else: numeric_vals = [float(v) for v in result_data.values() if isinstance(v, (int, float))]; perf = sum(numeric_vals) if numeric_vals else 0.0\n",
        "        elif isinstance(result_data, bool): perf = 1.0 if result_data else 0.0\n",
        "        stabilized_perf = self.agent.math_module.f0z_stabilize(torch.tensor(perf)).item()\n",
        "        return stabilized_perf / (complexity_penalty + 1e-8)\n",
        "    def adapt_task(self, task: Dict, result: Dict, performance: float) -> Dict: return task # Placeholder\n",
        "\n",
        "class DFSNAgent(ZSGAgent):\n",
        "    \"\"\"Agent capable of operating within the Dynamic Flow State Network.\"\"\"\n",
        "    def __init__(self, name: str, math_module: Optional[PyZeroMathTorch] = None, pre_trained_model_path: Optional[str] = None):\n",
        "        super().__init__(name)\n",
        "        self.flow_state = 'idle' # 'idle', 'flow', 'vibe_code'\n",
        "        self.performance_history: List[float] = []\n",
        "        self.math_module = math_module if math_module else PyZeroMathTorch()\n",
        "        self.optimizer = FlowStateOptimizer()\n",
        "        self.aiw = AgentIterativeWorkflow(self)\n",
        "        self.cpu_allocation = 0.0; self.memory_allocation = 0.0; self.state_vector = None\n",
        "        self.policy_model = self.load_policy_model(pre_trained_model_path)\n",
        "        # print(f\"DFSNAgent {name} initialized. Flow state: {self.flow_state}\") # Reduce noise\n",
        "\n",
        "    def load_policy_model(self, path: Optional[str]):\n",
        "         if path and os.path.exists(path): print(f\"{self.name}: Loading pre-trained model from {path}\"); return {\"model_type\": \"placeholder\", \"path\": path}\n",
        "         return None\n",
        "    def enter_flow_state(self):\n",
        "        if self.flow_state != 'flow': print(f\"{self.name} entered flow state (engagement: {self.engagement_state}).\")\n",
        "        self.set_engagement_state(max(5, self.engagement_state + 2)); self.flow_state = 'flow'\n",
        "    def enter_vibe_code_state(self):\n",
        "        if self.flow_state != 'vibe_code': print(f\"{self.name} entered vibe code state (engagement: {self.engagement_state}).\")\n",
        "        self.set_engagement_state(max(6, self.engagement_state + 1)); self.flow_state = 'vibe_code'\n",
        "    def exit_flow_state(self):\n",
        "        if self.flow_state != 'idle':\n",
        "             previous_state = self.flow_state; self.set_engagement_state(min(3, self.engagement_state - 2)); self.flow_state = 'idle'\n",
        "             print(f\"{self.name} exited {previous_state} state to idle (engagement: {self.engagement_state}).\")\n",
        "    def adjust_flow_state(self, task_complexity: float, performance: float, is_coding_task: bool = False):\n",
        "        stability = self.compute_stability(); current_state_features = [task_complexity, performance, stability]\n",
        "        action = self.optimizer.predict(current_state_features, is_coding_task)\n",
        "        if action == 0 and self.flow_state != 'idle': self.exit_flow_state()\n",
        "        elif action == 1 and self.flow_state != 'flow':\n",
        "            if self.flow_state == 'vibe_code': self.exit_flow_state()\n",
        "            self.enter_flow_state()\n",
        "        elif action == 2 and self.flow_state != 'vibe_code':\n",
        "            if self.flow_state == 'flow': self.exit_flow_state()\n",
        "            self.enter_vibe_code_state()\n",
        "        reward = performance - 0.05 * task_complexity; self.optimizer.update(reward)\n",
        "    def execute_task(self, task: Dict) -> Dict: # This method calls AIW\n",
        "        complexity = task.get('complexity', 5.0); result = self.aiw.iterate_task(task, complexity); return result\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict: # Base implementation for one iteration\n",
        "         error = self.check_task_requirements(task, []);\n",
        "         if error: return error\n",
        "         print(f\"Warning: {self.name} using base DFSNAgent _execute_single_task_iteration. Task type '{task.get('type', 'N/A')}' likely not handled.\")\n",
        "         time.sleep(0.01 * task.get('complexity', 1.0))\n",
        "         return {\"result\": f\"Base execution for {task.get('type', 'N/A')}\", \"agent\": self.name}\n",
        "    def share_state(self, peer: Agent):\n",
        "        if not isinstance(peer, DFSNAgent): print(f\"{self.name} cannot share state with non-DFSN agent {peer.name}\"); return\n",
        "        state_to_share = {\"performance_history\": self.performance_history[-10:], \"flow_state\": self.flow_state, \"engagement_state\": self.engagement_state, \"domain_data\": self.get_domain_specific_state()}\n",
        "        # print(f\"{self.name} sharing state with {peer.name}: Flow={self.flow_state}, Engage={self.engagement_state}\") # Reduce noise\n",
        "        peer.receive_state(state_to_share)\n",
        "    def receive_state(self, state: Dict):\n",
        "        # print(f\"{self.name} received state update from peer.\") # Reduce noise\n",
        "        if \"performance_history\" in state: self.performance_history.extend(state[\"performance_history\"]); self.performance_history = self.performance_history[-50:]\n",
        "        if \"domain_data\" in state: self.process_domain_specific_state(state[\"domain_data\"])\n",
        "    def compute_stability(self) -> float:\n",
        "        if len(self.performance_history) < 5: return 0.0\n",
        "        variance = F0ZAlgebra.f0z_variance(self.performance_history[-10:])\n",
        "        stability_score = 1.0 / (variance + float(self.math_module.epsilon_0))\n",
        "        return float(np.clip(stability_score, 0.0, 100.0))\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]: return None\n",
        "    def process_domain_specific_state(self, domain_data: Dict): pass\n",
        "\n",
        "# --- Core ZSG Systems ---\n",
        "# --- Core ZSG Systems ---\n",
        "class MemorySystem:\n",
        "    \"\"\"Manages short-term and long-term memory for ZSG agents.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Simple dict-based short-term memory (e.g., last result per task type)\n",
        "        self.short_term_memory: Dict[str, Any] = {}\n",
        "        # List-based long-term memory (e.g., history of significant events or results)\n",
        "        self.long_term_memory: List[Any] = []\n",
        "        # Task-specific storage (from later iterations)\n",
        "        self.task_store: Dict[str, List[Dict]] = {} # Key: episode_iteration string\n",
        "        self.episode_memory: Dict[Tuple[int, int], Any] = {} # Key: (episode, iteration) tuple\n",
        "        self.max_long_term_size = 1000 # Configurable limit\n",
        "        print(\"MemorySystem initialized.\")\n",
        "\n",
        "\n",
        "    def store_memory(self, memory_data: Any, memory_type: str = 'short', key: Optional[str] = None):\n",
        "        \"\"\"Store data in the specified memory type.\"\"\"\n",
        "        if memory_type == 'short':\n",
        "            if key is None: key = f\"generic_{time.time()}\" # Generate a key if none provided\n",
        "            self.short_term_memory[key] = memory_data\n",
        "            # print(f\"Stored in short-term memory with key '{key}'.\")\n",
        "        elif memory_type == 'long':\n",
        "            self.long_term_memory.append(memory_data)\n",
        "            # Enforce size limit\n",
        "            if len(self.long_term_memory) > self.max_long_term_size:\n",
        "                self.long_term_memory.pop(0) # Remove the oldest item\n",
        "            # print(f\"Stored in long-term memory (size: {len(self.long_term_memory)}).\")\n",
        "        else:\n",
        "            print(f\"Warning: Unknown memory type '{memory_type}'. Data not stored.\")\n",
        "\n",
        "    def retrieve_memory(self, memory_type: str = 'short', key: Optional[str] = None, criteria: Optional[callable] = None) -> Any:\n",
        "        \"\"\"\n",
        "        Retrieve data from the specified memory type.\n",
        "        For long-term, criteria can be a function that takes a memory item and returns True if it matches.\n",
        "        \"\"\"\n",
        "        if memory_type == 'short':\n",
        "            if key:\n",
        "                return self.short_term_memory.get(key)\n",
        "            else:\n",
        "                # Return the most recent item or None if empty\n",
        "                return list(self.short_term_memory.values())[-1] if self.short_term_memory else None\n",
        "        elif memory_type == 'long':\n",
        "            if criteria:\n",
        "                # Search long-term memory backwards (most recent first) based on criteria\n",
        "                for item in reversed(self.long_term_memory):\n",
        "                    try:\n",
        "                        if criteria(item):\n",
        "                            return item\n",
        "                    except Exception as e:\n",
        "                         # Catch errors in criteria function evaluation\n",
        "                        print(f\"Error applying criteria function in retrieve_memory: {e}\")\n",
        "                        continue # Skip this item\n",
        "                return None # No item matched the criteria\n",
        "            else:\n",
        "                # Return the most recent long-term memory item or None if empty\n",
        "                return self.long_term_memory[-1] if self.long_term_memory else None\n",
        "        else:\n",
        "            print(f\"Warning: Unknown memory type '{memory_type}'. Cannot retrieve.\")\n",
        "            return None\n",
        "\n",
        "    def store_task(self, episode: int, iteration: int, todo): # Assuming todo has a .to_json() method\n",
        "        \"\"\"Stores a specific task identified by episode and iteration.\"\"\"\n",
        "        key = f\"{episode}_{iteration}\"\n",
        "        if key not in self.task_store:\n",
        "            self.task_store[key] = []\n",
        "        # Ensure todo is serializable or get its JSON representation\n",
        "        try:\n",
        "            todo_data = todo.to_json() if hasattr(todo, 'to_json') else todo\n",
        "            if not isinstance(todo_data, dict):\n",
        "                 raise ValueError(\"Stored task data must be a dictionary or have a to_json method.\")\n",
        "            self.task_store[key].append(todo_data)\n",
        "            # print(f\"Stored task for {key}. Total tasks for key: {len(self.task_store[key])}\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error storing task for {key}: {e}. Data: {todo}\")\n",
        "\n",
        "\n",
        "    def store_episode(self, episode: int, iteration: int, results: Any):\n",
        "        \"\"\"Stores the results associated with a specific episode and iteration.\"\"\"\n",
        "        self.episode_memory[(episode, iteration)] = results\n",
        "        # print(f\"Stored results for episode {episode}, iteration {iteration}.\")\n",
        "\n",
        "    def retrieve_task(self, episode: int, iteration: int) -> Optional[List[Dict]]:\n",
        "         \"\"\"Retrieves tasks for a given episode and iteration.\"\"\"\n",
        "         key = f\"{episode}_{iteration}\"\n",
        "         return self.task_store.get(key)\n",
        "\n",
        "    def retrieve_episode(self, episode: int, iteration: int) -> Optional[Any]:\n",
        "         \"\"\"Retrieves results for a given episode and iteration.\"\"\"\n",
        "         return self.episode_memory.get((episode, iteration))\n",
        "\n",
        "class ResourceMonitor:\n",
        "    \"\"\"Monitors system resources and can adjust parameters like batch size.\"\"\"\n",
        "    def __init__(self, batch_size_init=32):\n",
        "        self.batch_size = batch_size_init\n",
        "        self.base_cpu = 10 # Base allocation % per agent\n",
        "        self.base_memory = 10 # Base allocation % per agent\n",
        "        self.total_cpu = psutil.cpu_count() * 100 # Theoretical max %\n",
        "        self.total_memory = 100 # Percentage based\n",
        "        self.processes = {} # Track resource usage per process/agent if needed\n",
        "        print(f\"ResourceMonitor initialized. Initial batch size: {self.batch_size}\")\n",
        "\n",
        "\n",
        "    def check_usage(self) -> Tuple[float, float]:\n",
        "        \"\"\"Checks current overall CPU and virtual memory usage.\"\"\"\n",
        "        cpu_usage = psutil.cpu_percent(interval=0.1) # Non-blocking short interval\n",
        "        mem_usage = psutil.virtual_memory().percent\n",
        "        # print(f\"Resource Check: CPU={cpu_usage}%, Memory={mem_usage}%\")\n",
        "        return cpu_usage, mem_usage\n",
        "\n",
        "    def adjust_batch_size(self) -> int:\n",
        "        \"\"\"Adjusts batch size based on resource usage.\"\"\"\n",
        "        cpu, mem = self.check_usage()\n",
        "        if cpu > 85.0 or mem > 85.0:\n",
        "            print(f\"High resource usage (CPU: {cpu:.1f}%, Mem: {mem:.1f}%). Reducing batch size.\")\n",
        "            self.batch_size = max(8, self.batch_size // 2) # Halve batch size, min 8\n",
        "        elif cpu < 50.0 and mem < 50.0:\n",
        "             # Gradually increase batch size if resources are low\n",
        "             self.batch_size = min(128, self.batch_size + 4) # Increase slowly, max 128\n",
        "        print(f\"Adjusted batch size to: {self.batch_size}\")\n",
        "        return self.batch_size\n",
        "\n",
        "    def pre_allocate(self, agents: List[DFSNAgent], task_complexity: float) -> Dict[str, float]:\n",
        "        \"\"\"Estimate and pre-allocate resources based on agent count and task complexity.\"\"\"\n",
        "        num_agents = len(agents)\n",
        "        if num_agents == 0: return {\"cpu_total\": 0, \"memory_total\": 0}\n",
        "\n",
        "        # Estimate needed resources - simple scaling\n",
        "        cpu_needed_per_agent = min(self.base_cpu + 5 * task_complexity, 100 / num_agents) # Limit by available share\n",
        "        mem_needed_per_agent = min(self.base_memory + 4 * task_complexity, 100 / num_agents)\n",
        "\n",
        "        total_cpu_allocated = 0\n",
        "        total_memory_allocated = 0\n",
        "        for agent in agents:\n",
        "            agent.cpu_allocation = cpu_needed_per_agent\n",
        "            agent.memory_allocation = mem_needed_per_agent\n",
        "            total_cpu_allocated += cpu_needed_per_agent\n",
        "            total_memory_allocated += mem_needed_per_agent\n",
        "\n",
        "        print(f\"Pre-allocated resources: ~{cpu_needed_per_agent:.1f}% CPU, ~{mem_needed_per_agent:.1f}% Mem per agent. Total: {total_cpu_allocated:.1f}% CPU, {total_memory_allocated:.1f}% Mem\")\n",
        "        return {\"cpu_total\": total_cpu_allocated, \"memory_total\": total_memory_allocated}\n",
        "\n",
        "\n",
        "    def update_allocations(self, agents: List[DFSNAgent]):\n",
        "        \"\"\"Dynamically update resource allocations based on agent engagement and performance.\"\"\"\n",
        "        num_agents = len(agents)\n",
        "        if num_agents == 0: return\n",
        "\n",
        "        active_agents = [agent for agent in agents if agent.engagement_state > 0]\n",
        "        num_active = len(active_agents)\n",
        "\n",
        "        if num_active == 0: # No active agents, allocate base resources\n",
        "            for agent in agents:\n",
        "                agent.cpu_allocation = self.base_cpu\n",
        "                agent.memory_allocation = self.base_memory\n",
        "            # print(\"No active agents. Allocating base resources.\")\n",
        "            return\n",
        "\n",
        "        # Calculate available resources beyond base allocation for all agents\n",
        "        available_extra_cpu = self.total_cpu - (self.base_cpu * num_agents)\n",
        "        available_extra_memory = self.total_memory - (self.base_memory * num_agents)\n",
        "\n",
        "        # Distribute extra resources among active agents based on performance (simple weighting)\n",
        "        total_performance_score = sum(agent.performance_history[-1] if agent.performance_history else 0.1 for agent in active_agents) + 1e-8 # Avoid zero division\n",
        "        if total_performance_score <= 0: total_performance_score = 1e-8 # Ensure positive\n",
        "\n",
        "        for agent in agents:\n",
        "            if agent in active_agents:\n",
        "                perf_score = agent.performance_history[-1] if agent.performance_history else 0.1\n",
        "                perf_weight = (perf_score / total_performance_score) if total_performance_score > 0 else (1/num_active)\n",
        "\n",
        "                # Allocate proportional share of extra resources + base\n",
        "                agent.cpu_allocation = self.base_cpu + available_extra_cpu * perf_weight\n",
        "                agent.memory_allocation = self.base_memory + available_extra_memory * perf_weight\n",
        "            else:\n",
        "                # Inactive agents get base allocation\n",
        "                agent.cpu_allocation = self.base_cpu\n",
        "                agent.memory_allocation = self.base_memory\n",
        "\n",
        "        # Clamp allocations to reasonable bounds (e.g., max 80% per agent)\n",
        "        for agent in agents:\n",
        "             agent.cpu_allocation = max(5.0, min(agent.cpu_allocation, 80.0))\n",
        "             agent.memory_allocation = max(5.0, min(agent.memory_allocation, 80.0))\n",
        "             # print(f\"  {agent.name}: CPU={agent.cpu_allocation:.1f}%, Mem={agent.memory_allocation:.1f}%\")\n",
        "\n",
        "        # print(\"Updated resource allocations for active agents based on performance.\")\n",
        "\n",
        "    def start(self):\n",
        "         \"\"\"Start monitoring (if running in a separate thread/process).\"\"\"\n",
        "         print(\"Resource monitoring started (simulated).\")\n",
        "\n",
        "    def stop(self):\n",
        "         \"\"\"Stop monitoring.\"\"\"\n",
        "         print(\"Resource monitoring stopped (simulated).\")\n",
        "\n",
        "class DynamicFlowStateNetwork:\n",
        "    \"\"\"Manages the flow states of a collection of agents based on task complexity.\"\"\"\n",
        "    def __init__(self, agents: List[DFSNAgent], task_complexity_threshold: float = 5.0, max_agents: int = 15):\n",
        "        self.agents = agents # This should be a reference, updated by ZSGManager\n",
        "        self.task_complexity_threshold = task_complexity_threshold\n",
        "        self.agent_states: Dict[str, str] = {agent.name: agent.flow_state for agent in agents}\n",
        "        self.max_agents = max_agents\n",
        "        self.is_dynamic_enabled = False\n",
        "        print(f\"DFSN initialized. Threshold: {self.task_complexity_threshold}, Max Agents: {self.max_agents}\")\n",
        "\n",
        "\n",
        "    def adjust_flow_states(self, current_task_complexity: float, batch_info: Optional[List] = None):\n",
        "        \"\"\"Adjust agent flow states based on complexity and potentially batch info.\"\"\"\n",
        "        if not self.is_dynamic_enabled:\n",
        "            # print(\"DFSN is disabled. No flow state adjustments.\")\n",
        "            return\n",
        "\n",
        "        # Determine complexity measure (use batch avg if available)\n",
        "        complexity_measure = current_task_complexity\n",
        "        if batch_info and len(batch_info) > 0:\n",
        "            # Assuming batch_info is a list of ZSGTodo objects or similar dicts with 'priority'\n",
        "            try:\n",
        "                batch_avg_complexity = sum(item.priority if hasattr(item, 'priority') else item.get('priority', 5.0) for item in batch_info) / len(batch_info)\n",
        "                complexity_measure = (current_task_complexity + batch_avg_complexity) / 2 # Average task and batch complexity\n",
        "                print(f\"DFSN using combined complexity: {complexity_measure:.2f} (Task: {current_task_complexity:.2f}, Batch Avg: {batch_avg_complexity:.2f})\")\n",
        "            except Exception as e:\n",
        "                print(f\"DFSN Warning: Could not calculate batch average complexity: {e}\")\n",
        "\n",
        "\n",
        "        # print(f\"DFSN adjusting flow states based on complexity measure: {complexity_measure:.2f} (Threshold: {self.task_complexity_threshold})\")\n",
        "        num_flow = 0\n",
        "        num_idle = 0\n",
        "        for agent in self.agents:\n",
        "            # Agents decide their own state via adjust_flow_state called within AIW/_execute_single_task_iteration\n",
        "            # DFSN can provide global context or override based on system-wide needs\n",
        "            performance = np.mean(agent.performance_history[-5:]) if agent.performance_history else 0.0\n",
        "            # The agent's internal optimizer will handle the transition logic\n",
        "            agent.adjust_flow_state(complexity_measure, performance)\n",
        "            self.agent_states[agent.name] = agent.flow_state # Update tracked state\n",
        "\n",
        "            if agent.flow_state == 'flow':\n",
        "                num_flow += 1\n",
        "            else:\n",
        "                num_idle += 1\n",
        "\n",
        "        print(f\"DFSN status: {num_flow} agents in flow, {num_idle} agents idle.\")\n",
        "        self.scale_agents(complexity_measure) # Scale agents after adjustments\n",
        "\n",
        "    def enable_dynamic_states(self):\n",
        "        \"\"\"Enable dynamic adjustments.\"\"\"\n",
        "        self.is_dynamic_enabled = True\n",
        "        print(\"DFSN enabled for dynamic state adjustments.\")\n",
        "\n",
        "    def disable_dynamic_states(self):\n",
        "        \"\"\"Disable dynamic adjustments and reset agents to idle.\"\"\"\n",
        "        self.is_dynamic_enabled = False\n",
        "        for agent in self.agents:\n",
        "             if agent.flow_state == 'flow':\n",
        "                 agent.exit_flow_state() # Explicitly exit flow\n",
        "             self.agent_states[agent.name] = agent.flow_state\n",
        "        print(\"DFSN disabled. Agents reset towards idle states.\")\n",
        "\n",
        "    def scale_agents(self, task_complexity: float):\n",
        "        \"\"\"Dynamically scale the number of active agents (placeholder).\"\"\"\n",
        "        # Example scaling logic: more complex tasks require more agents active/instantiated\n",
        "        target_active_agents = max(2, min(self.max_agents, int(task_complexity / 2.0) + 1))\n",
        "        num_current_agents = len(self.agents)\n",
        "        num_currently_active = sum(1 for agent in self.agents if agent.engagement_state > 0)\n",
        "\n",
        "        print(f\"DFSN Scaling Check: Complexity={task_complexity:.2f}, TargetActive={target_active_agents}, CurrentTotal={num_current_agents}, CurrentActive={num_currently_active}\")\n",
        "\n",
        "        # Add new agents if needed and below max limit\n",
        "        if num_current_agents < target_active_agents and num_current_agents < self.max_agents:\n",
        "            num_to_add = min(target_active_agents - num_current_agents, self.max_agents - num_current_agents)\n",
        "            print(f\"  Scaling up: Adding {num_to_add} new DFSNAgent(s).\")\n",
        "            for i in range(num_to_add):\n",
        "                 new_agent_name = f\"DFSNAgent_{num_current_agents + i}\"\n",
        "                 # This needs interaction with the ZSGManager to actually add the agent\n",
        "                 # self.manager.add_agent(DFSNAgent, new_agent_name) # Conceptual\n",
        "                 print(f\"    (Conceptual) Added {new_agent_name}\")\n",
        "            # Note: Need a reference to the manager or a callback to add agents properly.\n",
        "\n",
        "        # Deactivate surplus agents if complexity is low (or handle via engagement states)\n",
        "        elif num_currently_active > target_active_agents and task_complexity < self.task_complexity_threshold * 0.8:\n",
        "             num_to_deactivate = num_currently_active - target_active_agents\n",
        "             print(f\"  Scaling down: Deactivating {num_to_deactivate} agent(s) (setting to low engagement).\")\n",
        "             # Find agents to deactivate (e.g., lowest performance or idle)\n",
        "             agents_to_consider = sorted(self.agents, key=lambda a: (a.engagement_state, np.mean(a.performance_history[-5:]) if a.performance_history else 0))\n",
        "             for i in range(num_to_deactivate):\n",
        "                 if i < len(agents_to_consider):\n",
        "                     agents_to_consider[i].exit_flow_state() # Force lower engagement\n",
        "                     print(f\"    Deactivated {agents_to_consider[i].name}\")\n",
        "\n",
        "    def handle_chaos(self, chaos_metrics: Dict):\n",
        "         \"\"\"Adjust DFSN parameters based on chaos metrics.\"\"\"\n",
        "         lyapunov_exp = chaos_metrics.get(\"lyapunov\", 0.0)\n",
        "         print(f\"DFSN received chaos metrics: Lyapunov={lyapunov_exp:.3f}\")\n",
        "         if lyapunov_exp > 0.5: # System becoming more chaotic\n",
        "             print(\"  High chaos detected. Increasing stability preference (reducing threshold).\")\n",
        "             self.task_complexity_threshold *= 0.95 # Become sensitive to flow at lower complexity\n",
        "             # Maybe increase learning rate of flow optimizer to adapt faster\n",
        "             for agent in self.agents:\n",
        "                 agent.optimizer.learning_rate = min(0.2, agent.optimizer.learning_rate * 1.1)\n",
        "         elif lyapunov_exp < 0.1: # System very stable\n",
        "             print(\"  Low chaos detected. Increasing complexity tolerance (increasing threshold).\")\n",
        "             self.task_complexity_threshold *= 1.05\n",
        "             for agent in self.agents:\n",
        "                 agent.optimizer.learning_rate = max(0.05, agent.optimizer.learning_rate * 0.95)\n",
        "\n",
        "\n",
        "class MultiAgentCoordinator:\n",
        "    \"\"\"Coordinates task assignment and synchronization among multiple agents.\"\"\"\n",
        "    def __init__(self, agents: List[Agent]):\n",
        "        self.agents = agents # Reference to the list of agents managed by ZSGManager\n",
        "        self.task_queue: List[Tuple[int, Dict]] = [] # Priority queue (using heapq, neg priority)\n",
        "        # Basic resource pool tracking (can be enhanced by ResourceMonitor)\n",
        "        self.resource_pool = {\"cpu\": 100.0, \"memory\": 100.0}\n",
        "        self.agent_states: Dict[str, Dict] = {agent.name: {\"state\": \"idle\", \"load\": 0.0, \"engagement\": agent.get_engagement_state()} for agent in agents}\n",
        "        self.domain_map = self._build_domain_map()\n",
        "        print(f\"MultiAgentCoordinator initialized with {len(agents)} agents.\")\n",
        "\n",
        "    def _build_domain_map(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Builds a map from task types/domains to capable agent names.\"\"\"\n",
        "        # This should be dynamic based on agent capabilities\n",
        "        domain_map = {\n",
        "            \"physics_simulation\": [\"PhysicsAgent\"], # Can match parts of agent names\n",
        "            \"quantum\": [\"QuantumAgent\"], # Matches QuantumAgent_1, QuantumAgent_QA1 etc.\n",
        "            \"memory_task\": [\"MemoryAgent\"],\n",
        "            \"science_fair_experiment\": [\"CollaborativeAgent\"],\n",
        "            \"collaboration\": [\"CollaborativeAgent\"],\n",
        "            \"temporal_forecast\": [\"TemporalPrimeAgent\"],\n",
        "            \"organic_chemistry\": [\"OrganicChemistryAgent\"],\n",
        "            \"molecular_biology\": [\"MolecularBiologyAgent\"],\n",
        "            \"creative\": [\"CreativeAgent\"],\n",
        "            \"information_theory\": [\"InformationTheoryAgent\"],\n",
        "            \"data_science\": [\"DataScienceAgent\"],\n",
        "            \"astrophysics\": [\"AstrophysicsAgent\"],\n",
        "            \"robotics\": [\"RoboticsAgent\"],\n",
        "            \"environmental_science\": [\"EnvironmentalScienceAgent\"],\n",
        "            \"machine_learning\": [\"MachineLearningAgent\"],\n",
        "            \"validation\": [\"ValidationAgent\"],\n",
        "            \"chaos\": [\"PhysicsAgent\", \"FractalAgent\"], # Example: Multiple agents can handle\n",
        "            \"fractal_generate\": [\"FractalAgent\"],\n",
        "            \"hnn_update\": [\"HopfieldAgent\"],\n",
        "            \"temporal_scaling\": [\"TemporalPrimeAgent\"],\n",
        "            \"llada_task\": [\"LLaDATaskAgent\"],\n",
        "            \"quantum_poetry\": [\"QuantumAIMLLLM\"], # Handled by manager directly? Or a dedicated agent?\n",
        "            \"quantum_game\": [\"QuantumAgent\"],\n",
        "            \"quantum_field\": [\"QuantumAgent\"],\n",
        "            \"grover_search\": [\"QuantumAgent\"],\n",
        "            \"shor_factor\": [\"QuantumAgent\"],\n",
        "            \"quantum_circuit\": [\"QuantumAgent\"],\n",
        "            # Add more mappings as new agents/tasks are defined\n",
        "        }\n",
        "        print(\"Coordinator domain map built.\")\n",
        "        return domain_map\n",
        "\n",
        "    def find_capable_agents(self, task_type: str) -> List[Agent]:\n",
        "         \"\"\"Finds agents whose names or declared capabilities match the task type.\"\"\"\n",
        "         capable_agents = []\n",
        "         agent_dict = {agent.name: agent for agent in self.agents}\n",
        "\n",
        "         for domain_key, agent_name_patterns in self.domain_map.items():\n",
        "             if domain_key in task_type: # Simple substring matching for type\n",
        "                 for pattern in agent_name_patterns:\n",
        "                     for agent_name, agent in agent_dict.items():\n",
        "                         if pattern in agent_name and agent not in capable_agents:\n",
        "                             capable_agents.append(agent)\n",
        "\n",
        "         # Fallback if no specific agent found\n",
        "         if not capable_agents:\n",
        "             print(f\"No specific agent found for task type '{task_type}', assigning to general DFSNAgent.\")\n",
        "             # Find any generic DFSNAgent available\n",
        "             general_agents = [agent for agent in self.agents if isinstance(agent, DFSNAgent) and not any(pattern in agent.name for patterns in self.domain_map.values() for pattern in patterns)]\n",
        "             if general_agents:\n",
        "                 capable_agents.append(random.choice(general_agents)) # Assign to a random general agent\n",
        "             elif self.agents:\n",
        "                  capable_agents.append(random.choice(self.agents)) # Assign to any agent if no general one exists\n",
        "\n",
        "         # print(f\"Found {len(capable_agents)} capable agents for task '{task_type}': {[a.name for a in capable_agents]}\")\n",
        "         return capable_agents\n",
        "\n",
        "\n",
        "    def add_task(self, task: dict, priority: int):\n",
        "        \"\"\"Adds a task to the priority queue.\"\"\"\n",
        "        if 'type' not in task:\n",
        "            print(\"Warning: Task added without 'type'. Assigning low priority.\")\n",
        "            task['type'] = 'unknown'\n",
        "            priority = -10 # Use negative for min-heap\n",
        "\n",
        "        print(f\"Coordinator adding task: {task['type']} with priority {priority}\")\n",
        "        heappush(self.task_queue, (-priority, task)) # Use negative priority for max-heap behavior\n",
        "\n",
        "    def assign_tasks(self, task: Dict) -> Dict:\n",
        "         \"\"\"Assigns a single task to the most suitable agent(s).\"\"\"\n",
        "         task_type = task.get(\"type\", \"unknown\")\n",
        "         capable_agents = self.find_capable_agents(task_type)\n",
        "\n",
        "         if not capable_agents:\n",
        "             print(f\"Error: No capable agent found for task type '{task_type}'.\")\n",
        "             return {\"error\": \"No suitable agent found\", \"task_type\": task_type}\n",
        "\n",
        "         # Simple assignment: Assign to the least loaded capable agent\n",
        "         # More complex: Consider engagement state, specialization score, etc.\n",
        "         best_agent = min(capable_agents, key=lambda a: self.agent_states.get(a.name, {\"load\": 0.0})[\"load\"])\n",
        "\n",
        "         print(f\"Assigning task '{task_type}' to agent: {best_agent.name} (Load: {self.agent_states.get(best_agent.name, {}).get('load', 0):.2f})\")\n",
        "         self.agent_states[best_agent.name][\"load\"] += task.get(\"complexity\", 5.0) # Increment load estimate\n",
        "\n",
        "         # Execute the task via the agent's _execute_single_task_iteration method\n",
        "         result = best_agent._execute_single_task_iteration(task)\n",
        "\n",
        "         # Decrement load after execution (simplified)\n",
        "         self.agent_states[best_agent.name][\"load\"] -= task.get(\"complexity\", 5.0)\n",
        "         self.agent_states[best_agent.name][\"load\"] = max(0, self.agent_states[best_agent.name][\"load\"]) # Ensure load doesn't go negative\n",
        "\n",
        "         # Synchronize states after task completion\n",
        "         self.synchronize_flow_states({\"success\": \"error\" not in result})\n",
        "\n",
        "         return result\n",
        "\n",
        "    def execute_next_task(self) -> Optional[Dict]:\n",
        "        \"\"\"Retrieves and executes the highest priority task from the queue.\"\"\"\n",
        "        if not self.task_queue:\n",
        "            # print(\"Coordinator task queue is empty.\")\n",
        "            return None\n",
        "\n",
        "        neg_priority, task = heappop(self.task_queue)\n",
        "        priority = -neg_priority\n",
        "        print(f\"Coordinator executing next task (Priority: {priority}): {task.get('type', 'Unknown')}\")\n",
        "\n",
        "        return self.assign_tasks(task) # Use the assign_tasks logic\n",
        "\n",
        "\n",
        "    def synchronize_flow_states(self, task_feedback: Optional[Dict] = None):\n",
        "        \"\"\"Synchronizes agent states, potentially based on feedback or global state.\"\"\"\n",
        "        # print(\"Coordinator synchronizing flow states...\")\n",
        "        if not self.agents: return\n",
        "\n",
        "        engagement_levels = [agent.get_engagement_state() for agent in self.agents]\n",
        "        avg_engagement = np.mean(engagement_levels) if engagement_levels else 0\n",
        "        # print(f\"  Average engagement: {avg_engagement:.2f}\")\n",
        "\n",
        "        # Update internal tracking\n",
        "        for agent in self.agents:\n",
        "            self.agent_states[agent.name][\"engagement\"] = agent.get_engagement_state()\n",
        "            # Optional: Adjust load based on engagement?\n",
        "\n",
        "        # Basic synchronization: Agents adjust workload based on average (or peers)\n",
        "        for agent in self.agents:\n",
        "            # Pass average engagement, agent can decide how to react\n",
        "             agent.adjust_workload(avg_engagement)\n",
        "\n",
        "             # Optionally trigger DFSN adjustments based on global state\n",
        "             # if hasattr(self.manager, 'flow_state_network'): # Requires link to manager\n",
        "             #     self.manager.flow_state_network.adjust_flow_states(global_complexity_measure)\n",
        "             pass\n",
        "\n",
        "        # print(\"Flow state synchronization complete.\")\n",
        "\n",
        "\n",
        "class MLIW: # Machine Learning Iterative Workflow\n",
        "    \"\"\"Manages the episode and iteration state for the MLIW.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.current_episode: int = 0\n",
        "        self.current_iteration: int = 0\n",
        "        print(\"MLIW Controller initialized.\")\n",
        "\n",
        "    def start_episode(self, episode: Optional[int] = None, iteration: Optional[int] = None):\n",
        "        \"\"\"Starts or advances the episode/iteration counter.\"\"\"\n",
        "        if episode is not None:\n",
        "            self.current_episode = episode\n",
        "        else:\n",
        "            self.current_episode += 1\n",
        "\n",
        "        if iteration is not None:\n",
        "            self.current_iteration = iteration\n",
        "        else:\n",
        "            self.current_iteration = 1 # Reset iteration for new episode\n",
        "\n",
        "        print(f\"MLIW starting Episode {self.current_episode}, Iteration {self.current_iteration}\")\n",
        "\n",
        "    def next_iteration(self):\n",
        "        \"\"\"Advances to the next iteration within the current episode.\"\"\"\n",
        "        self.current_iteration += 1\n",
        "        print(f\"MLIW advanced to Episode {self.current_episode}, Iteration {self.current_iteration}\")\n",
        "\n",
        "    def get_state(self) -> Tuple[int, int]:\n",
        "        \"\"\"Returns the current episode and iteration.\"\"\"\n",
        "        return self.current_episode, self.current_iteration\n",
        "\n",
        "\n",
        "class ZSGTodo:\n",
        "    \"\"\"Represents a task item within the ZSG framework.\"\"\"\n",
        "    def __init__(self, task_id: str, description: str, status: str, priority: float, mliw_step: str, data_payload: Dict):\n",
        "        self.task_id = task_id # Unique ID (e.g., \"T001\", \"QuantumTask_abc\")\n",
        "        self.description = description # E.g., \"Optimize PNS sampling\", \"Run Grover Search\"\n",
        "        self.status = status # \"Pending\", \"In Progress\", \"Completed\", \"Failed\"\n",
        "        self.priority = priority # Numerical priority (higher is more important)\n",
        "        self.mliw_step = mliw_step # Which MLIW phase (e.g., \"Analyze\", \"Modulate\", \"Test\", \"Generate\", \"Validate\")\n",
        "        self.data_payload = data_payload # Input data, parameters, or results needed/produced (dict)\n",
        "\n",
        "        # Additional fields for tracking, if needed\n",
        "        self.creation_time = time.time()\n",
        "        self.assigned_agent: Optional[str] = None\n",
        "        self.completion_time: Optional[float] = None\n",
        "\n",
        "    def to_json(self) -> Dict:\n",
        "        \"\"\"Serializes the ZSGTodo object into a JSON-compatible dictionary.\"\"\"\n",
        "        return {\n",
        "            \"task_id\": self.task_id,\n",
        "            \"description\": self.description,\n",
        "            \"status\": self.status,\n",
        "            \"priority\": self.priority,\n",
        "            \"mliw_step\": self.mliw_step,\n",
        "            \"data\": self.data_payload, # Keep payload under 'data' key for consistency\n",
        "            \"creation_time\": self.creation_time,\n",
        "            \"assigned_agent\": self.assigned_agent,\n",
        "            \"completion_time\": self.completion_time\n",
        "        }\n",
        "\n",
        "    def update_status(self, new_status: str, agent_name: Optional[str] = None):\n",
        "        \"\"\"Updates the status of the TODO item.\"\"\"\n",
        "        self.status = new_status\n",
        "        if agent_name:\n",
        "            self.assigned_agent = agent_name\n",
        "        if new_status in [\"Completed\", \"Failed\"]:\n",
        "            self.completion_time = time.time()\n",
        "        print(f\"TODO {self.task_id} status updated to {new_status}\" + (f\" by {agent_name}\" if agent_name else \"\"))\n",
        "\n",
        "# --- Specialized Agent Implementations ---\n",
        "# PhysicsAgent, OrganicChemistryAgent, MolecularBiologyAgent, CreativeAgent,\n",
        "# HypothesisAgent, DataScienceAgent, AstrophysicsAgent, RoboticsAgent,\n",
        "# EnvironmentalScienceAgent, MachineLearningAgent, ValidationAgent,\n",
        "# FractalAgent, HopfieldAgent, MultiTaskAgent, ContextAwareAgent, MultiModalAgent\n",
        "# (Keep definitions from previous correct versions, ensuring they override _execute_single_task_iteration)\n",
        "# ... Example for CollaborativeAgent shown below ...\n",
        "\n",
        "# --- Physics & Quantum Agents ---\n",
        "\n",
        "class PhysicsAgent(DFSNAgent):\n",
        "    \"\"\"Domain-focused agent for physics simulations.\"\"\"\n",
        "    def __init__(self, name: str): # Changed agent_id to name for consistency\n",
        "        super().__init__(name, math_module=PyZeroMathTorch())\n",
        "        self.epsilon = self.math_module.epsilon_0 # Use epsilon from math module\n",
        "        print(f\"PhysicsAgent {self.name} initialized.\")\n",
        "\n",
        "\n",
        "    # Overriding the base execute method from DFSNAgent\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles physics-specific tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        required_keys_map = {\n",
        "            \"fluid_dynamics\": [\"grid_size\", \"reynolds\", \"velocity\", \"pressure\", \"viscosity\", \"density\"],\n",
        "            \"electromagnetism\": [\"electric_field\", \"magnetic_field\", \"charge_density\"],\n",
        "            \"thermodynamics\": [\"temperature\", \"thermal_diffusivity\", \"time_range\"]\n",
        "        }\n",
        "\n",
        "        if task_type != \"physics_simulation\":\n",
        "             return super()._execute_single_task_iteration(task) # Delegate to base if not physics\n",
        "\n",
        "        if not action:\n",
        "            return {\"error\": \"Missing 'action' key for physics_simulation task\", \"agent\": self.name}\n",
        "\n",
        "        keys_needed = required_keys_map.get(action)\n",
        "        if keys_needed is None:\n",
        "            return {\"error\": f\"Unsupported physics action: {action}\", \"agent\": self.name}\n",
        "\n",
        "        error = self.check_task_requirements(task, keys_needed)\n",
        "        if error:\n",
        "            return error\n",
        "\n",
        "        # Execute the specific physics action\n",
        "        try:\n",
        "            if action == \"fluid_dynamics\":\n",
        "                v = self.f0z_nav_stokes(\n",
        "                    task[\"grid_size\"], task[\"reynolds\"],\n",
        "                    task[\"velocity\"], task[\"pressure\"], task[\"viscosity\"], task[\"density\"]\n",
        "                )\n",
        "                return {\"result\": {\"velocity\": v}, \"agent\": self.name}\n",
        "            elif action == \"electromagnetism\":\n",
        "                e, b = self.f0z_maxwell(\n",
        "                    task[\"electric_field\"], task[\"magnetic_field\"], task[\"charge_density\"]\n",
        "                )\n",
        "                return {\"result\": {\"electric_field\": e, \"magnetic_field\": b}, \"agent\": self.name}\n",
        "            elif action == \"thermodynamics\":\n",
        "                temp = self.f0z_heat_equation(\n",
        "                    task[\"temperature\"], task[\"thermal_diffusivity\"], task[\"time_range\"]\n",
        "                )\n",
        "                return {\"result\": {\"temperature\": temp}, \"agent\": self.name}\n",
        "        except Exception as e:\n",
        "             print(f\"Error during physics execution ({action}) for {self.name}: {e}\")\n",
        "             return {\"error\": f\"Exception during physics execution: {e}\", \"agent\": self.name}\n",
        "\n",
        "        # Fallback if action somehow wasn't handled\n",
        "        return {\"error\": f\"Physics action '{action}' not implemented after check.\", \"agent\": self.name}\n",
        "\n",
        "\n",
        "    def f0z_nav_stokes(self, grid_size, reynolds, velocity, pressure, viscosity, density, time_step=0.01, iterations=10): # Reduced iterations\n",
        "        \"\"\"Solve 1D Navier-Stokes using finite differences with F0Z stabilization.\"\"\"\n",
        "        dx = 1.0 / (grid_size - 1) if grid_size > 1 else 1.0\n",
        "        v = torch.tensor(velocity, dtype=torch.float32) if isinstance(velocity, (list, np.ndarray)) else velocity * torch.ones(grid_size, dtype=torch.float32)\n",
        "        p = torch.tensor(pressure, dtype=torch.float32) if isinstance(pressure, (list, np.ndarray)) else pressure * torch.ones(grid_size, dtype=torch.float32)\n",
        "\n",
        "        for _ in range(iterations):\n",
        "            dv_dx = torch.zeros_like(v)\n",
        "            if grid_size > 1:\n",
        "                dv_dx[1:] = (v[1:] - v[:-1]) / dx\n",
        "            convective = -v * dv_dx\n",
        "\n",
        "            dp_dx = torch.zeros_like(p)\n",
        "            if grid_size > 1:\n",
        "                dp_dx[1:] = (p[1:] - p[:-1]) / dx\n",
        "            pressure_term = -dp_dx / (density + self.math_module.epsilon_0) # Stabilize density division\n",
        "\n",
        "            d2v_dx2 = torch.zeros_like(v)\n",
        "            if grid_size > 2:\n",
        "                d2v_dx2[1:-1] = (v[2:] - 2 * v[1:-1] + v[:-2]) / (dx ** 2 + self.math_module.epsilon_0) # Stabilize dx^2\n",
        "            viscous = viscosity * d2v_dx2\n",
        "\n",
        "            dv_dt = convective + pressure_term + viscous\n",
        "            v = v + time_step * dv_dt\n",
        "            v = self.math_module.f0z_stabilize(v, system_size=grid_size)\n",
        "\n",
        "            # Simplified pressure update (placeholder for projection method)\n",
        "            if grid_size > 1 :\n",
        "                 p = p + time_step * density * dv_dx * 0.1 # Damped pseudo-correction\n",
        "\n",
        "        return v.tolist() # Return as list\n",
        "\n",
        "\n",
        "    def f0z_maxwell(self, electric_field, magnetic_field, charge_density, time_step=0.01, iterations=10): # Reduced iterations\n",
        "        \"\"\"Solve 1D Maxwell's equations using finite differences with F0Z.\"\"\"\n",
        "        grid_size = len(electric_field)\n",
        "        dx = 1.0 / (grid_size - 1) if grid_size > 1 else 1.0\n",
        "        e = torch.tensor(electric_field, dtype=torch.float32)\n",
        "        b = torch.tensor(magnetic_field, dtype=torch.float32)\n",
        "        rho = torch.tensor(charge_density, dtype=torch.float32) if isinstance(charge_density, (list, np.ndarray)) else charge_density * torch.ones(grid_size, dtype=torch.float32)\n",
        "\n",
        "        c = 1.0 # Speed of light\n",
        "\n",
        "        for _ in range(iterations):\n",
        "            db_dx = torch.zeros_like(b)\n",
        "            if grid_size > 1:\n",
        "                db_dx[1:] = (b[1:] - b[:-1]) / dx\n",
        "            # Ampere-Maxwell Law: dE/dt = c*curl(B) - J (simplified 1D: curl(B) -> dB/dx, J=rho*v -> approx rho)\n",
        "            de_dt = c * db_dx - rho # Simplified current term J = rho\n",
        "\n",
        "            de_dx = torch.zeros_like(e)\n",
        "            if grid_size > 1:\n",
        "                de_dx[1:] = (e[1:] - e[:-1]) / dx\n",
        "            # Faraday's Law: dB/dt = -c*curl(E) (simplified 1D: curl(E) -> dE/dx)\n",
        "            db_dt = -c * de_dx\n",
        "\n",
        "            e = e + time_step * de_dt\n",
        "            b = b + time_step * db_dt\n",
        "\n",
        "            e = self.math_module.f0z_stabilize(e, system_size=grid_size)\n",
        "            b = self.math_module.f0z_stabilize(b, system_size=grid_size)\n",
        "\n",
        "        return e.tolist(), b.tolist()\n",
        "\n",
        "\n",
        "    def f0z_heat_equation(self, temperature, thermal_diffusivity, time_range, iterations=10): # Reduced iterations\n",
        "        \"\"\"Solve 1D heat equation using finite differences with F0Z.\"\"\"\n",
        "        grid_size = len(temperature)\n",
        "        dx = 1.0 / (grid_size - 1) if grid_size > 1 else 1.0\n",
        "        alpha = thermal_diffusivity\n",
        "        # Ensure stability: dt <= dx^2 / (2 * alpha)\n",
        "        dt = min(time_range) / iterations if time_range else 0.01 # Simplified dt calculation\n",
        "        stable_dt = (dx**2 / (2 * alpha + self.math_module.epsilon_0)) * 0.9\n",
        "        dt = min(dt, stable_dt) if alpha > 0 else dt # Ensure dt respects stability\n",
        "        if dt <= 0: dt = 0.001 # Fallback dt\n",
        "\n",
        "\n",
        "        T = torch.tensor(temperature, dtype=torch.float32)\n",
        "\n",
        "        steps = iterations # Use fixed iterations instead of time_range for simplicity here\n",
        "\n",
        "        for _ in range(steps):\n",
        "            d2T_dx2 = torch.zeros_like(T)\n",
        "            if grid_size > 2:\n",
        "                d2T_dx2[1:-1] = (T[2:] - 2 * T[1:-1] + T[:-2]) / (dx ** 2 + self.math_module.epsilon_0)\n",
        "            T = T + alpha * dt * d2T_dx2\n",
        "            T = self.math_module.f0z_stabilize(T, system_size=grid_size)\n",
        "\n",
        "        return T.tolist()\n",
        "\n",
        "    def get_system_size(self):\n",
        "        \"\"\"Estimate system size for stabilization adjustments.\"\"\"\n",
        "        # Example: could be grid size in simulations\n",
        "        return 100 # Placeholder\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        # Example: Could share current simulation parameters or field states\n",
        "        return {\"epsilon\": self.epsilon.item()} # Share current epsilon\n",
        "\n",
        "# --- Specialized Agent Implementations (Continued) ---\n",
        "\n",
        "class QuantumAgent(DFSNAgent):\n",
        "    \"\"\"Quantum agent for simulations and quantum algorithms.\"\"\"\n",
        "    def __init__(self, name: str): # Removed agent_id, domain - use name and task type\n",
        "        super().__init__(name)\n",
        "        self.epsilon = self.math_module.epsilon_0\n",
        "        # Each QuantumAgent might have its own simulator or share one via the bridge\n",
        "        # For simplicity, let's assume it uses a bridge provided by the manager\n",
        "        self.quantum_bridge: Optional[ZSGQuantumBridge] = None # To be set by manager potentially\n",
        "        print(f\"QuantumAgent {self.name} initialized.\")\n",
        "\n",
        "    def set_bridge(self, bridge: ZSGQuantumBridge):\n",
        "         \"\"\"Allows the manager to inject the shared quantum bridge.\"\"\"\n",
        "         self.quantum_bridge = bridge\n",
        "         print(f\"{self.name} received quantum bridge.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles quantum-specific tasks.\"\"\"\n",
        "        if not self.quantum_bridge or not self.quantum_bridge.simulator:\n",
        "             return {\"error\": f\"{self.name} does not have a configured quantum bridge/simulator.\", \"agent\": self.name}\n",
        "\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        required_keys_map = {\n",
        "            \"quantum_field\": [\"amplitude\", \"momentum\"],\n",
        "            \"grover_search\": [\"n_qubits\", \"target\"],\n",
        "            \"shor_factor\": [\"number\", \"n_qubits\"],\n",
        "            \"quantum_circuit\": [\"gates\", \"n_qubits\"],\n",
        "            \"quantum_game\": [\"game_data\"] # Example\n",
        "        }\n",
        "\n",
        "        # Check if task type suggests quantum work\n",
        "        if not any(qt in task_type for qt in [\"quantum\", \"grover\", \"shor\", \"circuit\", \"qft\"]):\n",
        "            return super()._execute_single_task_iteration(task) # Delegate if not a quantum task\n",
        "\n",
        "        if not action:\n",
        "             # Infer action from type if possible\n",
        "             if \"grover\" in task_type: action = \"grover_search\"\n",
        "             elif \"shor\" in task_type: action = \"shor_factor\"\n",
        "             elif \"circuit\" in task_type: action = \"quantum_circuit\"\n",
        "             elif \"field\" in task_type: action = \"quantum_field\"\n",
        "             elif \"game\" in task_type: action = \"quantum_game\"\n",
        "             else: return {\"error\": \"Missing 'action' key for quantum task\", \"agent\": self.name}\n",
        "             task['action'] = action # Add inferred action back to task dict\n",
        "\n",
        "        keys_needed = required_keys_map.get(action)\n",
        "        if keys_needed is None:\n",
        "            return {\"error\": f\"Unsupported quantum action: {action}\", \"agent\": self.name}\n",
        "\n",
        "        error = self.check_task_requirements(task, keys_needed)\n",
        "        if error:\n",
        "            return error\n",
        "\n",
        "        # Ensure simulator size matches task requirements if specified\n",
        "        n_qubits_req = task.get(\"n_qubits\")\n",
        "        if n_qubits_req and n_qubits_req != self.quantum_bridge.simulator.n_qubits:\n",
        "             # This ideally requires recreating the simulator or bridge, which is complex.\n",
        "             # For now, return an error or warning.\n",
        "             print(f\"Warning: Task requires {n_qubits_req} qubits, but simulator has {self.quantum_bridge.simulator.n_qubits}. Results may be incorrect.\")\n",
        "             # Or: return {\"error\": f\"Simulator qubit count mismatch\", \"agent\": self.name}\n",
        "\n",
        "\n",
        "        # Execute the specific quantum action\n",
        "        try:\n",
        "            if action == \"quantum_field\":\n",
        "                # Note: f0z_quantum_field was defined inside QuantumAgent in the prompt, but doesn't use quantum gates directly.\n",
        "                # It seems more like a classical simulation using F0Z. Let's keep it here but note its nature.\n",
        "                state = self.f0z_quantum_field_simulation(task[\"amplitude\"], task[\"momentum\"])\n",
        "                return {\"result\": {\"state\": state}, \"agent\": self.name, \"task_type\": \"quantum_field_simulation\"} # Clarify it's a simulation\n",
        "\n",
        "            elif action == \"grover_search\":\n",
        "                 if task[\"target\"] >= 2**self.quantum_bridge.simulator.n_qubits:\n",
        "                      return {\"error\": f\"Grover target {task['target']} out of range for {self.quantum_bridge.simulator.n_qubits} qubits.\", \"agent\": self.name}\n",
        "                 result = self.f0z_grover_search(task[\"n_qubits\"], task[\"target\"]) # n_qubits check done above\n",
        "                 return {\"result\": result, \"agent\": self.name}\n",
        "\n",
        "            elif action == \"shor_factor\":\n",
        "                 result = self.f0z_shor_factor(task[\"number\"], task[\"n_qubits\"])\n",
        "                 return {\"result\": result, \"agent\": self.name}\n",
        "\n",
        "            elif action == \"quantum_circuit\":\n",
        "                 final_state = self.f0z_quantum_circuit(task[\"gates\"], task[\"n_qubits\"])\n",
        "                 # Decode the final state for a meaningful result\n",
        "                 decoded = self.quantum_bridge.decode(quantum_state=final_state.tolist())\n",
        "                 return {\"result\": {\"final_state\": final_state.tolist(), \"decoded_info\": decoded}, \"agent\": self.name}\n",
        "\n",
        "            elif action == \"quantum_game\":\n",
        "                 # Example game logic: encode data, evolve state, decode result\n",
        "                 initial_state = self.quantum_bridge.encode(task[\"game_data\"])\n",
        "                 # Apply some game-specific gates (e.g., random rotations)\n",
        "                 for q in range(self.quantum_bridge.simulator.n_qubits):\n",
        "                     self.quantum_bridge.simulator.ry_gate(random.random() * np.pi, q)\n",
        "                 game_result_state = self.quantum_bridge.simulator.state.tolist()\n",
        "                 decoded_outcome = self.quantum_bridge.decode(quantum_state=game_result_state)\n",
        "                 return {\"result\": {\"game_outcome\": decoded_outcome}, \"agent\": self.name}\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error during quantum execution ({action}) for {self.name}: {e}\")\n",
        "             # traceback.print_exc() # Uncomment for detailed debugging\n",
        "             return {\"error\": f\"Exception during quantum execution: {e}\", \"agent\": self.name}\n",
        "\n",
        "        # Fallback\n",
        "        return {\"error\": f\"Quantum action '{action}' not implemented.\", \"agent\": self.name}\n",
        "\n",
        "\n",
        "    # --- Quantum Algorithm Implementations (using self.quantum_bridge.simulator) ---\n",
        "\n",
        "    def f0z_quantum_field_simulation(self, field_amplitude, momentum, time_step=0.01, iterations=10):\n",
        "        \"\"\"Simulate 1D quantum field evolution classically using F0Z (moved from initial prompt).\"\"\"\n",
        "        sim_state_vector = np.array([0.0]) # Placeholder, this needs a proper QFT simulation setup\n",
        "\n",
        "        try:\n",
        "            N = 64 # Discretized grid size\n",
        "            dx = 1.0 / (N - 1) if N > 1 else 1.0\n",
        "            mass = 1.0\n",
        "\n",
        "            # Initial field configuration\n",
        "            phi = np.array(field_amplitude) if isinstance(field_amplitude, (list, np.ndarray)) else field_amplitude * np.ones(N)\n",
        "            pi = np.array(momentum) if isinstance(momentum, (list, np.ndarray)) else momentum * np.ones(N)\n",
        "\n",
        "            # Represent state (simplified: using classical arrays, not qubit state vector)\n",
        "            current_phi = phi\n",
        "            current_pi = pi\n",
        "\n",
        "            for _ in range(iterations):\n",
        "                 # Simplified update using F0Z-stabilized derivatives (conceptual)\n",
        "                 d2phi_dx2 = F0ZAlgebra.f0z_gradient(F0ZAlgebra.f0z_gradient(current_phi)) # Approx Laplacian\n",
        "                 # Klein-Gordon equation: d^2phi/dt^2 = d^2phi/dx^2 - m^2*phi\n",
        "                 dphi_dt = current_pi\n",
        "                 dpi_dt = d2phi_dx2 - (mass**2) * current_phi\n",
        "\n",
        "                 # Stabilize updates\n",
        "                 dphi_dt_s = self.math_module.f0z_stabilize(torch.tensor(dphi_dt)).numpy()\n",
        "                 dpi_dt_s = self.math_module.f0z_stabilize(torch.tensor(dpi_dt)).numpy()\n",
        "\n",
        "                 current_phi = current_phi + time_step * dphi_dt_s\n",
        "                 current_pi = current_pi + time_step * dpi_dt_s\n",
        "\n",
        "                 # Stabilize state fields\n",
        "                 current_phi = self.math_module.f0z_stabilize(torch.tensor(current_phi)).numpy()\n",
        "                 current_pi = self.math_module.f0z_stabilize(torch.tensor(current_pi)).numpy()\n",
        "\n",
        "\n",
        "            sim_state_vector = current_phi + 1j * current_pi # Combine into complex field value\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in f0z_quantum_field_simulation: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "        return {\"phi\": current_phi.tolist(), \"pi\": current_pi.tolist()} # Return field components\n",
        "\n",
        "\n",
        "    def f0z_grover_search(self, n_qubits, target):\n",
        "        \"\"\"Grover's quantum search algorithm using the simulator.\"\"\"\n",
        "        sim = self.quantum_bridge.simulator\n",
        "        if n_qubits != sim.n_qubits:\n",
        "             print(f\"Warning: Grover using simulator's {sim.n_qubits} qubits, not requested {n_qubits}.\")\n",
        "             # Ideally, resize simulator or throw error\n",
        "        N = 2**sim.n_qubits\n",
        "\n",
        "        # 1. Initial Superposition\n",
        "        sim.state.fill(0)\n",
        "        sim.state[0] = 1.0 # Start from |0>\n",
        "        for q in range(sim.n_qubits):\n",
        "             sim.h_gate(q)\n",
        "\n",
        "        # Oracle function for FDO gate\n",
        "        def oracle_func(index):\n",
        "            return 1 if index == target else 0\n",
        "\n",
        "        # Number of iterations optimal for Grover\n",
        "        steps = int(np.pi * np.sqrt(N) / 4.0) if N > 0 else 0\n",
        "        print(f\"  Grover: Running {steps} iterations for {sim.n_qubits} qubits.\")\n",
        "\n",
        "        for _ in range(steps):\n",
        "             # 2. Apply Oracle (using FDO)\n",
        "             sim.fdo(oracle_func)\n",
        "             # 3. Apply Diffusion Operator\n",
        "             # Diffusion = H^{\\otimes n} (2|0><0| - I) H^{\\otimes n}\n",
        "             # Simplified implementation via sim.diffusion() assumed correct\n",
        "             for q in range(sim.n_qubits): sim.h_gate(q)\n",
        "             sim.fdo(lambda i: 1 if i == 0 else 0) # 2|0><0| - I equivalent phase flip\n",
        "             for q in range(sim.n_qubits): sim.h_gate(q)\n",
        "             # Apply F0Z stabilization after each full step\n",
        "             sim.state = self.math_module.f0z_stabilize(torch.tensor(sim.state, dtype=torch.complex64), system_size=N).numpy()\n",
        "\n",
        "\n",
        "        # 4. Measure\n",
        "        probs = np.abs(sim.state)**2\n",
        "        probs = self.math_module.f0z_stabilize(torch.tensor(probs)).numpy()\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        found_prob = probs[target] if target < len(probs) else 0\n",
        "        print(f\"  Grover: Probability of target state |{target}> = {found_prob:.4f}\")\n",
        "        return {\"final_probs\": probs.tolist(), \"target\": target, \"target_probability\": found_prob}\n",
        "\n",
        "\n",
        "    def f0z_shor_factor(self, number, n_qubits):\n",
        "        \"\"\"Shor's algorithm period finding (simplified simulation).\"\"\"\n",
        "        sim = self.quantum_bridge.simulator\n",
        "        if n_qubits != sim.n_qubits:\n",
        "             print(f\"Warning: Shor using simulator's {sim.n_qubits} qubits, not requested {n_qubits}.\")\n",
        "             # Need enough qubits for the number N. Let 2^n_qubits >= number^2\n",
        "             required_qubits = int(np.ceil(2 * np.log2(number)))\n",
        "             if sim.n_qubits < required_qubits:\n",
        "                  return {\"error\": f\"Shor needs at least {required_qubits} qubits for N={number}, simulator has {sim.n_qubits}\"}\n",
        "\n",
        "        N = number # The number to factor\n",
        "        # Classical part: Choose random 'a', check GCD\n",
        "        a = random.randint(2, N - 1)\n",
        "        g = np.gcd(a, N)\n",
        "        if g != 1:\n",
        "            print(f\"  Shor: Lucky guess! Found factor classically: {g}\")\n",
        "            return {\"factors\": [g, N // g], \"period\": None, \"base_a\": a}\n",
        "\n",
        "        print(f\"  Shor: Trying to factor {N} with base a={a}\")\n",
        "\n",
        "        # Quantum Part: Period Finding\n",
        "        # 1. Initialize registers (simulate using full state vector)\n",
        "        # Requires two registers, but we simulate the combined state evolution.\n",
        "        # Let simulator represent the second register (result of modular exponentiation).\n",
        "        # First register (input) state is implicitly handled by QFT later.\n",
        "        sim_size = sim.n_qubits\n",
        "        M = 2**sim_size # Size of simulator state space\n",
        "\n",
        "        # State preparation (|0> in first reg, |1> in second reg mapped implicitly)\n",
        "        sim.state.fill(0)\n",
        "        sim.state[1 % M] = 1.0 # Start second register in |1> (index 1)\n",
        "\n",
        "        # 2. Apply Quantum Modular Exponentiation (simulated classically)\n",
        "        # U|x>|y> = |x>|y * a^x mod N>\n",
        "        # We simulate the effect on the second register for an implicit superposition in the first.\n",
        "        # This is a major simplification. A real simulation applies QFT to the first register.\n",
        "        # Let's simulate the QFT result directly.\n",
        "        print(\"  Shor: Simulating Quantum Fourier Transform result...\")\n",
        "        # The QFT peaks at multiples of M/r, where r is the period.\n",
        "        # Need to find the period 'r' classically first for this simulation.\n",
        "        r = 1\n",
        "        while pow(a, r, N) != 1:\n",
        "            r += 1\n",
        "        print(f\"  Shor: Found period r={r} classically for simulation.\")\n",
        "\n",
        "        if r % 2 != 0:\n",
        "             print(\"  Shor: Period r is odd. Algorithm fails for this 'a'.\")\n",
        "             return {\"period\": r, \"factors\": \"Failed (odd period)\", \"base_a\": a}\n",
        "\n",
        "        # Simulate QFT measurement outcome probabilities peaking near k*M/r\n",
        "        sim.state.fill(0)\n",
        "        for k in range(r): # Superposition of peaks\n",
        "             peak_index = int(round(k * M / r)) % M\n",
        "             sim.state[peak_index] = 1.0 / np.sqrt(r) # Equal amplitude for peaks\n",
        "\n",
        "        sim.state = self.math_module.f0z_stabilize(torch.tensor(sim.state, dtype=torch.complex64), system_size=M).numpy()\n",
        "        probs = np.abs(sim.state)**2\n",
        "\n",
        "        # 3. Measure (get a peak related to the period)\n",
        "        measured_val = np.random.choice(M, p=(probs/probs.sum())) # Measure one outcome\n",
        "        print(f\"  Shor: Simulated measurement = {measured_val} (Expecting peak near k*M/r)\")\n",
        "\n",
        "        # 4. Classical Post-processing (Continued Fraction Algorithm - simplified)\n",
        "        # Try to deduce r from measurement c = k*M/r\n",
        "        # This part is complex, we'll use the known 'r' for now to find factors\n",
        "        print(f\"  Shor: Using known period r={r} for post-processing.\")\n",
        "        factor1 = np.gcd(pow(a, r//2, N) - 1, N)\n",
        "        factor2 = np.gcd(pow(a, r//2, N) + 1, N)\n",
        "\n",
        "        factors = []\n",
        "        if factor1 != 1 and factor1 != N: factors.append(factor1)\n",
        "        if factor2 != 1 and factor2 != N: factors.append(factor2)\n",
        "        # Add the corresponding co-factors\n",
        "        if factors:\n",
        "             cofactor = N // factors[0]\n",
        "             if cofactor != 1 and cofactor != N and cofactor not in factors:\n",
        "                  factors.append(cofactor)\n",
        "             factors = sorted(list(set(factors))) # Unique sorted factors\n",
        "\n",
        "        if factors:\n",
        "             print(f\"  Shor: Found factors: {factors}\")\n",
        "             return {\"period\": r, \"factors\": factors, \"base_a\": a}\n",
        "        else:\n",
        "             print(\"  Shor: Failed to find non-trivial factors from this 'a' and 'r'.\")\n",
        "             return {\"period\": r, \"factors\": \"Failed (no non-trivial factors found)\", \"base_a\": a}\n",
        "\n",
        "\n",
        "    def f0z_quantum_circuit(self, gates: List[Dict], n_qubits: int):\n",
        "        \"\"\"Executes a quantum circuit defined by a list of gates.\"\"\"\n",
        "        sim = self.quantum_bridge.simulator\n",
        "        if n_qubits != sim.n_qubits:\n",
        "            print(f\"Warning: Circuit using simulator's {sim.n_qubits} qubits, not requested {n_qubits}.\")\n",
        "\n",
        "        # Initialize state to |0...0>\n",
        "        sim.state.fill(0)\n",
        "        sim.state[0] = 1.0\n",
        "\n",
        "        print(f\"  Executing quantum circuit with {len(gates)} gates on {sim.n_qubits} qubits...\")\n",
        "        for i, gate_info in enumerate(gates):\n",
        "            gate_type = gate_info.get(\"type\", \"\").upper()\n",
        "            try:\n",
        "                if gate_type == \"H\":\n",
        "                    sim.h_gate(gate_info[\"qubits\"][0])\n",
        "                elif gate_type == \"X\":\n",
        "                    sim.x_gate(gate_info[\"qubits\"][0])\n",
        "                elif gate_type == \"Y\":\n",
        "                     sim.y_gate(gate_info[\"qubits\"][0])\n",
        "                elif gate_type == \"Z\":\n",
        "                     sim.z_gate(gate_info[\"qubits\"][0])\n",
        "                elif gate_type == \"RZ\":\n",
        "                    sim.rz_gate(gate_info[\"angle\"], gate_info[\"qubit\"])\n",
        "                elif gate_type == \"RY\":\n",
        "                    sim.ry_gate(gate_info[\"angle\"], gate_info[\"qubit\"])\n",
        "                elif gate_type == \"CNOT\" or gate_type == \"CX\":\n",
        "                    sim.cnot_gate(gate_info[\"control\"], gate_info[\"target\"])\n",
        "                elif gate_type == \"CZ\":\n",
        "                    sim.cz_gate(gate_info[\"control\"], gate_info[\"target\"])\n",
        "                else:\n",
        "                    print(f\"    Warning: Skipping unknown gate type '{gate_type}' at step {i}.\")\n",
        "                    continue\n",
        "\n",
        "                # Apply F0Z stabilization periodically or after each gate\n",
        "                if (i + 1) % 5 == 0 or i == len(gates) - 1: # Stabilize every 5 gates or at the end\n",
        "                    sim.state = self.math_module.f0z_stabilize(torch.tensor(sim.state, dtype=torch.complex64), system_size=2**sim.n_qubits).numpy()\n",
        "                    # Optional: Normalize after stabilization if needed\n",
        "                    norm = LA.norm(sim.state)\n",
        "                    if norm > 1e-9 : sim.state /= norm\n",
        "\n",
        "            except KeyError as e:\n",
        "                 print(f\"    Error: Missing key {e} in gate definition at step {i}: {gate_info}\")\n",
        "                 raise # Re-raise the error to stop execution\n",
        "            except Exception as e:\n",
        "                 print(f\"    Error applying gate at step {i} ({gate_type}): {e}\")\n",
        "                 raise # Re-raise\n",
        "\n",
        "        print(\"  Quantum circuit execution complete.\")\n",
        "        return sim.state # Return the final state vector\n",
        "\n",
        "\n",
        "    def share_state(self, peer: Agent):\n",
        "        \"\"\"Share quantum state vector with a peer.\"\"\"\n",
        "        if not isinstance(peer, QuantumAgent) or not self.quantum_bridge:\n",
        "            print(f\"{self.name} cannot share quantum state with {peer.name}\")\n",
        "            return\n",
        "\n",
        "        state_vector_list = self.quantum_bridge.simulator.state.tolist() if self.quantum_bridge.simulator else None\n",
        "        state_to_share = {\n",
        "            \"performance_history\": self.performance_history[-10:],\n",
        "            \"domain_data\": {\"quantum_state_vector\": state_vector_list}\n",
        "        }\n",
        "        print(f\"{self.name} sharing quantum state ({len(state_vector_list)} amplitudes) with {peer.name}\")\n",
        "        peer.receive_state(state_to_share)\n",
        "\n",
        "\n",
        "    def receive_state(self, state: Dict):\n",
        "        \"\"\"Receive and potentially update quantum state from a peer.\"\"\"\n",
        "        super().receive_state(state) # Handle performance history etc.\n",
        "        if self.quantum_bridge and \"domain_data\" in state and \"quantum_state_vector\" in state[\"domain_data\"]:\n",
        "            received_state = state[\"domain_data\"][\"quantum_state_vector\"]\n",
        "            if received_state is not None and len(received_state) == 2**self.quantum_bridge.simulator.n_qubits:\n",
        "                # Option 1: Overwrite state\n",
        "                # self.quantum_bridge.simulator.state = np.array(received_state, dtype=complex)\n",
        "                # Option 2: Average states (careful, may not be physically meaningful)\n",
        "                current_state = self.quantum_bridge.simulator.state\n",
        "                avg_state = 0.5 * (current_state + np.array(received_state, dtype=complex))\n",
        "                norm = LA.norm(avg_state)\n",
        "                if norm > 1e-9 : self.quantum_bridge.simulator.state = avg_state / norm\n",
        "\n",
        "                print(f\"{self.name} updated quantum state based on peer (averaged).\")\n",
        "            else:\n",
        "                 print(f\"{self.name} received invalid quantum state from peer.\")\n",
        "\n",
        "\n",
        "# --- Multi-Task and Contextual Agents (Refined) ---\n",
        "\n",
        "class MultiTaskAgent(DFSNAgent):\n",
        "    \"\"\"Agent capable of handling multiple predefined tasks based on state.\"\"\"\n",
        "    def __init__(self, name: str, task1: Any, task2: Any): # Tasks can be objects with a 'run' method\n",
        "        super().__init__(name)\n",
        "        self.task1 = task1\n",
        "        self.task2 = task2\n",
        "        print(f\"MultiTaskAgent {self.name} initialized with tasks: {type(task1).__name__}, {type(task2).__name__}\")\n",
        "\n",
        "    # Override _execute_single_task_iteration to handle the multi-task logic\n",
        "    def _execute_single_task_iteration(self, task: Optional[Dict]=None) -> Dict: # Task dict might be unused if tasks are predefined\n",
        "        \"\"\"Executes one of the predefined tasks based on engagement state.\"\"\"\n",
        "        print(f\"{self.name} deciding task based on engagement state: {self.engagement_state}\")\n",
        "        # Use AIW to manage iterations if needed, but select task first\n",
        "        if self.engagement_state > 3:\n",
        "             print(f\"  Engagement > 3, executing Task 1 ({type(self.task1).__name__})\")\n",
        "             if hasattr(self.task1, 'run'):\n",
        "                 result = self.task1.run()\n",
        "                 # AIW needs a task dict, we simulate one\n",
        "                 simulated_task = {\"type\": \"multitask_task1\", \"complexity\": 6}\n",
        "                 # We need to wrap the execution within the AIW framework if we want iterative refinement\n",
        "                 # This structure is a bit awkward. Maybe AIW should take the function to run?\n",
        "                 # Simplified: Just run the task directly and return result.\n",
        "                 return {\"result\": result, \"agent\": self.name, \"executed_task\": \"task1\"}\n",
        "             else: return {\"error\": \"Task 1 has no run method\", \"agent\": self.name}\n",
        "        else:\n",
        "             print(f\"  Engagement <= 3, executing Task 2 ({type(self.task2).__name__})\")\n",
        "             if hasattr(self.task2, 'run'):\n",
        "                 result = self.task2.run()\n",
        "                 return {\"result\": result, \"agent\": self.name, \"executed_task\": \"task2\"}\n",
        "             else: return {\"error\": \"Task 2 has no run method\", \"agent\": self.name}\n",
        "\n",
        "class ContextAwareAgent(DFSNAgent):\n",
        "    \"\"\"Agent that processes sequences with context awareness.\"\"\"\n",
        "    def __init__(self, name: str, context_window: int = 5):\n",
        "        super().__init__(name)\n",
        "        self.context_window = context_window\n",
        "        print(f\"ContextAwareAgent {self.name} initialized with window size {self.context_window}.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles tasks requiring context-aware processing.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"contextual_processing\":\n",
        "            if action == \"process_sequence\":\n",
        "                error = self.check_task_requirements(task, [\"sequence\"])\n",
        "                if error: return error\n",
        "                processed_tokens = self.process_tokens(task[\"sequence\"])\n",
        "                return {\"result\": {\"processed_tokens_count\": len(processed_tokens)}, \"agent\": self.name}\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate\n",
        "\n",
        "\n",
        "    def process_tokens(self, token_sequence: List[str]) -> List[Dict]:\n",
        "        \"\"\"Processes a sequence of tokens, considering context.\"\"\"\n",
        "        print(f\"Processing token sequence (length {len(token_sequence)}) with context window {self.context_window}...\")\n",
        "        processed_results = []\n",
        "        for i, token in enumerate(token_sequence):\n",
        "            # Determine context window [start, end)\n",
        "            start_index = max(0, i - self.context_window)\n",
        "            # Context includes current token up to index i\n",
        "            current_context = token_sequence[start_index : i+1]\n",
        "            # Handle the token with its context\n",
        "            processed_info = self.handle_token_with_context(token, current_context)\n",
        "            processed_results.append({\"token\": token, \"context\": current_context, \"info\": processed_info})\n",
        "\n",
        "        print(\"Token sequence processing complete.\")\n",
        "        # Adjust flow state based on sequence complexity (e.g., length, variability)\n",
        "        complexity = len(token_sequence) / 10.0 # Simple complexity measure\n",
        "        perf = len(processed_results) / len(token_sequence) if token_sequence else 0.0 # Performance = completion rate\n",
        "        self.adjust_flow_state(complexity, perf)\n",
        "\n",
        "        return processed_results\n",
        "\n",
        "    def handle_token_with_context(self, token: str, context: List[str]) -> str:\n",
        "        \"\"\"Placeholder logic for handling a token given its context.\"\"\"\n",
        "        # Example: Identify if token is novel in context or follows a pattern\n",
        "        context_str = \" \".join(context[:-1]) # Context before current token\n",
        "        # print(f\"  Processing token: '{token}' with context: '[...{context_str[-30:]}]'\")\n",
        "        if token in context[:-1]:\n",
        "             return \"repeated_in_context\"\n",
        "        elif len(context) > 2 and token.isdigit() and context[-2].isdigit():\n",
        "             return \"continuation_of_numbers\"\n",
        "        else:\n",
        "             return \"standard_processing\"\n",
        "\n",
        "\n",
        "class MultiModalAgent(DFSNAgent):\n",
        "    \"\"\"Agent capable of processing inputs from multiple modalities.\"\"\"\n",
        "    def __init__(self, name: str, modalities: List[str]):\n",
        "        super().__init__(name)\n",
        "        self.modalities = modalities\n",
        "        print(f\"MultiModalAgent {self.name} initialized. Handles: {', '.join(modalities)}\")\n",
        "        # Placeholder for modality-specific models (e.g., image classifier, text processor)\n",
        "        self.text_processor = lambda text: f\"Processed text: {text[:30]}...\"\n",
        "        self.image_processor = lambda img: f\"Processed image of shape: {img.shape}\" if isinstance(img, np.ndarray) else \"Processed non-numpy image\"\n",
        "        self.audio_processor = lambda aud: f\"Processed audio data (length {len(aud)} samples)\" if isinstance(aud, (list, np.ndarray)) else \"Processed non-array audio\"\n",
        "        self.video_processor = lambda vid: f\"Processed video data (frames: {len(vid)})\" if isinstance(vid, (list, np.ndarray)) else \"Processed non-array video\"\n",
        "\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles multimodal processing tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"multimodal_input\":\n",
        "             error = self.check_task_requirements(task, [\"input_data\"]) # Input data is expected to be a dict\n",
        "             if error: return error\n",
        "             if not isinstance(task[\"input_data\"], dict):\n",
        "                 return {\"error\": \"input_data must be a dictionary for multimodal agent\", \"agent\": self.name}\n",
        "\n",
        "             results = self.process_input(task[\"input_data\"])\n",
        "             # Adjust flow based on complexity (e.g., number of modalities, data size)\n",
        "             complexity = len(task[\"input_data\"].keys()) # Simple: complexity = number of modalities present\n",
        "             perf = 1.0 # Assume success if no errors\n",
        "             self.adjust_flow_state(complexity, perf)\n",
        "             return {\"result\": results, \"agent\": self.name}\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate\n",
        "\n",
        "    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, str]:\n",
        "        \"\"\"Processes input data based on supported modalities.\"\"\"\n",
        "        print(f\"{self.name} processing multimodal input...\")\n",
        "        results = {}\n",
        "        for modality in self.modalities:\n",
        "            data = input_data.get(modality) # Get data for the modality\n",
        "            if data is not None:\n",
        "                print(f\"  Processing modality: {modality}\")\n",
        "                handler = getattr(self, f\"handle_{modality}\", None)\n",
        "                if handler and callable(handler):\n",
        "                    try:\n",
        "                         result_info = handler(data)\n",
        "                         results[modality] = result_info\n",
        "                    except Exception as e:\n",
        "                         print(f\"    Error processing {modality}: {e}\")\n",
        "                         results[modality] = f\"Error: {e}\"\n",
        "                else:\n",
        "                     print(f\"    No handler found for modality: {modality}\")\n",
        "                     results[modality] = \"No handler available\"\n",
        "            # else: print(f\"  Modality '{modality}' not present in input.\")\n",
        "        print(\"Multimodal processing complete.\")\n",
        "        return results\n",
        "\n",
        "    # --- Modality Handlers ---\n",
        "    def handle_text(self, text_data: str) -> str:\n",
        "        # print(f\"  Handling text data...\")\n",
        "        return self.text_processor(text_data)\n",
        "\n",
        "    def handle_image(self, image_data: Any) -> str:\n",
        "        # print(f\"  Handling image data...\")\n",
        "        # Add actual image processing logic here\n",
        "        return self.image_processor(image_data)\n",
        "\n",
        "    def handle_audio(self, audio_data: Any) -> str:\n",
        "        # print(f\"  Handling audio data...\")\n",
        "        return self.audio_processor(audio_data)\n",
        "\n",
        "    def handle_video(self, video_data: Any) -> str:\n",
        "        # print(f\"  Handling video data...\")\n",
        "        return self.video_processor(video_data)\n",
        "\n",
        "\n",
        "# --- Other Specialized Agents ---\n",
        "\n",
        "class MemoryAgent(DFSNAgent):\n",
        "    \"\"\"Agent focused on managing and utilizing long-term memory.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.memory_system = MemorySystem() # Each agent could have its own or share one\n",
        "        print(f\"MemoryAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles memory-related tasks or uses memory for other tasks.\"\"\"\n",
        "        action = task.get(\"action\")\n",
        "        task_type = task.get(\"type\")\n",
        "\n",
        "        if task_type == \"memory_task\":\n",
        "            if action == \"store\":\n",
        "                 error = self.check_task_requirements(task, [\"key\", \"data\", \"memory_type\"])\n",
        "                 if error: return error\n",
        "                 self.memory_system.store_memory(task[\"data\"], task[\"memory_type\"], task[\"key\"])\n",
        "                 return {\"status\": \"stored\", \"agent\": self.name}\n",
        "            elif action == \"retrieve\":\n",
        "                 error = self.check_task_requirements(task, [\"key\", \"memory_type\"])\n",
        "                 if error: return error\n",
        "                 data = self.memory_system.retrieve_memory(task[\"memory_type\"], task[\"key\"])\n",
        "                 return {\"result\": data, \"status\": \"retrieved\", \"agent\": self.name}\n",
        "            elif action == \"manage_long_horizon\":\n",
        "                 # Example from prompt: process steps using memory\n",
        "                 error = self.check_task_requirements(task, [\"steps\"])\n",
        "                 if error: return error\n",
        "                 results = self.manage_long_horizon_task(task[\"steps\"])\n",
        "                 return {\"result\": results, \"agent\": self.name}\n",
        "\n",
        "        # If not a memory task, use AIW (which calls base__execute_single_task_iteration if not overridden)\n",
        "        return super()._execute_single_task_iteration(task)\n",
        "\n",
        "    def base_execute_task(self, task: Dict) -> Dict:\n",
        "         \"\"\"Base execution for MemoryAgent - maybe retrieves relevant memory first.\"\"\"\n",
        "         print(f\"{self.name} (MemoryAgent Base): Executing {task.get('type')}\")\n",
        "         # Example: Retrieve memory related to the task description\n",
        "         relevant_mem = self.memory_system.retrieve_memory(\n",
        "             memory_type='long',\n",
        "             criteria=lambda item: isinstance(item, dict) and task.get('description', '') in item.get('description', '')\n",
        "         )\n",
        "         if relevant_mem:\n",
        "             print(f\"  Found relevant memory: {relevant_mem}\")\n",
        "             # Use memory in task execution (placeholder)\n",
        "             task['context_from_memory'] = relevant_mem\n",
        "\n",
        "         # Simulate work\n",
        "         time.sleep(0.02 * task.get('complexity', 1.0))\n",
        "         return {\"result\": f\"Memory-enhanced execution for {task.get('type')}\", \"agent\": self.name}\n",
        "\n",
        "\n",
        "    def manage_long_horizon_task(self, steps: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Processes a sequence of steps, using memory when required.\"\"\"\n",
        "        step_results = []\n",
        "        print(f\"{self.name} managing long horizon task with {len(steps)} steps.\")\n",
        "        for i, step in enumerate(steps):\n",
        "             print(f\"  Step {i+1}: {step.getexecute_single_task_iteration}(task)\") # Delegate\n",
        "\n",
        "\n",
        "    def process_tokens(self, token_sequence: List[str]) -> List[Dict]:\n",
        "        \"\"\"Processes a sequence of tokens, considering context.\"\"\"\n",
        "        print(f\"Processing token sequence (length {len(token_sequence)}) with context window {self.context_window}...\")\n",
        "        processed_results = []\n",
        "        for i, token in enumerate(token_sequence):\n",
        "            # Determine context window [start, end)\n",
        "            start_index = max(0, i - self.context_window)\n",
        "            # Context includes current token up to index i\n",
        "            current_context = token_sequence[start_index : i+1]\n",
        "            # Handle the token with its context\n",
        "            processed_info = self.handle_token_with_context(token, current_context)\n",
        "            processed_results.append({\"token\": token, \"context\": current_context, \"info\": processed_info})\n",
        "\n",
        "        print(\"Token sequence processing complete.\")\n",
        "        # Adjust flow state based on sequence complexity (e.g., length, variability)\n",
        "        complexity = len(token_sequence) / 10.0 # Simple complexity measure\n",
        "        perf = len(processed_results) / len(token_sequence) if token_sequence else 0.0 # Performance = completion rate\n",
        "        self.adjust_flow_state(complexity, perf)\n",
        "\n",
        "        return processed_results\n",
        "\n",
        "    def handle_token_with_context(self, token: str, context: List[str]) -> str:\n",
        "        \"\"\"Placeholder logic for handling a token given its context.\"\"\"\n",
        "        # Example: Identify if token is novel in context or follows a pattern\n",
        "        context_str = \" \".join(context[:-1]) # Context before current token\n",
        "        # print(f\"  Processing token: '{token}' with context: '[...{context_str[-30:]}]'\")\n",
        "        if token in context[:-1]:\n",
        "             return \"repeated_in_context\"\n",
        "        elif len(context) > 2 and token.isdigit() and context[-2].isdigit():\n",
        "             return \"continuation_of_numbers\"\n",
        "        else:\n",
        "             return \"standard_processing\"\n",
        "\n",
        "\n",
        "class MultiModalAgent(DFSNAgent):\n",
        "    \"\"\"Agent capable of processing inputs from multiple modalities.\"\"\"\n",
        "    def __init__(self, name: str, modalities: List[str]):\n",
        "        super().__init__(name)\n",
        "        self.modalities = modalities\n",
        "        print(f\"MultiModalAgent {self.name} initialized. Handles: {', '.join(modalities)}\")\n",
        "        # Placeholder for modality-specific models (e.g., image classifier, text processor)\n",
        "        self.text_processor = lambda text: f\"Processed text: {text[:30]}...\"\n",
        "        self.image_processor = lambda img: f\"Processed image of shape: {img.shape}\" if isinstance(img, np.ndarray) else \"Processed non-numpy image\"\n",
        "        self.audio_processor = lambda aud: f\"Processed audio data (length {len(aud)} samples)\" if isinstance(aud, (list, np.ndarray)) else \"Processed non-array audio\"\n",
        "        self.video_processor = lambda vid: f\"Processed video data (frames: {len(vid)})\" if isinstance(vid, (list, np.ndarray)) else \"Processed non-array video\"\n",
        "\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles multimodal processing tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"multimodal_input\":\n",
        "             error = self.check_task_requirements(task, [\"input_data\"]) # Input data is expected to be a dict\n",
        "             if error: return error\n",
        "             if not isinstance(task[\"input_data\"], dict):\n",
        "                 return {\"error\": \"input_data must be a dictionary for multimodal agent\", \"agent\": self.name}\n",
        "\n",
        "             results = self.process_input(task[\"input_data\"])\n",
        "             # Adjust flow based on complexity (e.g., number of modalities, data size)\n",
        "             complexity = len(task[\"input_data\"].keys()) # Simple: complexity = number of modalities present\n",
        "             perf = 1.0 # Assume success if no errors\n",
        "             self.adjust_flow_state(complexity, perf)\n",
        "             return {\"result\": results, \"agent\": self.name}\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate\n",
        "\n",
        "    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, str]:\n",
        "        \"\"\"Processes input data based on supported modalities.\"\"\"\n",
        "        print(f\"{self.name} processing multimodal input...\")\n",
        "        results = {}\n",
        "        for modality in self.modalities:\n",
        "            data = input_data.get(modality) # Get data for the modality\n",
        "            if data is not None:\n",
        "                print(f\"  Processing modality: {modality}\")\n",
        "                handler = getattr(self, f\"handle_{modality}\", None)\n",
        "                if handler and callable(handler):\n",
        "                    try:\n",
        "                         result_info = handler(data)\n",
        "                         results[modality] = result_info\n",
        "                    except Exception as e:\n",
        "                         print(f\"    Error processing {modality}: {e}\")\n",
        "                         results[modality] = f\"Error: {e}\"\n",
        "                else:\n",
        "                     print(f\"    No handler found for modality: {modality}\")\n",
        "                     results[modality] = \"No handler available\"\n",
        "            # else: print(f\"  Modality '{modality}' not present in input.\")\n",
        "        print(\"Multimodal processing complete.\")\n",
        "        return results\n",
        "\n",
        "    # --- Modality Handlers ---\n",
        "    def handle_text(self, text_data: str) -> str:\n",
        "        # print(f\"  Handling text data...\")\n",
        "        return self.text_processor(text_data)\n",
        "\n",
        "    def handle_image(self, image_data: Any) -> str:\n",
        "        # print(f\"  Handling image data...\")\n",
        "        # Add actual image processing logic here\n",
        "        return self.image_processor(image_data)\n",
        "\n",
        "    def handle_audio(self, audio_data: Any) -> str:\n",
        "        # print(f\"  Handling audio data...\")\n",
        "        return self.audio_processor(audio_data)\n",
        "\n",
        "    def handle_video(self, video_data: Any) -> str:\n",
        "        # print(f\"  Handling video data...\")\n",
        "        return self.video_processor(video_data)\n",
        "\n",
        "\n",
        "# --- Other Specialized Agents ---\n",
        "\n",
        "class MemoryAgent(DFSNAgent):\n",
        "    \"\"\"Agent focused on managing and utilizing long-term memory.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.memory_system = MemorySystem() # Each agent could have its own or share one\n",
        "        print(f\"MemoryAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles memory-related tasks or uses memory for other tasks.\"\"\"\n",
        "        action = task.get(\"action\")\n",
        "        task_type = task.get(\"type\")\n",
        "\n",
        "        if task_type == \"memory_task\":\n",
        "            if action == \"store\":\n",
        "                 error = self.check_task_requirements(task, [\"key\", \"data\", \"memory_type\"])\n",
        "                 if error: return error\n",
        "                 self.memory_system.store_memory(task[\"data\"], task[\"memory_type\"], task[\"key\"])\n",
        "                 return {\"status\": \"stored\", \"agent\": self.name}\n",
        "            elif action == \"retrieve\":\n",
        "                 error = self.check_task_requirements(task, [\"key\", \"memory_type\"])\n",
        "                 if error: return error\n",
        "                 data = self.memory_system.retrieve_memory(task[\"memory_type\"], task[\"key\"])\n",
        "                 return {\"result\": data, \"status\": \"retrieved\", \"agent\": self.name}\n",
        "            elif action == \"manage_long_horizon\":\n",
        "                 # Example from prompt: process steps using memory\n",
        "                 error = self.check_task_requirements(task, [\"steps\"])\n",
        "                 if error: return error\n",
        "                 results = self.manage_long_horizon_task(task[\"steps\"])\n",
        "                 return {\"result\": results, \"agent\": self.name}\n",
        "\n",
        "        # If not a memory task, use AIW (which calls base__execute_single_task_iteration if not overridden)\n",
        "        return super()._execute_single_task_iteration(task)\n",
        "\n",
        "    def base_execute_task(self, task: Dict) -> Dict:\n",
        "         \"\"\"Base execution for MemoryAgent - maybe retrieves relevant memory first.\"\"\"\n",
        "         print(f\"{self.name} (MemoryAgent Base): Executing {task.get('type')}\")\n",
        "         # Example: Retrieve memory related to the task description\n",
        "         relevant_mem = self.memory_system.retrieve_memory(\n",
        "             memory_type='long',\n",
        "             criteria=lambda item: isinstance(item, dict) and task.get('description', '') in item.get('description', '')\n",
        "         )\n",
        "         if relevant_mem:\n",
        "             print(f\"  Found relevant memory: {relevant_mem}\")\n",
        "             # Use memory in task execution (placeholder)\n",
        "             task['context_from_memory'] = relevant_mem\n",
        "\n",
        "         # Simulate work\n",
        "         time.sleep(0.02 * task.get('complexity', 1.0))\n",
        "         return {\"result\": f\"Memory-enhanced execution for {task.get('type')}\", \"agent\": self.name}\n",
        "\n",
        "\n",
        "    def manage_long_horizon_task(self, steps: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Processes a sequence of steps, using memory when required.\"\"\"\n",
        "        step_results = []\n",
        "        print(f\"{self.name} managing long horizon task with {len(steps)} steps.\")\n",
        "        for i, step in enumerate(steps):\n",
        "             print(f\"  Step {i+1}: {step.get('description', 'No description')}\")\n",
        "             step_result = {}\n",
        "             if step.get('requires_memory', False):\n",
        "                 # Retrieve most relevant recent memory (e.g., based on description)\n",
        "                 memory_key = step.get(\"memory_key\")\n",
        "                 retrieved_data = self.memory_system.retrieve_memory(memory_type='short', key=memory_key)\n",
        "                 if retrieved_data:\n",
        "                     print(f\"    Retrieved memory for key '{memory_key}'. Applying to task.\")\n",
        "                     # Apply memory to task (e.g., add as context)\n",
        "                     step_result = self.process_task_step(step, memory_context=retrieved_data)\n",
        "                 else:\n",
        "                     print(f\"    Memory required but key '{memory_key}' not found. Proceeding without.\")\n",
        "                     step_result = self.process_task_step(step)\n",
        "             else:\n",
        "                 step_result = self.process_task_step(step)\n",
        "\n",
        "             # Store result of step in short-term memory if needed for next steps\n",
        "             if step.get(\"output_key\"):\n",
        "                  self.memory_system.store_memory(step_result.get(\"result\"), memory_type='short', key=step.get(\"output_key\"))\n",
        "\n",
        "             step_results.append(step_result)\n",
        "        return step_results\n",
        "\n",
        "    def process_task_step(self, step_data: Dict, memory_context: Optional[Any] = None) -> Dict:\n",
        "        \"\"\"Simulates processing a single step of a long-horizon task.\"\"\"\n",
        "        # Placeholder: actual processing depends on step description/type\n",
        "        print(f\"    Processing step: {step_data.get('description', '')}\" + (f\" with memory context.\" if memory_context else \"\"))\n",
        "        result_value = f\"Processed '{step_data.get('description', '')[:20]}...'\"\n",
        "        if memory_context:\n",
        "             result_value += \" (used memory)\"\n",
        "        time.sleep(0.01) # Simulate work\n",
        "        return {\"result\": result_value, \"status\": \"completed\"}\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        # Share recent memory keys or stats\n",
        "        return {\"recent_short_term_keys\": list(self.memory_system.short_term_memory.keys())[-5:],\n",
        "                \"long_term_size\": len(self.memory_system.long_term_memory)}\n",
        "\n",
        "    def process_domain_specific_state(self, domain_data: Dict):\n",
        "        # Could potentially pre-fetch memory based on peer's recent keys\n",
        "        pass\n",
        "\n",
        "\n",
        "class OrganicChemistryAgent(DFSNAgent):\n",
        "    \"\"\"Domain-focused agent for organic chemistry simulations.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.epsilon = self.math_module.epsilon_0\n",
        "        self.reaction_rates = {} # Store known reaction rates\n",
        "        print(f\"OrganicChemistryAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles organic chemistry tasks like reaction kinetics.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type != \"organic_chemistry\":\n",
        "            return super()._execute_single_task_iteration(task)\n",
        "\n",
        "        required_keys_map = {\n",
        "            \"reaction_kinetics\": [\"reactants\", \"rate_constant\", \"temperature\"],\n",
        "            \"bond_energy\": [\"bond_strength\", \"distance\"]\n",
        "        }\n",
        "\n",
        "        if not action:\n",
        "            return {\"error\": \"Missing 'action' key for organic_chemistry task\", \"agent\": self.name}\n",
        "\n",
        "        keys_needed = required_keys_map.get(action)\n",
        "        if keys_needed is None:\n",
        "            return {\"error\": f\"Unsupported organic chemistry action: {action}\", \"agent\": self.name}\n",
        "\n",
        "        error = self.check_task_requirements(task, keys_needed)\n",
        "        if error:\n",
        "            return error\n",
        "\n",
        "        try:\n",
        "            if action == \"reaction_kinetics\":\n",
        "                # Assume 'reactants' is a concentration value or dict\n",
        "                conc = task[\"reactants\"] if isinstance(task[\"reactants\"], (int, float)) else np.mean(list(task[\"reactants\"].values())) if isinstance(task[\"reactants\"], dict) else 1.0\n",
        "                rate = self.f0z_reaction_kinetics(conc, task[\"rate_constant\"], task[\"temperature\"])\n",
        "                return {\"result\": {\"reaction_rate\": rate}, \"agent\": self.name}\n",
        "            elif action == \"bond_energy\":\n",
        "                energy = self.f0z_bond_energy(task[\"bond_strength\"], task[\"distance\"])\n",
        "                return {\"result\": {\"bond_energy\": energy}, \"agent\": self.name}\n",
        "        except Exception as e:\n",
        "             print(f\"Error during organic chemistry execution ({action}) for {self.name}: {e}\")\n",
        "             return {\"error\": f\"Exception during organic chem execution: {e}\", \"agent\": self.name}\n",
        "\n",
        "        return {\"error\": f\"Organic chemistry action '{action}' not implemented.\", \"agent\": self.name}\n",
        "\n",
        "    def f0z_reaction_kinetics(self, concentration: float, rate_constant: float, temperature: float) -> float:\n",
        "        \"\"\"Calculates reaction rate using Arrhenius equation (simplified) with F0Z.\"\"\"\n",
        "        # Simplified: Ea/R = 1.0 for example\n",
        "        activation_energy_term = 1.0\n",
        "        temp_k = temperature + 273.15 # Assume input temp is Celsius\n",
        "        if temp_k <= 0: temp_k = self.epsilon # Avoid zero or negative Kelvin temp\n",
        "\n",
        "        # Rate = k * [A]^n * exp(-Ea/RT) - simplified to Rate = k * [A] * exp(-1/T)\n",
        "        rate = rate_constant * concentration * np.exp(-activation_energy_term / temp_k)\n",
        "        stabilized_rate = self.math_module.f0z_stabilize(torch.tensor(rate)).item()\n",
        "        # print(f\"  Reaction Rate: k={rate_constant}, [A]={concentration:.2f}, T={temp_k:.1f}K => Rate={stabilized_rate:.4e}\")\n",
        "        return stabilized_rate\n",
        "\n",
        "    def f0z_bond_energy(self, bond_strength: float, distance: float) -> float:\n",
        "        \"\"\"Calculates simplified bond energy with F0Z stabilization.\"\"\"\n",
        "        # Simple inverse relationship: Energy = Strength / Distance\n",
        "        if distance <= 0: distance = self.epsilon # Avoid zero or negative distance\n",
        "\n",
        "        energy = bond_strength / distance\n",
        "        stabilized_energy = self.math_module.f0z_stabilize(torch.tensor(energy)).item()\n",
        "        # Clipping to prevent extreme values from division by small distance\n",
        "        clipped_energy = np.clip(stabilized_energy, -1e6, 1e6)\n",
        "        # print(f\"  Bond Energy: Strength={bond_strength}, Dist={distance:.3f} => Energy={clipped_energy:.4e}\")\n",
        "        return clipped_energy\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        # Share known reaction rates\n",
        "        return {\"known_rates_count\": len(self.reaction_rates)}\n",
        "\n",
        "\n",
        "class MolecularBiologyAgent(DFSNAgent):\n",
        "    \"\"\"Domain-focused agent for molecular biology simulations.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.epsilon = self.math_module.epsilon_0\n",
        "        self.sequences = {} # Store known sequences\n",
        "        print(f\"MolecularBiologyAgent {self.name} initialized.\")\n",
        "\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles molecular biology tasks like DNA replication and protein folding.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type != \"molecular_biology\":\n",
        "            return super()._execute_single_task_iteration(task)\n",
        "\n",
        "        required_keys_map = {\n",
        "            \"dna_replication\": [\"sequence\", \"polymerase_rate\"],\n",
        "            \"protein_folding\": [\"amino_acids\", \"folding_energy\"],\n",
        "            \"analyze_sequence\": [\"sequence\"]\n",
        "        }\n",
        "\n",
        "        if not action:\n",
        "            return {\"error\": \"Missing 'action' key for molecular_biology task\", \"agent\": self.name}\n",
        "\n",
        "        keys_needed = required_keys_map.get(action)\n",
        "        if keys_needed is None:\n",
        "            return {\"error\": f\"Unsupported molecular biology action: {action}\", \"agent\": self.name}\n",
        "\n",
        "        error = self.check_task_requirements(task, keys_needed)\n",
        "        if error:\n",
        "            return error\n",
        "\n",
        "        try:\n",
        "            if action == \"dna_replication\":\n",
        "                result = self.f0z_dna_replication(task[\"sequence\"], task[\"polymerase_rate\"])\n",
        "                return {\"result\": result, \"agent\": self.name}\n",
        "            elif action == \"protein_folding\":\n",
        "                # Assume amino_acids is a sequence or list of properties\n",
        "                aa_data = task[\"amino_acids\"]\n",
        "                if isinstance(aa_data, str): # If it's a string sequence, use length as simple measure\n",
        "                     aa_measure = len(aa_data)\n",
        "                elif isinstance(aa_data, (list, np.ndarray)): # If numeric properties, use sum\n",
        "                     aa_measure = np.sum(np.abs(aa_data))\n",
        "                else: aa_measure = 1.0 # Fallback\n",
        "\n",
        "                energy = self.f0z_protein_folding(aa_measure, task[\"folding_energy\"])\n",
        "                return {\"result\": {\"folded_energy\": energy}, \"agent\": self.name}\n",
        "            elif action == \"analyze_sequence\":\n",
        "                 gc_content = self.analyze_sequence(task[\"sequence\"])\n",
        "                 return {\"result\": {\"gc_content\": gc_content}, \"agent\": self.name}\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error during molecular biology execution ({action}) for {self.name}: {e}\")\n",
        "             return {\"error\": f\"Exception during mol bio execution: {e}\", \"agent\": self.name}\n",
        "\n",
        "        return {\"error\": f\"Molecular biology action '{action}' not implemented.\", \"agent\": self.name}\n",
        "\n",
        "\n",
        "    def f0z_dna_replication(self, sequence: str, polymerase_rate: float) -> Dict:\n",
        "        \"\"\"Simulates DNA replication rate and creates complementary strand.\"\"\"\n",
        "        if not sequence: return {\"new_strand\": \"\", \"rate\": 0.0}\n",
        "        replication_rate = polymerase_rate * len(sequence)\n",
        "        stabilized_rate = self.math_module.f0z_stabilize(torch.tensor(replication_rate)).item()\n",
        "\n",
        "        # Generate complementary strand (simple A-T, G-C pairing)\n",
        "        complement_map = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G'}\n",
        "        try:\n",
        "            new_strand = \"\".join(complement_map.get(base.upper(), 'N') for base in reversed(sequence))\n",
        "        except Exception as e:\n",
        "             print(f\"Error generating complementary strand: {e}\")\n",
        "             new_strand = \"Error\"\n",
        "\n",
        "        # print(f\"  DNA Replication: Rate={stabilized_rate:.3f}, New Strand Generated.\")\n",
        "        return {\"new_strand\": new_strand, \"rate\": stabilized_rate}\n",
        "\n",
        "    def f0z_protein_folding(self, amino_acid_measure: float, folding_energy: float) -> float:\n",
        "        \"\"\"Calculates simplified protein folding energy.\"\"\"\n",
        "        # Simple inverse relationship: Energy = FoldingEnergy / AminoAcidMeasure\n",
        "        denominator = amino_acid_measure + self.epsilon # Stabilize denominator\n",
        "        if denominator == 0: denominator = self.epsilon\n",
        "\n",
        "        energy = folding_energy / denominator\n",
        "        stabilized_energy = self.math_module.f0z_stabilize(torch.tensor(energy)).item()\n",
        "        # Clip to prevent extreme values\n",
        "        clipped_energy = np.clip(stabilized_energy, -1e6, 1e6)\n",
        "        # print(f\"  Protein Folding: Energy={folding_energy}, AA Measure={amino_acid_measure:.2f} => Folded Energy={clipped_energy:.4e}\")\n",
        "        return clipped_energy\n",
        "\n",
        "    def analyze_sequence(self, sequence: str) -> float:\n",
        "         \"\"\"Calculates GC content of a sequence.\"\"\"\n",
        "         if not sequence: return 0.0\n",
        "         sequence = sequence.upper()\n",
        "         gc_count = sequence.count('G') + sequence.count('C')\n",
        "         total_len = len(sequence)\n",
        "         if total_len == 0: return 0.0\n",
        "         gc_content = gc_count / total_len\n",
        "         stabilized_gc = self.math_module.f0z_stabilize(torch.tensor(gc_content)).item()\n",
        "         return stabilized_gc\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        # Share count of stored sequences\n",
        "        return {\"known_sequences_count\": len(self.sequences)}\n",
        "\n",
        "\n",
        "class CreativeAgent(DFSNAgent):\n",
        "    \"\"\"Agent focused on generative and creative tasks.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.generated_outputs = [] # History of generated content\n",
        "        # Could potentially integrate a simple generative model or LLM call here\n",
        "        try:\n",
        "            # Use a smaller model suitable for creative text generation snippets\n",
        "            self.generator_pipe = pipeline(\"text-generation\", model=\"gpt2\", device=-1) # Use CPU for lighter model\n",
        "            print(f\"CreativeAgent {self.name} initialized with gpt2 pipeline.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to load gpt2 pipeline for CreativeAgent: {e}. Using placeholder.\")\n",
        "            self.generator_pipe = None\n",
        "\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles creative tasks like text generation.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"creative\":\n",
        "            if action == \"generate_text\":\n",
        "                error = self.check_task_requirements(task, [\"prompt\"])\n",
        "                if error: return error\n",
        "                feedback = task.get(\"feedback\", \"neutral\") # Use feedback if provided\n",
        "                complexity = task.get(\"complexity\", 5)\n",
        "                output = self.generate_creative_output(task[\"prompt\"], complexity, feedback)\n",
        "                return {\"result\": {\"generated_text\": output}, \"agent\": self.name}\n",
        "            elif action == \"generate_idea\": # From collaborative example\n",
        "                 idea = self.generate_idea()\n",
        "                 return {\"result\": {\"idea\": idea}, \"agent\": self.name}\n",
        "\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate non-creative tasks\n",
        "\n",
        "    def generate_creative_output(self, prompt: str, complexity: float, feedback: str) -> str:\n",
        "        \"\"\"Generates creative text output based on prompt, complexity, and feedback.\"\"\"\n",
        "        print(f\"{self.name} generating creative content. Prompt: '{prompt[:30]}...', Complexity: {complexity}, Feedback: {feedback}\")\n",
        "\n",
        "        # Adjust generation parameters based on complexity and feedback\n",
        "        max_len = int(max(30, min(150, 30 + complexity * 10))) # Longer output for higher complexity\n",
        "        num_seq = 1\n",
        "        temperature = 1.0 # Default creativity\n",
        "        if feedback == 'positive':\n",
        "            temperature = 0.8 # Be slightly more focused if feedback is positive\n",
        "        elif feedback == 'negative':\n",
        "            temperature = 1.2 # Be more random/exploratory if feedback is negative\n",
        "\n",
        "        # Use the generator pipeline if available\n",
        "        if self.generator_pipe:\n",
        "            try:\n",
        "                # Ensure prompt is not overly long for the model\n",
        "                truncated_prompt = prompt[:500]\n",
        "                generated = self.generator_pipe(\n",
        "                    truncated_prompt,\n",
        "                    max_length=max_len,\n",
        "                    num_return_sequences=num_seq,\n",
        "                    temperature=temperature,\n",
        "                    do_sample=True # Ensure sampling is enabled for temperature to have effect\n",
        "                )\n",
        "                output_text = generated[0]['generated_text']\n",
        "                # Clean up output, remove prompt potentially\n",
        "                if output_text.startswith(truncated_prompt):\n",
        "                    output_text = output_text[len(truncated_prompt):].strip()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error during text generation: {e}\")\n",
        "                output_text = f\"Error generating: {e}\"\n",
        "        else:\n",
        "            # Placeholder generation\n",
        "            output_text = f\"Simulated creative response to '{prompt[:20]}...' based on complexity {complexity} and feedback {feedback}. [T={temperature:.1f}, L={max_len}]\"\n",
        "\n",
        "        self.generated_outputs.append(output_text)\n",
        "        self.performance_history.append(len(output_text) / 100.0) # Simple performance metric: length\n",
        "        print(f\"  Generated output length: {len(output_text)}\")\n",
        "        return output_text\n",
        "\n",
        "    def generate_idea(self) -> str:\n",
        "         \"\"\"Generates a simple creative idea.\"\"\"\n",
        "         ideas = [\"Quantum poetry\", \"Fractal music\", \"AI-driven chemistry\", \"Entangled decision trees\", \"Zero-bug code\"]\n",
        "         idea = random.choice(ideas) + f\" (variant {random.randint(1,100)})\"\n",
        "         print(f\"{self.name} generated idea: {idea}\")\n",
        "         return idea\n",
        "\n",
        "    def integrate_idea(self, shared_idea: str):\n",
        "         \"\"\"Placeholder for integrating an idea from another agent.\"\"\"\n",
        "         print(f\"{self.name} received shared idea: '{shared_idea}'. Integrating into creative process.\")\n",
        "         # Could add to prompts or influence internal state\n",
        "         self.generate_creative_output(f\"Inspired by the idea: {shared_idea}\", complexity=6, feedback=\"neutral\")\n",
        "\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "         return {\"recent_outputs_count\": len(self.generated_outputs)}\n",
        "\n",
        "\n",
        "class InformationTheoryAgent(DFSNAgent):\n",
        "    \"\"\"Agent focused on information theory concepts like entropy and mutual information.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.epsilon = self.math_module.epsilon_0\n",
        "        self.mode = \"classical\" # Added from LLaDA example\n",
        "        self.todo_queue = [] # Added from LLaDA example\n",
        "        self.consolidated_todos = [] # Added from LLaDA example\n",
        "        print(f\"InformationTheoryAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles information theory tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        # Infer action from type if not specified\n",
        "        if not action:\n",
        "            if \"entropy\" in task_type: action = \"compute_entropy\"\n",
        "            elif \"mutual_info\" in task_type: action = \"compute_mutual_info\"\n",
        "            elif \"channel_capacity\" in task_type: action = \"optimize_channel_capacity\"\n",
        "            elif \"validate_hypothesis\" in task_type: action = \"validate_hypothesis\" # From collab example\n",
        "            task['action'] = action\n",
        "\n",
        "        if action == \"compute_entropy\":\n",
        "            error = self.check_task_requirements(task, [\"probabilities\"])\n",
        "            if error: return error\n",
        "            entropy = self.compute_entropy(task[\"probabilities\"])\n",
        "            return {\"result\": {\"entropy\": entropy}, \"agent\": self.name}\n",
        "        elif action == \"compute_mutual_info\":\n",
        "            error = self.check_task_requirements(task, [\"X\", \"Y\"])\n",
        "            if error: return error\n",
        "            mi = self.compute_mutual_info(task[\"X\"], task[\"Y\"])\n",
        "            return {\"result\": {\"mutual_information\": mi}, \"agent\": self.name}\n",
        "        elif action == \"optimize_channel_capacity\":\n",
        "            error = self.check_task_requirements(task, [\"signal\", \"noise\", \"bandwidth\"])\n",
        "            if error: return error\n",
        "            capacity = self.optimize_channel_capacity(task[\"signal\"], task[\"noise\"], task[\"bandwidth\"])\n",
        "            return {\"result\": {\"channel_capacity\": capacity}, \"agent\": self.name}\n",
        "        elif action == \"validate_hypothesis\": # From ZSG Collab Manager example\n",
        "             error = self.check_task_requirements(task, [\"hypothesis\"])\n",
        "             if error: return error\n",
        "             is_valid = self.validate_hypothesis(task[\"hypothesis\"])\n",
        "             return {\"result\": {\"is_valid\": is_valid}, \"agent\": self.name}\n",
        "\n",
        "\n",
        "        # If task involves TODO management (from LLaDA example)\n",
        "        elif action == \"add_todo\":\n",
        "             error = self.check_task_requirements(task, [\"todo_data\"])\n",
        "             if error: return error\n",
        "             self.add_todo(task[\"todo_data\"])\n",
        "             return {\"status\": \"todo_added\", \"agent\": self.name}\n",
        "        elif action == \"collect_results\":\n",
        "             error = self.check_task_requirements(task, [\"source_agent_name\"])\n",
        "             if error: return error\n",
        "             # Needs access to other agents - requires manager/registry\n",
        "             print(f\"Warning: collect_results needs agent registry access (not implemented here).\")\n",
        "             # result = self.collect_results(task[\"source_agent\"]) # Conceptual\n",
        "             return {\"status\": \"collect_results (simulated)\", \"agent\": self.name}\n",
        "        elif action == \"consolidate\":\n",
        "             # Use internal queue for consolidation\n",
        "             self.consolidate_todos(self.todo_queue) # Consolidate pending todos\n",
        "             return {\"status\": \"consolidation_attempted\", \"consolidated_count\": len(self.consolidated_todos), \"agent\": self.name}\n",
        "\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate other tasks\n",
        "\n",
        "\n",
        "    def compute_entropy(self, probabilities: Any) -> float:\n",
        "        \"\"\"Computes Shannon entropy with F0Z stabilization.\"\"\"\n",
        "        try:\n",
        "            probs_tensor = torch.tensor(probabilities, dtype=torch.float32)\n",
        "            # Ensure probabilities are non-negative and sum approximately to 1\n",
        "            probs_tensor = torch.clamp(probs_tensor, min=0.0)\n",
        "            prob_sum = torch.sum(probs_tensor)\n",
        "            if prob_sum <= 0: return 0.0 # Avoid division by zero or log(0) if sum is zero\n",
        "            probs_tensor = probs_tensor / prob_sum # Normalize\n",
        "\n",
        "            # Stabilize probabilities before log calculation\n",
        "            stabilized_probs = self.math_module.f0z_stabilize(probs_tensor, system_size=len(probabilities))\n",
        "\n",
        "            # Add epsilon inside log2 for numerical stability near zero probability\n",
        "            log_probs = torch.log2(stabilized_probs + float(self.epsilon))\n",
        "            entropy = -torch.sum(stabilized_probs * log_probs)\n",
        "            # print(f\"  Entropy calculated: {entropy.item():.4f}\")\n",
        "            return entropy.item()\n",
        "        except Exception as e:\n",
        "             print(f\"Error computing entropy: {e}\")\n",
        "             return 0.0\n",
        "\n",
        "    def compute_mutual_info(self, X: Any, Y: Any, bins: int = 10) -> float:\n",
        "        \"\"\"Computes Mutual Information between two variables X and Y with F0Z.\"\"\"\n",
        "        try:\n",
        "            X_np = np.array(X).flatten()\n",
        "            Y_np = np.array(Y).flatten()\n",
        "            if len(X_np) != len(Y_np):\n",
        "                 raise ValueError(\"X and Y must have the same length.\")\n",
        "            if len(X_np) == 0: return 0.0\n",
        "\n",
        "            # Calculate 2D histogram to get joint probability distribution\n",
        "            joint_hist, _, _ = np.histogram2d(X_np, Y_np, bins=bins, density=True)\n",
        "            joint_prob = torch.tensor(joint_hist, dtype=torch.float32)\n",
        "            # Normalize joint probability\n",
        "            joint_prob /= torch.sum(joint_prob)\n",
        "\n",
        "            # Stabilize joint probability\n",
        "            joint_prob_s = self.math_module.f0z_stabilize(joint_prob, system_size=joint_prob.numel())\n",
        "\n",
        "            # Calculate marginal probabilities\n",
        "            marginal_x = torch.sum(joint_prob_s, axis=1)\n",
        "            marginal_y = torch.sum(joint_prob_s, axis=0)\n",
        "\n",
        "            # Stabilize marginals (important!)\n",
        "            marginal_x_s = self.math_module.f0z_stabilize(marginal_x, system_size=len(marginal_x))\n",
        "            marginal_y_s = self.math_module.f0z_stabilize(marginal_y, system_size=len(marginal_y))\n",
        "\n",
        "            # Compute MI using stabilized probabilities\n",
        "            # MI = sum_{x,y} P(x,y) * log2( P(x,y) / (P(x)P(y)) )\n",
        "            # Add epsilon for stability in logs and division\n",
        "            term_xy = marginal_x_s[:, None] * marginal_y_s[None, :] + float(self.epsilon)\n",
        "            log_term = torch.log2(joint_prob_s + float(self.epsilon)) - torch.log2(term_xy)\n",
        "\n",
        "            mutual_info = torch.sum(joint_prob_s * log_term)\n",
        "\n",
        "            # Ensure MI is non-negative (theoretically should be, but numerical issues)\n",
        "            mi_val = max(0.0, mutual_info.item())\n",
        "            # print(f\"  Mutual Information calculated: {mi_val:.4f}\")\n",
        "            return mi_val\n",
        "        except Exception as e:\n",
        "             print(f\"Error computing mutual information: {e}\")\n",
        "             return 0.0\n",
        "\n",
        "\n",
        "    def optimize_channel_capacity(self, signal_power: float, noise_power: float, bandwidth: float) -> float:\n",
        "        \"\"\"Calculates channel capacity using Shannon-Hartley theorem with F0Z.\"\"\"\n",
        "        # C = B * log2(1 + S/N)\n",
        "        signal = torch.tensor(signal_power, dtype=torch.float32)\n",
        "        noise = torch.tensor(noise_power, dtype=torch.float32)\n",
        "        bw = torch.tensor(bandwidth, dtype=torch.float32)\n",
        "\n",
        "        # Stabilize noise power before division\n",
        "        noise_s = self.math_module.f0z_stabilize(noise, system_size=1)\n",
        "        if noise_s.item() <= 0: # If noise is stabilized to zero or negative, capacity is effectively infinite or undefined\n",
        "             print(\"  Warning: Noise power stabilized to zero or less. Returning large capacity value.\")\n",
        "             return 1e9 # Return a large number\n",
        "\n",
        "        snr = signal / noise_s\n",
        "        # Stabilize SNR >= 0 before adding 1\n",
        "        snr = torch.clamp(snr, min=0.0)\n",
        "\n",
        "        capacity = bw * torch.log2(1.0 + snr)\n",
        "        stabilized_capacity = self.math_module.f0z_stabilize(capacity, system_size=1)\n",
        "        # print(f\"  Channel Capacity: S={signal_power:.2f}, N={noise_power:.2f}, B={bandwidth:.2f} => C={stabilized_capacity.item():.4f}\")\n",
        "        return stabilized_capacity.item()\n",
        "\n",
        "    def validate_hypothesis(self, hypothesis: Dict) -> bool:\n",
        "        \"\"\"Validates a hypothesis based on information-theoretic principles (placeholder).\"\"\"\n",
        "        # Example: Check if hypothesis reduces uncertainty (increases information)\n",
        "        # Requires quantifying information content of hypothesis and data\n",
        "        print(f\"Validating hypothesis (ID: {hypothesis.get('id', 'N/A')}) using InfoTheory (placeholder).\")\n",
        "        # Placeholder logic: Check if expected info gain is positive\n",
        "        expected_info_gain = hypothesis.get(\"variables\", {}).get(\"expected_information_gain\", 0.1)\n",
        "        is_valid = expected_info_gain > 0 and random.random() < 0.9 # Simple probabilistic validation\n",
        "        print(f\"  Hypothesis valid: {is_valid}\")\n",
        "        return is_valid\n",
        "\n",
        "    # --- Methods from LLaDA Consolidation Example ---\n",
        "    def transition_mode(self, bridge: Optional[ZSGQuantumBridge] = None):\n",
        "        \"\"\"Transition between classical and quantum modes based on entropy.\"\"\"\n",
        "        if not bridge or not bridge.simulator:\n",
        "             print(f\"{self.name}: Cannot transition mode without quantum bridge.\")\n",
        "             self.mode = \"classical\"\n",
        "             return\n",
        "\n",
        "        # Encode a dummy high-priority task to check entanglement/entropy\n",
        "        bridge.encode({\"priority\": 10}, entangle=True)\n",
        "        entropy, _ = bridge.entropy_bot.monitor_state(bridge.simulator.state)\n",
        "        # Threshold could be adaptive\n",
        "        entropy_threshold = bridge.simulator.n_qubits * np.log2(2) * 0.6 # 60% of max entropy\n",
        "        new_mode = \"quantum\" if entropy > entropy_threshold else \"classical\"\n",
        "        if new_mode != self.mode:\n",
        "            self.mode = new_mode\n",
        "            print(f\"{self.name}: Transitioned to {self.mode} mode (Entropy: {entropy:.3f} / Threshold: {entropy_threshold:.3f}).\")\n",
        "\n",
        "    def add_todo(self, todo_data: Dict):\n",
        "        \"\"\"Adds a ZSGTodo item to the internal queue.\"\"\"\n",
        "        # Create a ZSGTodo object - Ensure necessary fields are present\n",
        "        task_id = todo_data.get(\"task_id\", f\"todo_{len(self.todo_queue)}_{int(time.time())}\")\n",
        "        description = todo_data.get(\"description\", \"No description\")\n",
        "        status = todo_data.get(\"status\", \"Pending\")\n",
        "        priority = todo_data.get(\"priority\", 5.0) # Default priority\n",
        "        mliw_step = todo_data.get(\"mliw_step\", \"Process\")\n",
        "        payload = todo_data.get(\"data\", {})\n",
        "\n",
        "        # Ensure priority is float\n",
        "        try:\n",
        "            priority = float(priority)\n",
        "        except (ValueError, TypeError):\n",
        "            priority = 5.0 # Default if conversion fails\n",
        "\n",
        "        todo = ZSGTodo(task_id, description, status, priority, mliw_step, payload)\n",
        "        self.todo_queue.append(todo)\n",
        "        print(f\"{self.name}: Added TODO {task_id} to internal queue (Queue size: {len(self.todo_queue)}).\")\n",
        "        # Optionally store in MemorySystem as well\n",
        "        # self.memory_system.store(f\"todo_{task_id}\", todo.to_json())\n",
        "\n",
        "    def collect_results(self, agent: Agent, bridge: Optional[ZSGQuantumBridge] = None):\n",
        "        \"\"\"Collects results (TODOs) from another agent, potentially syncing quantum state.\"\"\"\n",
        "        if not hasattr(agent, 'state'):\n",
        "            print(f\"Warning: Agent {agent.name} does not have a 'state' attribute to collect results from.\")\n",
        "            return None\n",
        "\n",
        "        agent_state = agent.state # Assuming agent stores its state here\n",
        "\n",
        "        # Quantum state synchronization if in quantum mode\n",
        "        if self.mode == \"quantum\" and bridge and bridge.simulator:\n",
        "            peer_quantum_state = agent_state.get(\"quantum_state_vector\")\n",
        "            if peer_quantum_state and len(peer_quantum_state) == 2**bridge.simulator.n_qubits:\n",
        "                # Option 1: Update bridge state\n",
        "                bridge.simulator.state = np.array(peer_quantum_state, dtype=complex)\n",
        "                print(f\"{self.name}: Synchronized quantum state from {agent.name}.\")\n",
        "            else:\n",
        "                print(f\"{self.name}: Failed to synchronize quantum state from {agent.name} (invalid state).\")\n",
        "\n",
        "        # Collect completed TODO from agent state\n",
        "        todo_in_agent_state = agent_state.get(\"current_todo\")\n",
        "        is_completed = agent_state.get(\"task_completed\", False)\n",
        "\n",
        "        if isinstance(todo_in_agent_state, ZSGTodo) and is_completed:\n",
        "             print(f\"{self.name}: Collected completed TODO {todo_in_agent_state.task_id} from {agent.name}.\")\n",
        "             # Remove from internal queue if it exists there\n",
        "             self.todo_queue = [t for t in self.todo_queue if t.task_id != todo_in_agent_state.task_id]\n",
        "             # Store completion in memory system (optional)\n",
        "             # self.memory_system.store(f\"completed_{agent.name}_{todo.task_id}\", todo.to_json())\n",
        "             return todo_in_agent_state # Return the completed TODO object\n",
        "        elif isinstance(todo_in_agent_state, dict) and todo_in_agent_state.get('status') == 'Completed':\n",
        "              # Handle if agent stores basic dict instead of ZSGTodo object\n",
        "              print(f\"{self.name}: Collected completed TODO (dict) {todo_in_agent_state.get('task_id', 'N/A')} from {agent.name}.\")\n",
        "              self.todo_queue = [t for t in self.todo_queue if t.task_id != todo_in_agent_state.get('task_id')]\n",
        "              return todo_in_agent_state\n",
        "\n",
        "        # print(f\"{self.name}: No completed TODO found in {agent.name}'s state.\")\n",
        "        return None\n",
        "\n",
        "    def consolidate_todos(self, todos_to_consolidate: List[ZSGTodo]):\n",
        "        \"\"\"Consolidates a list of TODOs if they meet balance criteria.\"\"\"\n",
        "        if not todos_to_consolidate or len(todos_to_consolidate) < 2:\n",
        "            # print(f\"{self.name}: Not enough TODOs ({len(todos_to_consolidate)}) to consolidate.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            priorities = [float(todo.priority) for todo in todos_to_consolidate]\n",
        "            # Use variance from payload if available, else default\n",
        "            variances = [float(todo.data_payload.get(\"variance\", 0.0)) for todo in todos_to_consolidate]\n",
        "\n",
        "            # Balance score: lower is more balanced (less variance in priority/data variance)\n",
        "            balance_score = np.std(priorities) + np.std(variances)\n",
        "            balance_threshold = 1.5 # Tunable threshold\n",
        "\n",
        "            print(f\"{self.name}: Checking consolidation for {len(todos_to_consolidate)} TODOs. Balance score: {balance_score:.3f} (Threshold: {balance_threshold})\")\n",
        "\n",
        "            if balance_score < balance_threshold:\n",
        "                consolidated_id = f\"consolidated_{len(self.consolidated_todos)}_{int(time.time())}\"\n",
        "                consolidated = {\n",
        "                    \"consolidated_id\": consolidated_id,\n",
        "                    \"task_ids\": [todo.task_id for todo in todos_to_consolidate],\n",
        "                    \"total_priority\": sum(priorities),\n",
        "                    \"average_priority\": np.mean(priorities),\n",
        "                    \"total_variance\": sum(variances),\n",
        "                    \"average_variance\": np.mean(variances),\n",
        "                    \"mliw_step\": \"Consolidated\", # Mark step\n",
        "                    \"status\": \"Consolidated\",\n",
        "                    \"description\": f\"Consolidated {len(todos_to_consolidate)} tasks.\"\n",
        "                    # Combine payloads intelligently if needed (e.g., merge dicts)\n",
        "                    # \"combined_payload\": self._merge_payloads([todo.data_payload for todo in todos_to_consolidate])\n",
        "                }\n",
        "                self.consolidated_todos.append(consolidated)\n",
        "                # Remove consolidated todos from the main queue\n",
        "                consolidated_ids = set(consolidated[\"task_ids\"])\n",
        "                self.todo_queue = [todo for todo in self.todo_queue if todo.task_id not in consolidated_ids]\n",
        "\n",
        "                print(f\"  Consolidated {len(todos_to_consolidate)} TODOs into document {consolidated_id}. Queue size: {len(self.todo_queue)}\")\n",
        "                # Store consolidated doc in memory (optional)\n",
        "                # self.memory_system.store(consolidated_id, consolidated, memory_type='long')\n",
        "            else:\n",
        "                print(f\"  TODOs not balanced enough for consolidation (Score {balance_score:.3f} >= {balance_threshold}).\")\n",
        "                # Optional: Redistribute or re-queue logic could go here\n",
        "                # self.redistribute(todos_to_consolidate) # Example redistribution call\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during TODO consolidation: {e}\")\n",
        "\n",
        "    def redistribute(self, todos: List[ZSGTodo]):\n",
        "        \"\"\"Handles redistribution of TODOs if consolidation fails or is undone.\"\"\"\n",
        "        print(f\"{self.name}: Redistributing {len(todos)} TODOs back to AIW queue.\")\n",
        "        # Simply add them back to the internal queue for now\n",
        "        # More complex logic could involve re-prioritizing or assigning to specific agents\n",
        "        for todo in todos:\n",
        "            if not any(t.task_id == todo.task_id for t in self.todo_queue): # Avoid duplicates\n",
        "                 todo.status = \"Pending\" # Reset status\n",
        "                 self.todo_queue.append(todo)\n",
        "\n",
        "    def _merge_payloads(self, payloads: List[Dict]) -> Dict:\n",
        "         \"\"\"Helper to merge data payloads from multiple TODOs.\"\"\"\n",
        "         merged = {}\n",
        "         all_keys = set(k for p in payloads for k in p.keys())\n",
        "         for key in all_keys:\n",
        "             values = [p.get(key) for p in payloads if p.get(key) is not None]\n",
        "             if not values: continue\n",
        "             # Simple merge: store values in a list, or average if numeric\n",
        "             if all(isinstance(v, (int, float)) for v in values):\n",
        "                 merged[key] = np.mean(values)\n",
        "             else:\n",
        "                 merged[key] = values # Store as list if non-numeric or mixed\n",
        "         return merged\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "         return {\"mode\": self.mode, \"queue_size\": len(self.todo_queue), \"consolidated_count\": len(self.consolidated_todos)}\n",
        "\n",
        "\n",
        "class HypothesisAgent(DFSNAgent):\n",
        "    \"\"\"Agent responsible for generating and proposing hypotheses.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.hypotheses_proposed = []\n",
        "        print(f\"HypothesisAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles hypothesis generation or related tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if action == \"propose_hypothesis\":\n",
        "            # Generate hypothesis based on input data or task context\n",
        "            input_data = task.get(\"data\", {})\n",
        "            hypothesis = self.propose_hypothesis(input_data)\n",
        "            if hypothesis:\n",
        "                self.hypotheses_proposed.append(hypothesis)\n",
        "                return {\"result\": hypothesis, \"agent\": self.name}\n",
        "            else:\n",
        "                return {\"error\": \"Could not generate hypothesis\", \"agent\": self.name}\n",
        "        elif action == \"suggest_improvement\": # From info theory agent example interaction\n",
        "             input_data = task.get(\"data\", {})\n",
        "             suggestion = self.suggest_improvement(input_data)\n",
        "             return {\"result\": {\"suggestion\": suggestion}, \"agent\": self.name}\n",
        "\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate other tasks\n",
        "\n",
        "    def propose_hypothesis(self, context_data: Dict) -> Optional[Dict]:\n",
        "        \"\"\"Generates a new hypothesis based on context.\"\"\"\n",
        "        # Simple rule-based or template-based generation for now\n",
        "        hypothesis_id = f\"hyp_{len(self.hypotheses_proposed)}_{int(time.time())}\"\n",
        "        description = \"Generated Hypothesis: \"\n",
        "        variables = {}\n",
        "\n",
        "        # Example: If context involves quantum simulation results\n",
        "        if context_data.get(\"task_type\") == \"quantum_circuit\" and \"decoded_info\" in context_data.get(\"result\", {}):\n",
        "            entropy = context_data[\"result\"][\"decoded_info\"].get(\"entropy\", 0)\n",
        "            if entropy > 1.0:\n",
        "                description += \"High entanglement observed correlates with task success.\"\n",
        "                variables = {\"correlation_metric\": \"entanglement_entropy\", \"threshold\": 1.0, \"expected_outcome\": \"improved_performance\"}\n",
        "            else:\n",
        "                description += \"Low entanglement suggests simpler states dominate.\"\n",
        "                variables = {\"correlation_metric\": \"entanglement_entropy\", \"threshold\": 1.0, \"expected_outcome\": \"simpler_dynamics\"}\n",
        "        elif context_data.get(\"task_type\") == \"physics_simulation\":\n",
        "             # Look for chaotic behavior in results (placeholder)\n",
        "             if context_data.get(\"result\", {}).get(\"chaos_detected\", False):\n",
        "                 description += \"System exhibits chaotic behavior under current parameters.\"\n",
        "                 variables = {\"phenomenon\": \"chaos\", \"trigger_params\": context_data.get(\"params\"), \"prediction\": \"sensitivity_to_initial_conditions\"}\n",
        "             else:\n",
        "                 description += \"System appears stable or periodic.\"\n",
        "                 variables = {\"phenomenon\": \"stability\", \"conditions\": context_data.get(\"params\")}\n",
        "        else:\n",
        "            # Generic hypothesis\n",
        "            description += f\"Observed pattern in data payload suggests a power-law relationship.\"\n",
        "            variables = {\"relationship\": \"power-law\", \"confidence\": 0.6}\n",
        "\n",
        "        print(f\"{self.name} proposing hypothesis: {description}\")\n",
        "        hypothesis = {\n",
        "             \"id\": hypothesis_id,\n",
        "             \"description\": description,\n",
        "             \"variables\": variables,\n",
        "             \"status\": \"proposed\",\n",
        "             \"validation_results\": {}\n",
        "        }\n",
        "        return hypothesis\n",
        "\n",
        "\n",
        "    def suggest_improvement(self, task_result_data: Dict) -> Optional[str]:\n",
        "        \"\"\"Suggests improvements or next steps based on results and hypotheses.\"\"\"\n",
        "        # Example: If a hypothesis was recently proposed, suggest testing it\n",
        "        if self.hypotheses_proposed:\n",
        "            last_hypothesis = self.hypotheses_proposed[-1]\n",
        "            if last_hypothesis[\"status\"] == \"proposed\":\n",
        "                 suggestion = f\"Suggest testing hypothesis '{last_hypothesis['id']}'. Design experiment targeting variables: {list(last_hypothesis['variables'].keys())}\"\n",
        "                 print(f\"{self.name} suggesting improvement: {suggestion}\")\n",
        "                 return suggestion\n",
        "\n",
        "        # Example: If results show high error, suggest parameter tuning\n",
        "        if task_result_data.get(\"error\", 0) > 0.5:\n",
        "             suggestion = \"High error observed. Suggest parameter sweep or trying alternative model/algorithm.\"\n",
        "             print(f\"{self.name} suggesting improvement: {suggestion}\")\n",
        "             return suggestion\n",
        "\n",
        "        return None # No specific suggestion\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        return {\"proposed_hypotheses_count\": len(self.hypotheses_proposed)}\n",
        "\n",
        "\n",
        "class DataScienceAgent(DFSNAgent):\n",
        "    \"\"\"Agent for data analysis and modeling tasks.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.models = {} # Store trained models or analysis results\n",
        "        print(f\"DataScienceAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles data science tasks like regression.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"data_science\":\n",
        "            if action == \"linear_regression\":\n",
        "                error = self.check_task_requirements(task, [\"X\", \"Y\"])\n",
        "                if error: return error\n",
        "                try:\n",
        "                    slope, intercept = self.linear_regression(task[\"X\"], task[\"Y\"])\n",
        "                    return {\"result\": {\"slope\": slope, \"intercept\": intercept}, \"agent\": self.name}\n",
        "                except Exception as e:\n",
        "                    return {\"error\": f\"Regression failed: {e}\", \"agent\": self.name}\n",
        "            elif action == \"analyze_data\":\n",
        "                 error = self.check_task_requirements(task, [\"data\"])\n",
        "                 if error: return error\n",
        "                 stats = self.analyze_data(task[\"data\"])\n",
        "                 return {\"result\": {\"statistics\": stats}, \"agent\": self.name}\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate other tasks\n",
        "\n",
        "\n",
        "    def linear_regression(self, X: Any, Y: Any) -> Tuple[float, float]:\n",
        "        \"\"\"Performs simple linear regression using F0Z-stabilized calculations.\"\"\"\n",
        "        try:\n",
        "            X_t = torch.tensor(X, dtype=torch.float32).squeeze() # Ensure 1D\n",
        "            Y_t = torch.tensor(Y, dtype=torch.float32).squeeze() # Ensure 1D\n",
        "            if X_t.dim() != 1 or Y_t.dim() != 1 or len(X_t) != len(Y_t) or len(X_t) < 2:\n",
        "                 raise ValueError(\"Inputs must be 1D arrays of the same length (>= 2).\")\n",
        "\n",
        "            # Using F0Z variance and mean\n",
        "            mean_x = torch.mean(X_t)\n",
        "            mean_y = torch.mean(Y_t)\n",
        "            # Stabilized variance of X\n",
        "            var_x_val = torch.mean((X_t - mean_x)**2)\n",
        "            var_x_s = self.math_module.f0z_stabilize(var_x_val, system_size=len(X_t))\n",
        "\n",
        "            if var_x_s.item() == 0: # Avoid division by zero if variance is stabilized to zero\n",
        "                 print(\"  Warning: Stabilized variance of X is zero. Cannot compute slope.\")\n",
        "                 return 0.0, mean_y.item() # Return slope 0, intercept as mean Y\n",
        "\n",
        "            # Covariance\n",
        "            cov_xy = torch.mean((X_t - mean_x) * (Y_t - mean_y))\n",
        "            cov_xy_s = self.math_module.f0z_stabilize(cov_xy, system_size=len(X_t))\n",
        "\n",
        "            # Slope = Cov(X, Y) / Var(X)\n",
        "            slope = cov_xy_s / var_x_s\n",
        "            slope_s = self.math_module.f0z_stabilize(slope, system_size=len(X_t))\n",
        "\n",
        "            # Intercept = mean(Y) - slope * mean(X)\n",
        "            intercept = mean_y - slope_s * mean_x\n",
        "            intercept_s = self.math_module.f0z_stabilize(intercept, system_size=len(X_t))\n",
        "\n",
        "            print(f\"  Linear Regression: Slope={slope_s.item():.4f}, Intercept={intercept_s.item():.4f}\")\n",
        "            return slope_s.item(), intercept_s.item()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during linear regression: {e}\")\n",
        "            raise # Re-raise exception\n",
        "\n",
        "\n",
        "    def analyze_data(self, data: Any) -> Dict:\n",
        "         \"\"\"Performs basic statistical analysis on data.\"\"\"\n",
        "         stats = {}\n",
        "         try:\n",
        "             data_np = np.array(data).flatten()\n",
        "             numeric_data = data_np[np.isfinite(data_np) & ~np.isnan(data_np)] # Filter out NaN/inf\n",
        "             if numeric_data.size == 0: return {\"error\": \"No valid numeric data found\"}\n",
        "\n",
        "             stats['count'] = len(numeric_data)\n",
        "             stats['mean'] = self.math_module.f0z_stabilize(torch.tensor(np.mean(numeric_data))).item()\n",
        "             stats['median'] = self.math_module.f0z_stabilize(torch.tensor(np.median(numeric_data))).item()\n",
        "             stats['std_dev'] = np.sqrt(F0ZAlgebra.f0z_variance(numeric_data))\n",
        "             stats['min'] = self.math_module.f0z_stabilize(torch.tensor(np.min(numeric_data))).item()\n",
        "             stats['max'] = self.math_module.f0z_stabilize(torch.tensor(np.max(numeric_data))).item()\n",
        "             print(f\"  Data Analysis: Mean={stats['mean']:.3f}, StdDev={stats['std_dev']:.3f}, Count={stats['count']}\")\n",
        "         except Exception as e:\n",
        "             print(f\"Error during data analysis: {e}\")\n",
        "             return {\"error\": str(e)}\n",
        "         return stats\n",
        "\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        return {\"model_count\": len(self.models)}\n",
        "\n",
        "\n",
        "class AstrophysicsAgent(DFSNAgent):\n",
        "    \"\"\"Agent for astrophysics calculations and simulations.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.orbital_data = {} # Store orbital parameters\n",
        "        self.G = 6.67430e-11 # Gravitational constant\n",
        "        print(f\"AstrophysicsAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles astrophysics tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"astrophysics\":\n",
        "            if action == \"compute_orbital_velocity\":\n",
        "                error = self.check_task_requirements(task, [\"central_mass\", \"distance\"])\n",
        "                if error: return error\n",
        "                velocity = self.compute_orbital_velocity(task[\"central_mass\"], task[\"distance\"])\n",
        "                return {\"result\": {\"orbital_velocity_mps\": velocity}, \"agent\": self.name}\n",
        "            elif action == \"simulate_orbit\":\n",
        "                 error = self.check_task_requirements(task, [\"central_mass\", \"body_mass\", \"initial_pos\", \"initial_vel\", \"time_steps\"])\n",
        "                 if error: return error\n",
        "                 trajectory = self.simulate_orbit(task[\"central_mass\"], task[\"body_mass\"], task[\"initial_pos\"], task[\"initial_vel\"], task[\"time_steps\"])\n",
        "                 return {\"result\": {\"trajectory\": trajectory}, \"agent\": self.name}\n",
        "\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate other tasks\n",
        "\n",
        "    def compute_orbital_velocity(self, central_mass: float, distance: float) -> float:\n",
        "        \"\"\"Computes circular orbital velocity using F0Z.\"\"\"\n",
        "        mass_t = torch.tensor(central_mass, dtype=torch.float64) # Use float64 for precision\n",
        "        dist_t = torch.tensor(distance, dtype=torch.float64)\n",
        "\n",
        "        dist_s = self.math_module.f0z_stabilize(dist_t, system_size=1)\n",
        "        if dist_s.item() <= 0:\n",
        "            print(\"  Warning: Distance is zero or negative. Returning zero velocity.\")\n",
        "            return 0.0\n",
        "\n",
        "        # v = sqrt(G * M / r)\n",
        "        velocity_sq = (self.G * mass_t) / dist_s\n",
        "        velocity_sq = torch.clamp(velocity_sq, min=0.0) # Ensure non-negative before sqrt\n",
        "        velocity = torch.sqrt(velocity_sq)\n",
        "\n",
        "        stabilized_velocity = self.math_module.f0z_stabilize(velocity, system_size=1)\n",
        "        print(f\"  Orbital Velocity: M={central_mass:.2e}, r={distance:.2e} => v={stabilized_velocity.item():.3f} m/s\")\n",
        "        return stabilized_velocity.item()\n",
        "\n",
        "    def simulate_orbit(self, central_mass: float, body_mass: float, initial_pos: List[float], initial_vel: List[float], time_steps: int, dt: float = 86400.0): # dt default = 1 day\n",
        "        \"\"\"Simulates a simple 2D orbit using F0Z-stabilized Euler method.\"\"\"\n",
        "        pos = torch.tensor(initial_pos, dtype=torch.float64)\n",
        "        vel = torch.tensor(initial_vel, dtype=torch.float64)\n",
        "        m1 = torch.tensor(central_mass, dtype=torch.float64)\n",
        "        # m2 = torch.tensor(body_mass, dtype=torch.float64) # Body mass often negligible\n",
        "\n",
        "        trajectory = [pos.tolist()]\n",
        "\n",
        "        for _ in range(time_steps):\n",
        "             # Calculate distance and direction vector\n",
        "             dist_vec = -pos # Vector from body to central mass at origin\n",
        "             distance = torch.norm(dist_vec)\n",
        "             dist_s = self.math_module.f0z_stabilize(distance, system_size=1)\n",
        "\n",
        "             if dist_s.item() <= 0:\n",
        "                 print(\"  Warning: Orbital distance became zero. Stopping simulation.\")\n",
        "                 break\n",
        "\n",
        "             # Calculate gravitational force F = G * m1 * m2 / r^2 (direction is dist_vec/distance)\n",
        "             # Acceleration a = F / m2 = G * m1 / r^2 * (dist_vec/distance)\n",
        "             accel_mag = (self.G * m1) / (dist_s**2)\n",
        "             accel_vec = accel_mag * (dist_vec / dist_s)\n",
        "             accel_s = self.math_module.f0z_stabilize(accel_vec, system_size=2)\n",
        "\n",
        "             # Update velocity and position using Euler method\n",
        "             vel = vel + accel_s * dt\n",
        "             pos = pos + vel * dt\n",
        "\n",
        "             # Stabilize position and velocity vectors\n",
        "             vel = self.math_module.f0z_stabilize(vel, system_size=2)\n",
        "             pos = self.math_module.f0z_stabilize(pos, system_size=2)\n",
        "\n",
        "             trajectory.append(pos.tolist())\n",
        "\n",
        "        print(f\"  Simulated orbit for {time_steps} steps.\")\n",
        "        return trajectory\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        return {\"orbital_data_count\": len(self.orbital_data)}\n",
        "\n",
        "\n",
        "class RoboticsAgent(DFSNAgent):\n",
        "    \"\"\"Agent for robotics tasks like path planning.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.path_data = [] # Store planned paths\n",
        "        print(f\"RoboticsAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles robotics tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"robotics\":\n",
        "            if action == \"plan_path\":\n",
        "                error = self.check_task_requirements(task, [\"start_pose\", \"goal_pose\", \"environment\"])\n",
        "                if error: return error\n",
        "                path = self.plan_path(task[\"start_pose\"], task[\"goal_pose\"], task[\"environment\"])\n",
        "                if path:\n",
        "                     self.path_data.append(path)\n",
        "                     return {\"result\": {\"path\": path}, \"agent\": self.name}\n",
        "                else:\n",
        "                     return {\"error\": \"Path planning failed\", \"agent\": self.name}\n",
        "            elif action == \"execute_path\":\n",
        "                 error = self.check_task_requirements(task, [\"path\"])\n",
        "                 if error: return error\n",
        "                 success = self.execute_path(task[\"path\"])\n",
        "                 return {\"result\": {\"execution_success\": success}, \"agent\": self.name}\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate other tasks\n",
        "\n",
        "\n",
        "    def plan_path(self, start_pose: Dict, goal_pose: Dict, environment: Dict) -> Optional[List[Dict]]:\n",
        "        \"\"\"Plans a path from start to goal (simplified A* placeholder).\"\"\"\n",
        "        print(f\"Planning path from {start_pose} to {goal_pose}...\")\n",
        "        # --- Placeholder A* or RRT* Implementation ---\n",
        "        # 1. Represent environment (e.g., grid map, obstacles)\n",
        "        # 2. Initialize open set (priority queue) with start node\n",
        "        # 3. Initialize closed set\n",
        "        # 4. Loop while open set is not empty:\n",
        "        #    a. Get node with lowest f-cost (g+h) from open set\n",
        "        #    b. If current is goal, reconstruct path and return\n",
        "        #    c. Add current to closed set\n",
        "        #    d. For each neighbor:\n",
        "        #       i. If neighbor is obstacle or in closed set, skip\n",
        "        #       ii. Calculate tentative g-cost\n",
        "        #       iii. If new path to neighbor is shorter OR neighbor not in open set:\n",
        "        #            - Update neighbor's g, h, f costs and parent\n",
        "        #            - Add neighbor to open set\n",
        "        # --- End Placeholder ---\n",
        "\n",
        "        # Simplified straight-line path for now\n",
        "        path = [start_pose, goal_pose] # Direct path\n",
        "        print(f\"  Planned simple path with {len(path)} points.\")\n",
        "\n",
        "        # Apply F0Z to waypoints if needed (e.g., stabilize coordinates)\n",
        "        stabilized_path = []\n",
        "        for pose in path:\n",
        "            # Assuming pose is like {'x': float, 'y': float, 'theta': float}\n",
        "            stable_pose = pose.copy()\n",
        "            for key, val in pose.items():\n",
        "                 if isinstance(val, (int, float)):\n",
        "                      stable_pose[key] = self.math_module.f0z_stabilize(torch.tensor(val)).item()\n",
        "            stabilized_path.append(stable_pose)\n",
        "\n",
        "\n",
        "        return stabilized_path\n",
        "\n",
        "    def execute_path(self, path: List[Dict]) -> bool:\n",
        "         \"\"\"Simulates executing a planned path.\"\"\"\n",
        "         if not path: return False\n",
        "         print(f\"Executing path with {len(path)} waypoints...\")\n",
        "         # Simulate movement, check for collisions etc.\n",
        "         time.sleep(0.05 * len(path)) # Simulate time taken\n",
        "         success = random.random() > 0.1 # 90% success rate\n",
        "         print(f\"  Path execution {'succeeded' if success else 'failed'}.\")\n",
        "         return success\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        return {\"paths_planned\": len(self.path_data)}\n",
        "\n",
        "\n",
        "class EnvironmentalScienceAgent(DFSNAgent):\n",
        "    \"\"\"Agent for environmental modeling and simulation.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.eco_data = {} # Store ecological parameters or simulation results\n",
        "        print(f\"EnvironmentalScienceAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles environmental science tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"environmental_science\":\n",
        "            if action == \"simulate_population\":\n",
        "                error = self.check_task_requirements(task, [\"initial_population\", \"growth_rate\", \"carrying_capacity\", \"time_steps\"])\n",
        "                if error: return error\n",
        "                population_trend = self.simulate_population_logistic(\n",
        "                    task[\"initial_population\"], task[\"growth_rate\"], task[\"carrying_capacity\"], task[\"time_steps\"]\n",
        "                )\n",
        "                return {\"result\": {\"population_trend\": population_trend}, \"agent\": self.name}\n",
        "            elif action == \"model_pollution_spread\":\n",
        "                 error = self.check_task_requirements(task, [\"source_location\", \"diffusion_rate\", \"grid_size\", \"time_steps\"])\n",
        "                 if error: return error\n",
        "                 concentration_grid = self.model_pollution_spread(\n",
        "                      task[\"source_location\"], task[\"diffusion_rate\"], task[\"grid_size\"], task[\"time_steps\"]\n",
        "                 )\n",
        "                 return {\"result\": {\"concentration_grid\": concentration_grid}, \"agent\": self.name}\n",
        "\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate other tasks\n",
        "\n",
        "    def simulate_population_logistic(self, initial_pop: float, growth_rate: float, capacity: float, time_steps: int) -> List[float]:\n",
        "        \"\"\"Simulates population growth using logistic model with F0Z.\"\"\"\n",
        "        pop = torch.tensor(initial_pop, dtype=torch.float32)\n",
        "        r = torch.tensor(growth_rate, dtype=torch.float32)\n",
        "        K = torch.tensor(capacity, dtype=torch.float32)\n",
        "        K_s = self.math_module.f0z_stabilize(K, system_size=1) # Stabilize carrying capacity\n",
        "        if K_s.item() <= 0: K_s = torch.tensor(1e6) # Ensure positive capacity\n",
        "\n",
        "        pop_trend = [pop.item()]\n",
        "\n",
        "        for _ in range(time_steps):\n",
        "             # Logistic growth: dP/dt = r * P * (1 - P/K)\n",
        "             dP_dt = r * pop * (1.0 - pop / K_s)\n",
        "             dP_dt_s = self.math_module.f0z_stabilize(dP_dt, system_size=1)\n",
        "             # Simple Euler integration\n",
        "             pop = pop + dP_dt_s # Assuming dt=1 step\n",
        "             pop = torch.clamp(pop, min=0.0) # Population cannot be negative\n",
        "             pop_s = self.math_module.f0z_stabilize(pop, system_size=1) # Stabilize pop each step\n",
        "             pop = pop_s\n",
        "             pop_trend.append(pop.item())\n",
        "\n",
        "        print(f\"  Simulated logistic population growth for {time_steps} steps. Final pop: {pop_trend[-1]:.2f}\")\n",
        "        return pop_trend\n",
        "\n",
        "\n",
        "    def model_pollution_spread(self, source_loc: Tuple[int, int], diffusion_rate: float, grid_size: Tuple[int, int], time_steps: int, dt: float = 0.1):\n",
        "        \"\"\"Simulates 2D pollution spread using diffusion equation (like heat eq).\"\"\"\n",
        "        rows, cols = grid_size\n",
        "        grid = torch.zeros(rows, cols, dtype=torch.float32)\n",
        "        alpha = torch.tensor(diffusion_rate, dtype=torch.float32)\n",
        "\n",
        "        # Add initial pollution source\n",
        "        sx, sy = source_loc\n",
        "        if 0 <= sx < rows and 0 <= sy < cols:\n",
        "            grid[sx, sy] = 100.0 # Initial concentration amount\n",
        "\n",
        "        # Spatial steps\n",
        "        dx, dy = 1.0, 1.0 # Assume unit spacing\n",
        "\n",
        "        print(f\"Simulating pollution spread on {rows}x{cols} grid for {time_steps} steps...\")\n",
        "        for _ in range(time_steps):\n",
        "            laplacian = torch.zeros_like(grid)\n",
        "            # Calculate Laplacian using finite differences (5-point stencil)\n",
        "            if rows > 2 and cols > 2:\n",
        "                 laplacian[1:-1, 1:-1] = (grid[:-2, 1:-1] + grid[2:, 1:-1] +\n",
        "                                          grid[1:-1, :-2] + grid[1:-1, 2:] -\n",
        "                                          4 * grid[1:-1, 1:-1]) / (dx**2) # Assume dx=dy\n",
        "\n",
        "            # Update grid: dC/dt = alpha * Laplacian(C)\n",
        "            grid = grid + alpha * dt * laplacian\n",
        "            # Apply F0Z stabilization\n",
        "            grid = self.math_module.f0z_stabilize(grid, system_size=grid.numel())\n",
        "            # Optional: Add boundary conditions (e.g., reflecting, absorbing)\n",
        "\n",
        "        print(\"  Pollution spread simulation complete.\")\n",
        "        return grid.tolist() # Return concentration grid as list of lists\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        return {\"eco_data_entries\": len(self.eco_data)}\n",
        "\n",
        "\n",
        "class MachineLearningAgent(DFSNAgent):\n",
        "    \"\"\"Agent specializing in machine learning tasks like training and prediction.\"\"\"\n",
        "    def __init__(self, name: str, model_params: Optional[Dict] = None):\n",
        "        super().__init__(name)\n",
        "        default_params = {\"input_features\": 2, \"output_features\": 1}\n",
        "        params = model_params if model_params else default_params\n",
        "\n",
        "        # --- CHANGE HERE: Use nn.Parameter ---\n",
        "        # Initialize weights and bias data first\n",
        "        weights_data = torch.randn(params[\"input_features\"], params[\"output_features\"], dtype=torch.float32) * 0.1\n",
        "        bias_data = torch.randn(params[\"output_features\"], dtype=torch.float32) * 0.1\n",
        "\n",
        "        # Wrap the data in nn.Parameter to register them as trainable parameters\n",
        "        self.learning_rate = 0.01 # Learning rate for optimizer\n",
        "        self.weights = nn.Parameter(weights_data)\n",
        "        self.bias = nn.Parameter(bias_data)\n",
        "\n",
        "        # --- CORRECTED OPTIMIZER INITIALIZATION ---\n",
        "        self.learning_rate = 0.01 # Define learning rate as attribute if needed elsewhere\n",
        "        # Pass only the parameters to be optimized in the list, lr is a keyword argument\n",
        "        self.optimizer = torch.optim.SGD([self.weights, self.bias], lr=self.learning_rate)\n",
        "        # --- END CORRECTION -\n",
        "\n",
        "        # requires_grad is True by default for nn.Parameter\n",
        "\n",
        "        # --- Optimizer creation should now work ---\n",
        "        #self.optimizer = torch.optim.SGD([self.weights, self.bias], lr=0.01)\n",
        "        self.loss_history = []\n",
        "        print(f\"MachineLearningAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles ML tasks like training and prediction.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"machine_learning\":\n",
        "            if action == \"train\":\n",
        "                error = self.check_task_requirements(task, [\"X\", \"Y\"])\n",
        "                if error: return error\n",
        "                try:\n",
        "                    X_data = task[\"X\"]\n",
        "                    Y_data = task[\"Y\"]\n",
        "                    loss = self.train_model(X_data, Y_data)\n",
        "                    self.loss_history.append(loss)\n",
        "                    return {\"result\": {\"final_loss\": loss}, \"agent\": self.name}\n",
        "                except Exception as e:\n",
        "                     # traceback.print_exc() # Debugging\n",
        "                     return {\"error\": f\"Training failed: {e}\", \"agent\": self.name}\n",
        "            elif action == \"predict\":\n",
        "                error = self.check_task_requirements(task, [\"X\"])\n",
        "                if error: return error\n",
        "                try:\n",
        "                     X_data = task[\"X\"]\n",
        "                     predictions = self.predict(X_data)\n",
        "                     return {\"result\": {\"predictions\": predictions}, \"agent\": self.name}\n",
        "                except Exception as e:\n",
        "                     # traceback.print_exc() # Debugging\n",
        "                     return {\"error\": f\"Prediction failed: {e}\", \"agent\": self.name}\n",
        "\n",
        "\n",
        "        # Delegate other tasks via AIW. AIW will call base__execute_single_task_iteration if needed.\n",
        "        # This agent doesn't need a specific base__execute_single_task_iteration override if it only handles ML tasks.\n",
        "        return super()._execute_single_task_iteration(task)\n",
        "\n",
        "\n",
        "    def train_model(self, X: Any, Y: Any, epochs: int = 5) -> float:\n",
        "        \"\"\"Trains the simple linear model using F0Z stabilization.\"\"\"\n",
        "        try:\n",
        "            X_t = torch.tensor(X, dtype=torch.float32)\n",
        "            Y_t = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "            # Reshape Y if necessary to match output dimensions\n",
        "            if Y_t.dim() == 1 and self.weights.shape[1] == 1:\n",
        "                 Y_t = Y_t.unsqueeze(1)\n",
        "            elif Y_t.dim() == 0 and self.weights.shape[1] == 1:\n",
        "                 Y_t = Y_t.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            # Basic input validation\n",
        "            if X_t.shape[0] != Y_t.shape[0]: raise ValueError(\"Number of samples in X and Y must match.\")\n",
        "            if X_t.shape[1] != self.weights.shape[0]: raise ValueError(f\"Input features mismatch: X has {X_t.shape[1]}, model expects {self.weights.shape[0]}.\")\n",
        "            if Y_t.shape[1] != self.weights.shape[1]: raise ValueError(f\"Output features mismatch: Y has {Y_t.shape[1]}, model expects {self.weights.shape[1]}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error processing training data: {e}\")\n",
        "             raise # Re-raise error\n",
        "\n",
        "        print(f\"  Training model on {X_t.shape[0]} samples for {epochs} epochs...\")\n",
        "        last_loss = float('inf')\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass: y_pred = X @ W + b\n",
        "            logits = X_t @ self.weights + self.bias\n",
        "            # Apply F0Z stabilization to model output (logits)\n",
        "            y_pred = self.math_module.f0z_stabilize(logits, system_size=X_t.shape[0])\n",
        "\n",
        "            # Loss calculation (MSE Loss)\n",
        "            loss = torch.mean((y_pred - Y_t)**2)\n",
        "            loss_s = self.math_module.f0z_stabilize(loss, system_size=1) # Stabilize loss value\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            self.optimizer.zero_grad()\n",
        "            loss_s.backward() # Backpropagate the stabilized loss\n",
        "\n",
        "            # Optional: Stabilize gradients before optimizer step\n",
        "            # for param in [self.weights, self.bias]:\n",
        "            #     if param.grad is not None:\n",
        "            #         param.grad = self.math_module.f0z_stabilize(param.grad, system_size=param.numel())\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            last_loss = loss_s.item()\n",
        "            if (epoch + 1) % (epochs // 2 + 1) == 0: # Print progress occasionally\n",
        "                 print(f\"    Epoch {epoch+1}/{epochs}, Loss: {last_loss:.6f}\")\n",
        "\n",
        "        print(f\"  Training complete. Final Loss: {last_loss:.6f}\")\n",
        "        return last_loss\n",
        "\n",
        "\n",
        "    def predict(self, X: Any) -> List:\n",
        "        \"\"\"Makes predictions using the trained linear model.\"\"\"\n",
        "        try:\n",
        "             X_t = torch.tensor(X, dtype=torch.float32)\n",
        "             if X_t.dim() == 1 and self.weights.shape[0] == 1: # Handle single feature input\n",
        "                 X_t = X_t.unsqueeze(1)\n",
        "             elif X_t.dim() == 1 and self.weights.shape[0] != 1:\n",
        "                  raise ValueError(f\"Input has 1 feature, model expects {self.weights.shape[0]}.\")\n",
        "             elif X_t.dim() > 1 and X_t.shape[1] != self.weights.shape[0]:\n",
        "                  raise ValueError(f\"Input features mismatch: X has {X_t.shape[1]}, model expects {self.weights.shape[0]}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error processing prediction data: {e}\")\n",
        "             raise # Re-raise error\n",
        "\n",
        "        print(f\"  Predicting for {X_t.shape[0]} samples...\")\n",
        "        with torch.no_grad(): # No need to track gradients during prediction\n",
        "            logits = X_t @ self.weights + self.bias\n",
        "            # Stabilize final prediction\n",
        "            y_pred = self.math_module.f0z_stabilize(logits, system_size=X_t.shape[0])\n",
        "        print(\"  Prediction complete.\")\n",
        "        return y_pred.tolist()\n",
        "\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        # Share model parameters (or hash/summary) and recent loss\n",
        "        return {\n",
        "            \"model_weights_norm\": torch.norm(self.weights).item(),\n",
        "            \"model_bias_norm\": torch.norm(self.bias).item(),\n",
        "            \"recent_loss\": self.loss_history[-1] if self.loss_history else None\n",
        "        }\n",
        "\n",
        "    def receive_state(self, state: Dict):\n",
        "        super().receive_state(state)\n",
        "        # Could implement model averaging or parameter updates based on peer state\n",
        "        domain_data = state.get(\"domain_data\", {})\n",
        "        peer_loss = domain_data.get(\"recent_loss\")\n",
        "        my_loss = self.loss_history[-1] if self.loss_history else None\n",
        "        # Example: If peer loss is significantly better, maybe slightly adjust weights towards peer? (Complex)\n",
        "        if peer_loss is not None and my_loss is not None and peer_loss < my_loss * 0.8:\n",
        "             print(f\"{self.name} observed better loss from peer ({peer_loss:.4f} vs {my_loss:.4f}). Considering adaptation (placeholder).\")\n",
        "\n",
        "\n",
        "class ValidationAgent(DFSNAgent):\n",
        "    \"\"\"Agent responsible for validating results produced by other agents.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.validation_history = [] # Store True/False results\n",
        "        print(f\"ValidationAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles validation tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"validation\":\n",
        "            if action == \"verify_result\":\n",
        "                error = self.check_task_requirements(task, [\"original_result\", \"task_data\"])\n",
        "                if error: return error\n",
        "                is_valid = self.verify_result(task[\"original_result\"], task[\"task_data\"])\n",
        "                self.validation_history.append(is_valid)\n",
        "                return {\"result\": {\"is_valid\": is_valid}, \"agent\": self.name}\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate other tasks\n",
        "\n",
        "    def verify_result(self, original_result: Any, task_data: Dict) -> bool:\n",
        "        \"\"\"Verifies a result by re-running or checking constraints (simplified).\"\"\"\n",
        "        task_type = task_data.get(\"type\")\n",
        "        action = task_data.get(\"action\")\n",
        "        print(f\"Validating result for task type '{task_type}', action '{action}'...\")\n",
        "\n",
        "        # --- Verification Logic (Task-Specific) ---\n",
        "        is_valid = True # Default to valid unless checks fail\n",
        "        tolerance = 1e-3 # Tolerance for numeric comparisons\n",
        "\n",
        "        try:\n",
        "            if task_type == \"machine_learning\" and action == \"predict\":\n",
        "                # Re-run prediction (needs access to model state - complex)\n",
        "                # Simplified check: Ensure output format/type is correct\n",
        "                if not isinstance(original_result, dict) or \"predictions\" not in original_result:\n",
        "                     is_valid = False; print(\"  Fail: ML prediction result format incorrect.\")\n",
        "                elif not isinstance(original_result[\"predictions\"], list):\n",
        "                     is_valid = False; print(\"  Fail: ML predictions are not a list.\")\n",
        "                # Could add range checks if expected output range is known\n",
        "\n",
        "            elif task_type == \"physics_simulation\" and action == \"fluid_dynamics\":\n",
        "                 # Check if velocity list has expected length/numeric values\n",
        "                 res_data = original_result.get(\"result\", {})\n",
        "                 vel = res_data.get(\"velocity\")\n",
        "                 grid_size = task_data.get(\"grid_size\")\n",
        "                 if not isinstance(vel, list): is_valid = False; print(\"  Fail: Velocity is not a list.\")\n",
        "                 elif grid_size and len(vel) != grid_size: is_valid = False; print(f\"  Fail: Velocity length mismatch ({len(vel)} vs {grid_size}).\")\n",
        "                 elif not all(isinstance(v, (int, float)) for v in vel): is_valid = False; print(\"  Fail: Velocity contains non-numeric values.\")\n",
        "                 # Could add physics constraints checks (e.g., max velocity)\n",
        "\n",
        "            elif task_type == \"temporal_forecast\":\n",
        "                 res_data = original_result.get(\"result\", {})\n",
        "                 forecast = res_data.get(\"forecast\")\n",
        "                 if not isinstance(forecast, (int, float)): is_valid = False; print(\"  Fail: Forecast is not numeric.\")\n",
        "                 # Could check if forecast is within reasonable bounds based on input data\n",
        "\n",
        "            # Add more verification rules for other task types...\n",
        "\n",
        "            else:\n",
        "                print(f\"  No specific verification rules for task type '{task_type}/{action}'. Assuming valid.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error during verification: {e}. Marking as invalid.\")\n",
        "            is_valid = False\n",
        "\n",
        "        print(f\"  Verification result: {'Valid' if is_valid else 'Invalid'}\")\n",
        "        return is_valid\n",
        "\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        # Share validation success rate\n",
        "        rate = sum(self.validation_history) / len(self.validation_history) if self.validation_history else 0.0\n",
        "        return {\"validation_success_rate\": rate, \"validations_performed\": len(self.validation_history)}\n",
        "\n",
        "\n",
        "class FractalAgent(DFSNAgent):\n",
        "    \"\"\"Agent for generating and analyzing fractals using F0Z.\"\"\"\n",
        "    def __init__(self, name: str, config: Optional[Dict]=None, primes: Optional[List[int]]=None): # Matches TemporalPrimeAgent structure\n",
        "        super().__init__(name)\n",
        "        default_config = {\"prime_limit\": 100}\n",
        "        self.config = config if config else default_config\n",
        "        self.primes = primes if primes else self._generate_primes(self.config[\"prime_limit\"])\n",
        "        print(f\"FractalAgent {self.name} initialized.\")\n",
        "\n",
        "    def _generate_primes(self, n): # Duplicated from TemporalPrimeAgent - should be in a utility module\n",
        "        primes = []\n",
        "        is_prime = [True] * (n + 1)\n",
        "        if n >= 0: is_prime[0] = False\n",
        "        if n >= 1: is_prime[1] = False\n",
        "        for p in range(2, int(np.sqrt(n)) + 1):\n",
        "            if is_prime[p]:\n",
        "                for i in range(p * p, n + 1, p):\n",
        "                    is_prime[i] = False\n",
        "        for p in range(2, n + 1):\n",
        "            if is_prime[p]:\n",
        "                primes.append(p)\n",
        "        return primes if primes else [2, 3, 5]\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles fractal generation tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"fractal_generate\": # Assuming type implies action here\n",
        "             # Keys based on RealTimeForecastingManager example usage\n",
        "             required_keys = [\"initial_c\", \"iterations\", \"embeddings\", \"gamma_factor\",\n",
        "                              \"hnn_state\", \"x_t\", \"delta\", \"gamma\", \"h_t\", \"use_dl\"]\n",
        "             error = self.check_task_requirements(task, required_keys)\n",
        "             if error: return error\n",
        "\n",
        "             try:\n",
        "                 fractal_data, final_z = self.fractal_generator(\n",
        "                     task[\"initial_c\"], task[\"iterations\"],\n",
        "                     task[\"embeddings\"], task[\"gamma_factor\"], task[\"hnn_state\"],\n",
        "                     task[\"x_t\"], task[\"delta\"], task[\"gamma\"], task[\"h_t\"], task[\"use_dl\"]\n",
        "                 )\n",
        "                 # Compute fractal dimension (placeholder)\n",
        "                 dimension = self.detect_fractals({\"trajectory\": fractal_data})\n",
        "                 return {\"result\": {\"fractal_sequence\": fractal_data, \"final_z\": final_z, \"fractal_dimension\": dimension}, \"agent\": self.name}\n",
        "             except Exception as e:\n",
        "                  return {\"error\": f\"Fractal generation failed: {e}\", \"agent\": self.name}\n",
        "\n",
        "        elif action == \"detect_fractals\": # Task to analyze existing data\n",
        "             error = self.check_task_requirements(task, [\"data\"])\n",
        "             if error: return error\n",
        "             dimension = self.detect_fractals({\"trajectory\": task[\"data\"]}) # Pass data structure expected by detect_fractals\n",
        "             return {\"result\": {\"fractal_dimension\": dimension}, \"agent\": self.name}\n",
        "\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate other tasks\n",
        "\n",
        "    def fractal_generator(self, initial_c, iterations, embeddings, gamma_factor, hnn_state, x_t, delta, gamma, h_t, use_dl):\n",
        "        \"\"\"\n",
        "        Generates a fractal-like sequence based on complex dynamics influenced by multiple inputs,\n",
        "        using F0Z stabilization. Adapted from RealTimeForecastingManager example.\n",
        "        \"\"\"\n",
        "        if not self.primes: return [], 0.0 # Need primes\n",
        "\n",
        "        # Ensure inputs are usable numpy arrays or scalars\n",
        "        embeddings = np.array(embeddings) if isinstance(embeddings, list) else embeddings\n",
        "        hnn_state = np.array(hnn_state) if isinstance(hnn_state, list) else hnn_state\n",
        "        x_t = np.array(x_t) if isinstance(x_t, list) else x_t\n",
        "        h_t = np.array(h_t) if isinstance(h_t, list) else h_t\n",
        "\n",
        "        # Calculate mean values, handle potential empty arrays\n",
        "        mu_p = np.mean(self.primes)\n",
        "        mu_e = np.mean(embeddings) if embeddings.size > 0 else 0.0\n",
        "        mean_hnn = np.mean(hnn_state) if hnn_state.size > 0 else 0.0\n",
        "        mean_xt = np.mean(x_t) if x_t.size > 0 else 0.0\n",
        "        mean_ht = np.mean(h_t) if h_t.size > 0 else 0.0\n",
        "\n",
        "        # Use median for robust error, handle potential NaN from mean_xt\n",
        "        pred = mu_e # Simple prediction based on embeddings mean\n",
        "        robust_error = np.median([mean_xt - pred, 0.1]) if not np.isnan(mean_xt) else 0.1\n",
        "\n",
        "        # Initial complex value z\n",
        "        z = torch.tensor(initial_c, dtype=torch.complex64) # Start with initial_c as complex\n",
        "        fractal_sequence = []\n",
        "\n",
        "        print(f\"  Generating fractal sequence ({iterations} iterations)...\")\n",
        "        for t in range(iterations):\n",
        "            # Calculate the complex parameter 'c' dynamically\n",
        "            # Combine influences: primes, embeddings, iteration, hnn, input data, error, hidden state, dl flag\n",
        "            c_real = (mu_p + mu_e +\n",
        "                      t * gamma_factor + # Iteration influence\n",
        "                      mean_hnn + # Hopfield state influence\n",
        "                      0.2 * mean_xt + # Recent input data influence\n",
        "                      delta * robust_error + # Error feedback\n",
        "                      gamma * mean_ht * use_dl) # Influence from temporal DL state\n",
        "\n",
        "            # Introduce imaginary part based on variance or other metric (example)\n",
        "            c_imag = 0.1 * np.std(x_t) if x_t.size > 1 else 0.01\n",
        "            c_imag = np.nan_to_num(c_imag) # Handle NaN if std dev fails\n",
        "\n",
        "            c = torch.tensor(c_real + 1j * c_imag, dtype=torch.complex64)\n",
        "            c_s = self.math_module.f0z_stabilize(c, system_size=iterations) # Stabilize c\n",
        "\n",
        "            # Apply the fractal iteration: z = z^2 + c\n",
        "            z = z**2 + c_s\n",
        "            z = self.math_module.f0z_stabilize(z, system_size=iterations) # Stabilize z\n",
        "\n",
        "            # Optional: Bounding z to prevent explosion (though F0Z helps)\n",
        "            # z = z / (1 + torch.abs(z)) # Normalize/bound\n",
        "\n",
        "            fractal_sequence.append(z.item()) # Store complex value\n",
        "\n",
        "        # Add variance of the sequence (real part) to performance history\n",
        "        real_parts = [v.real for v in fractal_sequence]\n",
        "        variance = F0ZAlgebra.f0z_variance(real_parts) if real_parts else 0.0\n",
        "        self.performance_history.append(variance)\n",
        "\n",
        "        final_z = fractal_sequence[-1] if fractal_sequence else initial_c + 0j\n",
        "        print(f\"  Fractal generation complete. Final z = {final_z:.3f}\")\n",
        "        return fractal_sequence, final_z # Return sequence and final complex value\n",
        "\n",
        "\n",
        "    def detect_fractals(self, data: Dict) -> float:\n",
        "        \"\"\"Estimates fractal dimension (placeholder).\"\"\"\n",
        "        trajectory = data.get(\"trajectory\")\n",
        "        if trajectory is None or len(trajectory) < 10:\n",
        "            return 0.0 # Not enough data\n",
        "\n",
        "        # Example: Box-counting dimension (simplified)\n",
        "        # This requires a proper implementation, treating trajectory as points in space\n",
        "        # Placeholder: Return a value based on variance or complexity\n",
        "        try:\n",
        "             # Use variance of real part as proxy for complexity/fractal nature\n",
        "             if isinstance(trajectory[0], complex):\n",
        "                 real_parts = [v.real for v in trajectory]\n",
        "                 variance = np.var(real_parts)\n",
        "             elif isinstance(trajectory[0], (list, np.ndarray)): # Handle simulation trajectories\n",
        "                 flat_traj = np.array(trajectory).flatten()\n",
        "                 numeric_traj = flat_traj[np.isfinite(flat_traj)]\n",
        "                 variance = np.var(numeric_traj) if numeric_traj.size > 0 else 0.0\n",
        "             else: # Assume list of numbers\n",
        "                 numeric_traj = np.array(trajectory)[np.isfinite(trajectory)]\n",
        "                 variance = np.var(numeric_traj) if numeric_traj.size > 0 else 0.0\n",
        "\n",
        "             # Map variance to a dimension-like value (0 to 2)\n",
        "             dimension = 1.0 + np.tanh(variance / 10.0) # Scale variance effect\n",
        "             dimension = np.clip(dimension, 1.0, 2.0) # Typical range for simple fractals\n",
        "             print(f\"  Estimated fractal dimension (proxy): {dimension:.3f} (based on variance {variance:.3f})\")\n",
        "             return dimension\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating fractal dimension proxy: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        # Share prime numbers used\n",
        "        return {\"primes_used_sample\": self.primes[:10]}\n",
        "\n",
        "\n",
        "class HopfieldAgent(DFSNAgent):\n",
        "    \"\"\"Agent for Hopfield Neural Network operations.\"\"\"\n",
        "    def __init__(self, name: str, config: Optional[Dict]=None, primes: Optional[List[int]]=None):\n",
        "        super().__init__(name)\n",
        "        default_config = {\"hnn_size\": 100, \"sparsity\": 0.7, \"prime_limit\": 100}\n",
        "        self.config = config if config else default_config\n",
        "        self.size = self.config.get(\"hnn_size\", 100)\n",
        "        self.primes = primes if primes else self._generate_primes(self.config[\"prime_limit\"])\n",
        "        # Initialize weights with F0Z in mind (potentially small random values)\n",
        "        self.weights = torch.randn(self.size, self.size, dtype=torch.float32) * 0.01\n",
        "        self.weights.fill_diagonal_(0) # No self-connections\n",
        "        self.sparsity = self.config.get(\"sparsity\", 0.7)\n",
        "        self._apply_sparsity()\n",
        "        print(f\"HopfieldAgent {self.name} initialized (Size: {self.size}, Sparsity: {self.sparsity}).\")\n",
        "\n",
        "    def _generate_primes(self, n): # Duplicated code - refactor needed\n",
        "        primes = []\n",
        "        is_prime = [True] * (n + 1)\n",
        "        if n >= 0: is_prime[0] = False\n",
        "        if n >= 1: is_prime[1] = False\n",
        "        for p in range(2, int(np.sqrt(n)) + 1):\n",
        "            if is_prime[p]:\n",
        "                for i in range(p * p, n + 1, p):\n",
        "                    is_prime[i] = False\n",
        "        for p in range(2, n + 1):\n",
        "            if is_prime[p]:\n",
        "                primes.append(p)\n",
        "        return primes if primes else [2, 3, 5]\n",
        "\n",
        "    def _apply_sparsity(self):\n",
        "         \"\"\"Applies sparsity mask to the weights.\"\"\"\n",
        "         mask = torch.rand(self.weights.size()) > self.sparsity\n",
        "         self.weights.data *= mask.float()\n",
        "         print(f\"  Applied sparsity ({self.sparsity}) to Hopfield weights.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles Hopfield network tasks like state updates.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"hnn_update\": # Assuming type implies action\n",
        "            required_keys = [\"state\", \"context_vector\", \"zero_equilibrium\", \"alpha\", \"t\",\n",
        "                             \"x_t\", \"delta\", \"gamma\", \"h_t\", \"use_dl\"]\n",
        "            error = self.check_task_requirements(task, required_keys)\n",
        "            if error: return error\n",
        "\n",
        "            try:\n",
        "                 new_state = self.update_state(\n",
        "                     task[\"state\"], task[\"context_vector\"], task[\"zero_equilibrium\"], task[\"alpha\"], task[\"t\"],\n",
        "                     task[\"x_t\"], task[\"delta\"], task[\"gamma\"], task[\"h_t\"], task[\"use_dl\"]\n",
        "                 )\n",
        "                 return {\"result\": {\"hnn_state\": new_state.tolist()}, \"agent\": self.name}\n",
        "            except Exception as e:\n",
        "                 return {\"error\": f\"HNN update failed: {e}\", \"agent\": self.name}\n",
        "        elif task_type == \"hnn_train\":\n",
        "             error = self.check_task_requirements(task, [\"patterns\"])\n",
        "             if error: return error\n",
        "             self.train(task[\"patterns\"])\n",
        "             return {\"status\": \"HNN trained\", \"agent\": self.name}\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate other tasks\n",
        "\n",
        "\n",
        "    def train(self, patterns: List[List[int]]):\n",
        "        \"\"\"Trains the Hopfield network using Hebbian rule with F0Z and primes.\"\"\"\n",
        "        if not patterns or not isinstance(patterns[0], (list, np.ndarray)):\n",
        "             print(\"Error: HNN training requires a list of patterns (lists/arrays).\")\n",
        "             return\n",
        "\n",
        "        num_patterns = len(patterns)\n",
        "        pattern_len = len(patterns[0])\n",
        "        if pattern_len != self.size:\n",
        "             print(f\"Error: Pattern length ({pattern_len}) does not match network size ({self.size}).\")\n",
        "             return\n",
        "\n",
        "        print(f\"Training Hopfield network with {num_patterns} patterns...\")\n",
        "        # Hebbian rule: W_ij = sum_p (pattern_p_i * pattern_p_j) / N\n",
        "        new_weights = torch.zeros_like(self.weights)\n",
        "        patterns_t = torch.tensor(patterns, dtype=torch.float32) # Convert patterns to tensor [-1, 1]\n",
        "\n",
        "        # Incorporate primes into training (example: weighted sum)\n",
        "        prime_weights = torch.tensor(self.primes[:num_patterns] / np.sum(self.primes[:num_patterns]), dtype=torch.float32) if self.primes else torch.ones(num_patterns) / num_patterns\n",
        "        if len(prime_weights) < num_patterns: # Repeat primes if not enough\n",
        "            prime_weights = prime_weights.repeat(num_patterns // len(prime_weights) + 1)[:num_patterns]\n",
        "\n",
        "        for p_idx in range(num_patterns):\n",
        "            pattern = patterns_t[p_idx]\n",
        "            # Outer product: pattern.view(-1, 1) * pattern.view(1, -1)\n",
        "            weight_update = torch.outer(pattern, pattern) * prime_weights[p_idx]\n",
        "            new_weights += weight_update\n",
        "\n",
        "        # Normalize weights (optional) and apply F0Z\n",
        "        new_weights /= num_patterns # Average contribution\n",
        "        new_weights.fill_diagonal_(0) # Ensure no self-connections\n",
        "        stabilized_weights = self.math_module.f0z_stabilize(new_weights, system_size=self.size*self.size)\n",
        "\n",
        "        # Combine with existing weights (e.g., incremental learning) or replace\n",
        "        self.weights = 0.8 * self.weights + 0.2 * stabilized_weights # Moving average update\n",
        "        self._apply_sparsity() # Re-apply sparsity after training\n",
        "        print(\"Hopfield network training complete.\")\n",
        "\n",
        "\n",
        "    def update_state(self, state, context_vector, zero_equilibrium, alpha, t, x_t, delta, gamma, h_t, use_dl):\n",
        "        \"\"\"Updates the network state asynchronously with F0Z and external influences.\"\"\"\n",
        "        state_t = torch.tensor(state, dtype=torch.float32)\n",
        "        context_t = torch.tensor(context_vector, dtype=torch.float32) if context_vector is not None else torch.zeros(self.size)\n",
        "        xt_t = torch.tensor(x_t[-self.size:], dtype=torch.float32) if x_t else torch.zeros(self.size) # Use recent part of x_t\n",
        "        ht_t = torch.tensor(h_t, dtype=torch.float32) if h_t is not None else torch.zeros(self.size) # Use DL hidden state\n",
        "\n",
        "        if len(xt_t) < self.size: xt_t = torch.cat((torch.zeros(self.size - len(xt_t)), xt_t)) # Pad if needed\n",
        "        if len(ht_t) < self.size: ht_t = torch.cat((torch.zeros(self.size - len(ht_t)), ht_t)) # Pad if needed\n",
        "\n",
        "\n",
        "        # Calculate input signal: h_i = sum_j (W_ij * s_j)\n",
        "        signal = self.weights @ state_t\n",
        "\n",
        "        # Calculate inhibition term based on context and equilibrium\n",
        "        inhibition_base = zero_equilibrium + torch.dot(context_t[:self.size], state_t) # Ensure context matches size\n",
        "        flow_factor = 2.5 if self.flow_state == \"flow\" else 1.0\n",
        "        inhibition = inhibition_base * flow_factor\n",
        "\n",
        "        # Calculate prediction and robust error (using stabilized mean)\n",
        "        pred = self.math_module.f0z_stabilize(torch.mean(state_t)).item()\n",
        "        mean_xt = self.math_module.f0z_stabilize(torch.mean(xt_t)).item()\n",
        "        robust_error = np.median([mean_xt - pred, 0.1])\n",
        "\n",
        "        # Prime influence for this step\n",
        "        prime_influence = alpha * self.primes[t % len(self.primes)] if self.primes else 0.0\n",
        "\n",
        "        # Combine all influences\n",
        "        combined_input = (signal - inhibition +\n",
        "                          prime_influence +\n",
        "                          0.2 * xt_t + # Direct influence from recent input data\n",
        "                          delta * robust_error + # Error feedback term\n",
        "                          gamma * ht_t * use_dl) # Influence from temporal DL state\n",
        "\n",
        "        # Apply F0Z stabilization to the combined input before activation\n",
        "        stabilized_input = self.math_module.f0z_stabilize(combined_input, system_size=self.size)\n",
        "\n",
        "        # Activation function (sign function for Hopfield)\n",
        "        new_state = torch.sign(stabilized_input)\n",
        "        # Ensure output is binary (-1 or 1), handle zero case\n",
        "        new_state = torch.where(new_state == 0, torch.tensor(1.0), new_state)\n",
        "\n",
        "        # Update performance history (e.g., stability or distance to target pattern)\n",
        "        # Using distance to the input x_t as a simple measure\n",
        "        if xt_t.shape == new_state.shape:\n",
        "             dist = torch.mean(torch.abs(new_state - torch.sign(xt_t))) # Compare signs\n",
        "             self.performance_history.append(1.0 - dist.item()) # Performance = 1 - distance\n",
        "        else:\n",
        "             self.performance_history.append(0.5) # Default performance if shapes mismatch\n",
        "\n",
        "        # print(f\"  Hopfield state updated. Change magnitude: {torch.norm(new_state - state_t):.3f}\")\n",
        "        return new_state.numpy() # Return as numpy array\n",
        "\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        # Share weight matrix norm or sparsity info\n",
        "        return {\"weight_norm\": torch.norm(self.weights).item(), \"sparsity\": self.sparsity}\n",
        "\n",
        "    def receive_state(self, state: Dict):\n",
        "        super().receive_state(state)\n",
        "        # Could average weights with peer if compatible size/sparsity\n",
        "        domain_data = state.get(\"domain_data\", {})\n",
        "        if \"weights\" in domain_data and isinstance(domain_data[\"weights\"], list):\n",
        "             try:\n",
        "                 peer_weights = torch.tensor(domain_data[\"weights\"])\n",
        "                 if peer_weights.shape == self.weights.shape:\n",
        "                     # Average weights\n",
        "                     self.weights = 0.7 * self.weights + 0.3 * peer_weights\n",
        "                     self.weights.fill_diagonal_(0) # Ensure no self-connections\n",
        "                     self._apply_sparsity() # Re-apply sparsity\n",
        "                     print(f\"{self.name} averaged weights with peer.\")\n",
        "                 else:\n",
        "                      print(f\"{self.name} received incompatible weights from peer.\")\n",
        "             except Exception as e:\n",
        "                  print(f\"{self.name} failed to process peer weights: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "class CollaborativeAgent(DFSNAgent):\n",
        "    \"\"\"Agent focused on collaboration, state sharing, and synchronization.\"\"\"\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name); self.peers: List[str] = []; self.shared_data: Dict[str, Any] = {}\n",
        "        self.agent_registry: Optional[Dict[str, Agent]] = None; print(f\"CollaborativeAgent {self.name} initialized.\")\n",
        "    def set_agent_registry(self, registry: Dict[str, Agent]): self.agent_registry = registry\n",
        "    def add_peer(self, peer_name: str):\n",
        "        if peer_name not in self.peers: self.peers.append(peer_name); print(f\"{self.name} added peer: {peer_name}\")\n",
        "    def find_peer_object(self, peer_name: str) -> Optional[Agent]:\n",
        "         if not self.agent_registry: print(f\"Error: Agent registry not set for {self.name}.\"); return None\n",
        "         peer = self.agent_registry.get(peer_name); # if not peer: print(f\"Error: Peer '{peer_name}' not found.\") # Reduce noise\n",
        "         return peer\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        action = task.get(\"action\"); task_type = task.get(\"type\")\n",
        "        print(f\"DEBUG: CollabAgent received task: type='{task_type}', action='{action}'\") # <-- ADD DEBUG\n",
        "\n",
        "        # Handle science fair experiment first (as corrected before)\n",
        "        if task_type == \"science_fair_experiment\":\n",
        "            exp_desc = task.get('data', {}).get('prompt', task.get('description', 'N/A'))\n",
        "            print(f\"{self.name} running science fair experiment: {exp_desc}\")\n",
        "            time.sleep(0.1 * task.get('complexity', 1.0)); result_value = random.uniform(0.5, 1.0)\n",
        "            # Ensure the key matches what ZSGBatchModeScienceFair expects\n",
        "            return {\"result\": {\"experiment_outcome\": result_value}, \"agent\": self.name}\n",
        "\n",
        "        # ... (collaborate, synchronize_with_peers, receive_state, get_domain_specific_state methods) ...\n",
        "        # THEN handle collaboration tasks\n",
        "        elif task_type == \"collaboration\":\n",
        "            print(f\"DEBUG: CollabAgent handling collaboration action: '{action}'\") # <-- ADD DEBUG\n",
        "\n",
        "            if action == \"collaborate_on_task\":\n",
        "                print(f\"DEBUG: CollabAgent executing COLLAB ON TASK action.\") # <-- ADD DEBUG\n",
        "                error = self.check_task_requirements(task, [\"data\", \"peer_names\"]) # Check task['data']\n",
        "                if error: return error\n",
        "                # Pass task['data']['task_data'] if it's nested like that, otherwise just task['data']\n",
        "                task_content = task.get(\"data\", {}).get(\"task_data\", task.get(\"data\"))\n",
        "                peer_names = task.get(\"data\", {}).get(\"peer_names\", [])\n",
        "                result = self.collaborate(task_content, peer_names)\n",
        "                return {\"result\": result, \"agent\": self.name} # Return result directly\n",
        "\n",
        "            elif action == \"synchronize\":\n",
        "                self.synchronize_with_peers(); return {\"status\": \"sync complete\", \"agent\": self.name}\n",
        "                print(f\"DEBUG: CollabAgent executing SYNCHRONIZE action.\") # <-- ADD DEBUG\n",
        "\n",
        "            elif action == \"combine\": # From Science Fair example\n",
        "                print(f\"DEBUG: CollabAgent executing COMBINE action.\") # <-- ADD DEBUG\n",
        "                error = self.check_task_requirements(task, [\"data\"]) # Checks task['data'] exists\n",
        "                if error: return error\n",
        "                # --- Combine Logic ---\n",
        "                combined_data = {}\n",
        "                # task[\"data\"] should be {\"batch_results\": [...], \"forecast\": ...}\n",
        "                combine_input = task.get(\"data\", {})\n",
        "                for key, value in combine_input.items():\n",
        "                     combined_data[f\"{key}_combined\"] = value # Add suffix\n",
        "                print(f\"{self.name}: Combined data for keys: {list(combine_input.keys())}\")\n",
        "                return {\"result\": combined_data, \"agent\": self.name} # Return combined data\n",
        "            else:\n",
        "                 # Unrecognized collaboration action\n",
        "                 return {\"error\": f\"Unsupported collaboration action: {action}\", \"agent\": self.name}\n",
        "\n",
        "        # Delegate tasks not handled above\n",
        "        return super()._execute_single_task_iteration(task)\n",
        "\n",
        "    # Need to update collaborate method slightly based on the change above\n",
        "    def collaborate(self, task_content: Any, peer_names: List[str]) -> Dict:\n",
        "        print(f\"{self.name} initiating collaboration on task with peers: {peer_names}\")\n",
        "        if not self.agent_registry: return {\"error\": \"Agent registry not available.\"}\n",
        "        results = {}\n",
        "        for peer_name in peer_names:\n",
        "            peer = self.find_peer_object(peer_name)\n",
        "            if peer and isinstance(peer, DFSNAgent):\n",
        "                print(f\"  Sharing state and task with {peer_name}\")\n",
        "                self.share_state(peer)\n",
        "                # Create the sub-task for the peer\n",
        "                peer_task = {\n",
        "                    \"type\": \"collaborative_subtask\", # Give it a type\n",
        "                    \"action\": \"process_collab_data\", # Define an action for the peer\n",
        "                    \"data\": task_content, # Pass the actual task content\n",
        "                    \"collaborator\": self.name,\n",
        "                    \"complexity\": 5.0 # Assign complexity\n",
        "                }\n",
        "                peer_result = peer.execute_task(peer_task) # Use execute_task -> AIW\n",
        "                results[peer_name] = peer_result\n",
        "            else: results[peer_name] = {\"error\": \"Peer unavailable\"}\n",
        "        final_result = {\"collaboration_summary\": f\"Results from {len(results)} peers.\", \"details\": results}\n",
        "        return final_result\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        action = task.get(\"action\"); task_type = task.get(\"type\")\n",
        "        # --- Logic from previous correction ---\n",
        "        if task.get(\"type\") == \"science_fair_experiment\":\n",
        "            exp_desc = task.get('data', {}).get('prompt', task.get('description', 'N/A')) # Corrected description access\n",
        "            print(f\"{self.name} running science fair experiment: {exp_desc}\")\n",
        "            time.sleep(0.1 * task.get('complexity', 1.0)); result_value = random.uniform(0.5, 1.0)\n",
        "            return {\"result\": {\"experiment_outcome\": result_value}, \"agent\": self.name}\n",
        "        # --- End Logic from previous correction ---\n",
        "        elif task_type == \"collaboration\":\n",
        "            if action == \"collaborate_on_task\": error = self.check_task_requirements(task, [\"task_data\", \"peer_names\"]); # ... (rest of collab logic) ...\n",
        "            elif action == \"synchronize\": self.synchronize_with_peers(); return {\"status\": \"sync complete\", \"agent\": self.name}\n",
        "            elif action == \"combine\": error = self.check_task_requirements(task, [\"data\"]); # ... (rest of combine logic) ...\n",
        "        return super()._execute_single_task_iteration(task)\n",
        "\n",
        "# --- Qiskit Integration ---\n",
        "\n",
        "# --- ZSGQuantumSimulator Modification ---\n",
        "class ZSGQuantumSimulator:\n",
        "    \"\"\"Simulates quantum circuits using Qiskit AerSimulator.\"\"\"\n",
        "    def __init__(self, n_qubits: int, noise_level: float = 0.01, use_noise: bool = True):\n",
        "        if QuantumCircuit is None: raise ImportError(\"Qiskit not found.\")\n",
        "        self.n_qubits = n_qubits\n",
        "        self.circuit = QuantumCircuit(n_qubits)\n",
        "        self.math_module = PyZeroMathTorch()\n",
        "        self.noise_level = noise_level\n",
        "        self.simulator = AerSimulator() # Default ideal simulator\n",
        "        if use_noise and depolarizing_error:\n",
        "            print(f\"Setting up Qiskit noise model (Depolarizing p={self.noise_level})...\")\n",
        "            self.noise_model = NoiseModel()\n",
        "            error_1q = depolarizing_error(self.noise_level, 1)\n",
        "            error_2q = depolarizing_error(self.noise_level * 5, 2)\n",
        "            self.noise_model.add_all_qubit_quantum_error(error_1q, ['u1', 'u2', 'u3', 'h', 'x', 'y', 'z', 'rz', 'ry']) # Added Y, Z\n",
        "            self.noise_model.add_all_qubit_quantum_error(error_2q, ['cx', 'cz'])\n",
        "            self.simulator = AerSimulator(noise_model=self.noise_model)\n",
        "            print(\"Qiskit AerSimulator configured with noise model.\")\n",
        "        else: self.noise_model = None; print(\"Qiskit AerSimulator configured without noise.\")\n",
        "\n",
        "    def _reset_circuit(self): self.circuit = QuantumCircuit(self.n_qubits)\n",
        "    # --- Redefined gate methods ---\n",
        "    def h_gate(self, qubit: int): self.circuit.h(qubit)\n",
        "    def x_gate(self, qubit: int): self.circuit.x(qubit)\n",
        "    def y_gate(self, qubit: int): self.circuit.y(qubit)\n",
        "    def z_gate(self, qubit: int): self.circuit.z(qubit)\n",
        "    def rz_gate(self, angle: float, qubit: int): self.circuit.rz(angle, qubit)\n",
        "    def ry_gate(self, angle: float, qubit: int): self.circuit.ry(angle, qubit)\n",
        "    def cz_gate(self, control_qubit: int, target_qubit: int): self.circuit.cz(control_qubit, target_qubit)\n",
        "    def cnot_gate(self, control_qubit: int, target_qubit: int): self.circuit.cx(control_qubit, target_qubit)\n",
        "\n",
        "    def measure(self, qubit_indices: Optional[List[int]] = None, shots: int = 1024) -> Dict:\n",
        "        if qubit_indices is None: qubit_indices = list(range(self.n_qubits))\n",
        "        meas_circuit = self.circuit.copy(); num_classical_bits = len(qubit_indices)\n",
        "        if num_classical_bits > 0:\n",
        "            cr_name = 'c' # Default classical register name\n",
        "            if not meas_circuit.cregs or cr_name not in [reg.name for reg in meas_circuit.cregs]:\n",
        "                 cr = ClassicalRegister(num_classical_bits, name=cr_name)\n",
        "                 meas_circuit.add_register(cr)\n",
        "            else:\n",
        "                 # Ensure existing register is large enough\n",
        "                 existing_cr = next(reg for reg in meas_circuit.cregs if reg.name == cr_name)\n",
        "                 if existing_cr.size < num_classical_bits:\n",
        "                      # This is tricky - might need to remove/add register or handle error\n",
        "                      print(f\"Warning: Existing classical register '{cr_name}' is too small.\")\n",
        "                      # Attempt to use available bits\n",
        "                      qubit_indices = qubit_indices[:existing_cr.size]\n",
        "                      num_classical_bits = len(qubit_indices)\n",
        "                      if num_classical_bits == 0: return {\"counts\": {}, \"notes\": \"Classical register size mismatch\"}\n",
        "\n",
        "            # Map qubits to classical bits\n",
        "            meas_map = [(qubit_indices[i], i) for i in range(num_classical_bits)]\n",
        "            meas_circuit.measure([m[0] for m in meas_map], [m[1] for m in meas_map]) # Measure specified qubits to first N classical bits\n",
        "\n",
        "            # transpiled_circuit = transpile(meas_circuit, self.simulator) # Often needed\n",
        "            try:\n",
        "                 job = self.simulator.run(meas_circuit, shots=shots); result = job.result(); counts = result.get_counts(0)\n",
        "                 total_shots = sum(counts.values())\n",
        "                 probabilities = {state: count / total_shots for state, count in counts.items()}\n",
        "                 return {\"counts\": counts, \"probabilities\": probabilities, \"shots\": shots}\n",
        "            except Exception as e:\n",
        "                 print(f\"Qiskit simulation error: {e}\"); return {\"error\": str(e)}\n",
        "        else: return {\"counts\": {}, \"probabilities\": {}, \"shots\": shots, \"notes\": \"No qubits measured.\"}\n",
        "\n",
        "    def get_statevector(self) -> np.ndarray:\n",
        "         if self.noise_model: print(\"Warning: Cannot get exact statevector with noise.\"); return np.array([])\n",
        "         sv_sim = AerSimulator(method='statevector')\n",
        "         sv_circuit = self.circuit.copy(); sv_circuit.save_statevector()\n",
        "         try:\n",
        "              job = sv_sim.run(sv_circuit); result = job.result(); statevector = result.get_statevector(0).data\n",
        "              stabilized_sv = self.math_module.f0z_stabilize(torch.tensor(statevector, dtype=torch.complex64)).numpy()\n",
        "              return stabilized_sv\n",
        "         except Exception as e: print(f\"Qiskit statevector error: {e}\"); return np.array([])\n",
        "\n",
        "# --- QRL Policy Network (Required by Qiskit-based QRL Agent) ---\n",
        "\n",
        "class TemporalPrimeAgent(DFSNAgent):\n",
        "    \"\"\"Agent focused on time-series analysis and forecasting using prime number properties.\"\"\"\n",
        "    def __init__(self, name: str, config: Optional[Dict]=None, primes: Optional[List[int]]=None):\n",
        "        super().__init__(name)\n",
        "        # Default config if none provided\n",
        "        default_config = {\"prime_limit\": 100, \"memory_horizon\": 20, \"tau\": 5.0, \"use_deep_learning\": False, \"hidden_size\": 10, \"sparsity\": 0.7}\n",
        "        self.config = config if config else default_config\n",
        "\n",
        "        self.primes = primes if primes else self._generate_primes(self.config[\"prime_limit\"])\n",
        "        self.time_scale = 1.0\n",
        "        self.forecast_history = []\n",
        "        self.memory_horizon = self.config[\"memory_horizon\"]\n",
        "        self.tau = self.config.get(\"tau\", 5.0)\n",
        "        self.state_history: List[float] = [] # History of internal state z_t\n",
        "        self.use_dl = self.config.get(\"use_deep_learning\", False)\n",
        "        self.lstm = None\n",
        "        self.lstm_optimizer = None\n",
        "        if self.use_dl:\n",
        "            self.lstm = SparseLSTMModel(input_size=1, hidden_size=self.config[\"hidden_size\"], sparsity=self.config.get(\"sparsity\", 0.7))\n",
        "            self.lstm_optimizer = torch.optim.Adam(self.lstm.parameters(), lr=0.01)\n",
        "            print(f\"TemporalPrimeAgent {self.name} initialized with LSTM.\")\n",
        "        else:\n",
        "             print(f\"TemporalPrimeAgent {self.name} initialized (No DL). Primes up to {self.config['prime_limit']}.\")\n",
        "\n",
        "\n",
        "    def _generate_primes(self, n):\n",
        "        \"\"\"Generates prime numbers up to n using Sieve of Eratosthenes.\"\"\"\n",
        "        primes = []\n",
        "        is_prime = [True] * (n + 1)\n",
        "        if n >= 0: is_prime[0] = False\n",
        "        if n >= 1: is_prime[1] = False\n",
        "        for p in range(2, int(np.sqrt(n)) + 1):\n",
        "            if is_prime[p]:\n",
        "                for i in range(p * p, n + 1, p):\n",
        "                    is_prime[i] = False\n",
        "        for p in range(2, n + 1):\n",
        "            if is_prime[p]:\n",
        "                primes.append(p)\n",
        "        return primes if primes else [2, 3, 5] # Fallback primes\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Handles temporal forecasting and scaling tasks.\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"temporal_forecast\":\n",
        "             error = self.check_task_requirements(task, [\"data\", \"horizon\"])\n",
        "             if error: return error\n",
        "             forecast_result = self.forecast(task[\"data\"], task[\"horizon\"])\n",
        "             self.forecast_history.append(forecast_result)\n",
        "             return {\"result\": {\"forecast\": forecast_result}, \"agent\": self.name}\n",
        "        elif task_type == \"temporal_scaling\": # From RealTimeForecastingManager example\n",
        "             error = self.check_task_requirements(task, [\"beta\", \"eta\", \"lambda\", \"delta\", \"gamma\", \"t\", \"x_t\"])\n",
        "             if error: return error\n",
        "             z_t, pred, error_val = self.temporal_scaling(\n",
        "                 task[\"beta\"], task[\"eta\"], task[\"lambda\"], task[\"delta\"], task[\"gamma\"], task[\"t\"], task[\"x_t\"]\n",
        "             )\n",
        "             return {\"result\": {\"z_t\": z_t, \"pred\": pred, \"error\": error_val}, \"agent\": self.name}\n",
        "\n",
        "        # Delegate other tasks to AIW/base\n",
        "        return super()._execute_single_task_iteration(task)\n",
        "\n",
        "    def forecast(self, data: Any, horizon: int) -> float:\n",
        "        \"\"\"Simple forecasting based on recent stabilized average.\"\"\"\n",
        "        # Extract recent numeric values from various data types\n",
        "        if isinstance(data, dict):\n",
        "             # Try to find numeric values in the dict, prioritize keys like 'value', 'result'\n",
        "             values = [v for k, v in data.items() if isinstance(v, (int, float))]\n",
        "             if not values: values = list(data.values()) # Fallback to all values\n",
        "        elif isinstance(data, (list, np.ndarray, torch.Tensor)):\n",
        "             values = np.array(data).flatten()\n",
        "        elif isinstance(data, (int, float)):\n",
        "             values = [data]\n",
        "        else:\n",
        "             values = []\n",
        "\n",
        "        numeric_values = [v for v in values if isinstance(v, (int, float))]\n",
        "\n",
        "        if not numeric_values:\n",
        "            print(f\"{self.name} forecast: No numeric data found.\")\n",
        "            return 0.0\n",
        "\n",
        "        recent_values = numeric_values[-5:] # Use last 5 numeric points\n",
        "        if not recent_values: return 0.0\n",
        "\n",
        "        avg_val = np.mean(recent_values)\n",
        "        stabilized_avg = self.math_module.f0z_stabilize(torch.tensor(avg_val)).item()\n",
        "\n",
        "        # Simple projection: scale average by time scale and horizon\n",
        "        forecast_value = stabilized_avg * self.time_scale * horizon\n",
        "        # Stabilize the final forecast value as well\n",
        "        final_forecast = self.math_module.f0z_stabilize(torch.tensor(forecast_value)).item()\n",
        "        print(f\"{self.name} forecast: Avg={stabilized_avg:.3f}, Horizon={horizon}, Forecast={final_forecast:.3f}\")\n",
        "        return final_forecast\n",
        "\n",
        "    def temporal_scaling(self, beta, eta, lambda_, delta, gamma, t, x_t, noise_threshold=0.1):\n",
        "        \"\"\"Applies the Temporal Prime Scaling update rule.\"\"\"\n",
        "        if not self.primes: # Ensure primes are available\n",
        "             return 0.0, 0.0, x_t # Return default values if no primes\n",
        "\n",
        "        # Get previous internal state z_{t-1}\n",
        "        z_prev = self.state_history[-1] if self.state_history else 0.0\n",
        "\n",
        "        # Calculate memory term based on past states\n",
        "        memory_term = 0.0\n",
        "        flow_factor = 2.5 if self.flow_state == \"flow\" else 1.0 # Influence of flow state\n",
        "        horizon = min(self.memory_horizon, len(self.state_history))\n",
        "        for k in range(1, horizon + 1):\n",
        "            weight = np.exp(-k / self.tau) * (1 + 0.15 * flow_factor) # Weight decays exponentially, boosted by flow\n",
        "            memory_term += weight * self.state_history[-k]\n",
        "\n",
        "        # Handle deep learning component if enabled\n",
        "        h_t = torch.zeros(self.config[\"hidden_size\"]) # Default hidden state\n",
        "        if self.use_dl and self.lstm:\n",
        "             try:\n",
        "                 # Prepare input for LSTM (batch size 1, seq length 1)\n",
        "                 lstm_input = torch.tensor([[x_t]], dtype=torch.float32)\n",
        "                 lstm_output = self.lstm(lstm_input) # Get output from LSTM\n",
        "                 h_t = lstm_output.detach().squeeze().numpy() # Convert to numpy array\n",
        "                 # Simple LSTM training step (online learning)\n",
        "                 # We need a target to train. Let's predict the *next* x_t based on current h_t\n",
        "                 # This requires storing next_x_t or modifying the structure.\n",
        "                 # Simplified: Train based on reconstructing current x_t from h_t (autoencoder style)\n",
        "                 # output_layer = nn.Linear(self.config[\"hidden_size\"], 1) # Assume a simple output layer\n",
        "                 # predicted_x = output_layer(lstm_output)\n",
        "                 # loss = nn.MSELoss()(predicted_x, torch.tensor([[x_t]]))\n",
        "                 # self.lstm_optimizer.zero_grad()\n",
        "                 # loss.backward()\n",
        "                 # self.lstm_optimizer.step()\n",
        "             except Exception as e:\n",
        "                 print(f\"Error during LSTM processing/training: {e}\")\n",
        "                 h_t = np.zeros(self.config[\"hidden_size\"]) # Fallback\n",
        "\n",
        "\n",
        "        # Predict next value based on memory and DL state\n",
        "        # Prediction combines memory term and last element of hidden state\n",
        "        prediction = memory_term + 0.5 * h_t[-1] * self.use_dl\n",
        "        pred_s = self.math_module.f0z_stabilize(torch.tensor(prediction)).item()\n",
        "\n",
        "        # Calculate robust error (median absolute deviation from prediction, clipped by noise threshold)\n",
        "        error = x_t - pred_s\n",
        "        robust_error = np.sign(error) * min(abs(error), noise_threshold + abs(np.median([error, 0.0])))\n",
        "        # Alternative robust error: median of [error, threshold_or_zero]\n",
        "        # robust_error = np.median([error, noise_threshold * np.sign(error)])\n",
        "\n",
        "        # Calculate new internal state z_t using the TPS formula\n",
        "        prime_term = self.primes[t % len(self.primes)] # Cyclical prime influence\n",
        "        dl_term = h_t[-1] * self.use_dl # Contribution from deep learning state\n",
        "\n",
        "        # Combine terms for the update\n",
        "        update_val = (z_prev +\n",
        "                      beta * prime_term +\n",
        "                      eta * memory_term +\n",
        "                      lambda_ * x_t + # Direct influence of current observation\n",
        "                      delta * robust_error + # Influence of prediction error\n",
        "                      gamma * dl_term) # Influence of DL component\n",
        "\n",
        "        # Apply tanh activation and F0Z stabilization\n",
        "        z_t_tensor = torch.tanh(self.math_module.f0z_stabilize(torch.tensor(update_val)))\n",
        "        z_t = z_t_tensor.item()\n",
        "\n",
        "        # Update state history and performance history\n",
        "        self.state_history.append(z_t)\n",
        "        if len(self.state_history) > self.memory_horizon * 2: # Limit history size\n",
        "             self.state_history.pop(0)\n",
        "        # Use absolute error for performance tracking\n",
        "        self.performance_history.append(abs(robust_error))\n",
        "\n",
        "        # print(f\"  TPS (t={t}): x_t={x_t:.3f}, Mem={memory_term:.3f}, Pred={pred_s:.3f}, Err={robust_error:.3f}, z_t={z_t:.3f}\")\n",
        "        return z_t, pred_s, robust_error\n",
        "\n",
        "\n",
        "    def get_domain_specific_state(self) -> Optional[Dict]:\n",
        "        # Share recent internal state and forecast history\n",
        "        return {\"state_history_tail\": self.state_history[-5:],\n",
        "                \"forecast_history_tail\": self.forecast_history[-5:]}\n",
        "\n",
        "    def receive_state(self, state: Dict):\n",
        "        super().receive_state(state)\n",
        "        # Could potentially align internal state or forecasts based on peer\n",
        "        if \"state_history_tail\" in state.get(\"domain_data\", {}):\n",
        "             # Simple merge/ignore for now\n",
        "             pass\n",
        "\n",
        "\n",
        "\n",
        "class QuantumPolicyNetwork(nn.Module):\n",
        "    \"\"\"Classical network to output action probabilities based on state embedding.\"\"\"\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, action_size)\n",
        "        self.softmax = nn.Softmax(dim=-1) # Output probabilities\n",
        "\n",
        "    def forward(self, state_embedding):\n",
        "        x = self.relu(self.fc1(state_embedding))\n",
        "        action_logits = self.fc2(x)\n",
        "        action_probs = self.softmax(action_logits)\n",
        "        return action_probs\n",
        "\n",
        "# --- LLM Integration Agent ---\n",
        "\n",
        "# Remove inheritance from DFSNAgent for F0ZAgent if it's just an interface\n",
        "class F0ZAgent: # NOT inheriting DFSNAgent\n",
        "    \"\"\"Agent integrating a local LLM for inference, simulating F0Z concepts.\"\"\"\n",
        "    def __init__(self, name=\"F0ZAgent_LLM\"): # Give it a name\n",
        "        # Removed super().__init__(name)\n",
        "        self.name = name # Set name directly\n",
        "        self.llm = None\n",
        "        self.math_sim = PyZeroMathTorch()\n",
        "        self.k=1.0; self.Ea=0.1; self.kB=8.617e-5; self.T=300\n",
        "        try:\n",
        "            device = -1 # Default to CPU for the small embedded model\n",
        "            # Use CPU for gpt2 if Ollama agent handles the main LLM tasks\n",
        "            self.llm = pipeline(\"text-generation\", model=\"gpt2\", device=device)\n",
        "            print(f\"F0ZAgent LLM interface initialized with: gpt2 on CPU\")\n",
        "        except Exception as e:\n",
        "            print(f\"LLM loading failed for F0ZAgent: {e}. Inference disabled.\")\n",
        "            self.llm = None\n",
        "\n",
        "\n",
        "    def simulate_f0z_reaction(self, D_Pd_ratio: float, defect_density: float = 0.7, f0z_symmetry: float = 0.95) -> float:\n",
        "        \"\"\"Simulates a hypothetical F0Z-influenced reaction rate based on formula in prompt.\"\"\"\n",
        "        # R_F0Z = k * (D/Pd)^3.2 * exp(-Ea/kBT) * (1 + 0.14*defect) * (1 + 0.10*symmetry)\n",
        "        if D_Pd_ratio < 0: D_Pd_ratio = 0 # Ensure base is non-negative\n",
        "\n",
        "        term1 = self.k * (D_Pd_ratio ** 3.2)\n",
        "        term2 = np.exp(-self.Ea / (self.kB * self.T))\n",
        "        term3 = (1.0 + 0.14 * defect_density)\n",
        "        term4 = (1.0 + 0.10 * f0z_symmetry)\n",
        "\n",
        "        rate = term1 * term2 * term3 * term4\n",
        "\n",
        "        # Apply F0Z stabilization to the final rate\n",
        "        stabilized_rate = self.math_sim.f0z_stabilize(torch.tensor(rate)).item()\n",
        "        print(f\"  Simulated F0Z Reaction Rate: {stabilized_rate:.4e}\")\n",
        "        return stabilized_rate\n",
        "\n",
        "    def infer(self, query: str, max_length: int = 100) -> str:\n",
        "        \"\"\"Uses the integrated LLM to answer queries related to F0Z or other topics.\"\"\"\n",
        "        if not self.llm:\n",
        "            return \"LLM not available in F0ZAgent.\"\n",
        "        try:\n",
        "            # Add context to the query for the LLM\n",
        "            full_prompt = f\"Regarding the Formula for Zero (F0Z) concept, {query}\"\n",
        "            # Ensure max_length is reasonable\n",
        "            max_len = max(20, min(max_length, 200)) # Clamp length\n",
        "            result = self.llm(full_prompt, max_length=max_len, num_return_sequences=1, do_sample=True)\n",
        "            generated_text = result[0]['generated_text']\n",
        "            # Clean up the response (e.g., remove the prompt if model includes it)\n",
        "            if generated_text.startswith(full_prompt):\n",
        "                 cleaned_text = generated_text[len(full_prompt):].strip()\n",
        "            else:\n",
        "                 cleaned_text = generated_text\n",
        "            print(f\"  LLM Inference: Q='{query[:30]}...' -> A='{cleaned_text[:50]}...'\")\n",
        "            return cleaned_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error during LLM inference: {e}\")\n",
        "            return f\"Error during inference: {e}\"\n",
        "\n",
        "    # Make F0ZAgent behave like a DFSNAgent for integration\n",
        "    # Remove execute_task if not needed, or simplify it if called directly\n",
        "    def execute_simulation_task(self, task: Dict) -> Dict:\n",
        "         \"\"\"Handles ONLY f0z_simulation tasks directly.\"\"\"\n",
        "         if task.get(\"type\") == \"f0z_simulation\" or task.get(\"action\") == \"simulate_f0z_reaction\":\n",
        "              # Basic check, assumes 'data' payload contains keys if needed\n",
        "              required = task.get(\"data\", {}).get(\"required_keys\", [\"D_Pd_ratio\"])\n",
        "              missing = [k for k in required if k not in task.get(\"data\", {})]\n",
        "              if missing: return {\"error\": f\"F0Z Sim Missing keys: {missing}\", \"agent\": self.name}\n",
        "              rate = self.simulate_f0z_reaction(\n",
        "                  task[\"data\"][\"D_Pd_ratio\"],\n",
        "                  task.get(\"data\", {}).get(\"defect_density\", 0.7),\n",
        "                  task.get(\"data\", {}).get(\"f0z_symmetry\", 0.95)\n",
        "              )\n",
        "              return {\"result\": {\"reaction_rate\": rate}, \"agent\": self.name}\n",
        "         return {\"error\": f\"Unsupported task type for F0ZAgent simulation: {task.get('type')}\", \"agent\": self.name}\n",
        "\n",
        "    def execute_inference_task(self, task: Dict) -> Dict:\n",
        "         \"\"\"Handles ONLY llm_inference tasks directly.\"\"\"\n",
        "         if task.get(\"type\") == \"llm_inference\" or task.get(\"action\") == \"infer\":\n",
        "              query = task.get(\"data\", {}).get(\"query\", task.get(\"query\")) # Get query from data or root\n",
        "              if not query: return {\"error\": \"Missing query for inference\", \"agent\": self.name}\n",
        "              response = self.infer(query, task.get(\"data\", {}).get(\"max_length\", 100))\n",
        "              return {\"result\": {\"llm_response\": response}, \"agent\": self.name}\n",
        "         return {\"error\": f\"Unsupported task type for F0ZAgent inference: {task.get('type')}\", \"agent\": self.name}\n",
        "\n",
        "    # Keep check_task_requirements if used by execute_* methods above\n",
        "\n",
        "    # Add dummy methods required by DFSNAgent structure if needed for coordination\n",
        "    #def __init__(self, name=\"F0ZAgent_LLM\"): # Give it a name\n",
        "    def __init__(self, name=\"F0ZAgent_LLM\"): # Give it a name\n",
        "        super().__init__() # Call parent init if inheriting (needs DFSNAgent inheritance) - OR just be standalone\n",
        "        self.llm = None # Re-init LLM here\n",
        "        self.math_sim = PyZeroMathTorch()\n",
        "        self.k=1.0; self.Ea=0.1; self.kB=8.617e-5; self.T=300\n",
        "        try:\n",
        "            device = 0 if torch.cuda.is_available() else -1\n",
        "            # Ensure model name is valid or handle error\n",
        "            valid_models = [\"gpt2\", \"distilgpt2\"] # Example small models\n",
        "            model_to_load = \"gpt2\" # Default to gpt2\n",
        "            # If a specific model was intended: model_to_load = \"meta-llama/Llama-3.1\" # Requires access/download\n",
        "            self.llm = pipeline(\"text-generation\", model=model_to_load, device=device)\n",
        "            print(f\"F0ZAgent LLM interface initialized with: {model_to_load}\")\n",
        "        except Exception as e:\n",
        "            print(f\"LLM loading failed for F0ZAgent: {e}. Inference disabled.\")\n",
        "            self.llm = None\n",
        "\n",
        "    def get_engagement_state(self): return 0 # LLM agent might not use DFSN states\n",
        "    def set_engagement_state(self, state): pass\n",
        "    def adjust_workload(self, level): pass\n",
        "    def enter_flow_state(self): pass\n",
        "    def exit_flow_state(self): pass\n",
        "    def adjust_flow_state(self, c, p): pass\n",
        "    def check_task_requirements(self, task, keys):\n",
        "         missing = [k for k in keys if k not in task]\n",
        "         return {\"error\": f\"Missing keys: {missing}\"} if missing else None\n",
        "    def share_state(self, peer): pass # LLM state isn't typically shared this way\n",
        "    def receive_state(self, state): pass\n",
        "    def compute_stability(self): return 0.0\n",
        "    def get_domain_specific_state(self): return None\n",
        "    def process_domain_specific_state(self, data): pass\n",
        "\n",
        "\n",
        "# --- Task-Specific Classes (Examples) ---\n",
        "\n",
        "class ImageClassificationTask:\n",
        "    \"\"\"Represents an image classification task.\"\"\"\n",
        "    def __init__(self, dataset: str):\n",
        "        self.dataset = dataset\n",
        "        print(f\"ImageClassificationTask created for dataset: {self.dataset}\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Simulates running the image classification task.\"\"\"\n",
        "        print(f\"--- Running Image Classification on {self.dataset} ---\")\n",
        "        # Simulate loading data, model prediction, evaluation\n",
        "        time.sleep(0.2)\n",
        "        accuracy = random.uniform(0.85, 0.98)\n",
        "        print(f\"--- Task Complete. Accuracy: {accuracy:.4f} ---\")\n",
        "        return {\"accuracy\": accuracy}\n",
        "\n",
        "class TextSummarizationTask:\n",
        "    \"\"\"Represents a text summarization task.\"\"\"\n",
        "    def __init__(self, dataset: str):\n",
        "        self.dataset = dataset\n",
        "        print(f\"TextSummarizationTask created for dataset: {self.dataset}\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Simulates running the text summarization task.\"\"\"\n",
        "        print(f\"--- Running Text Summarization on {self.dataset} ---\")\n",
        "        # Simulate loading text, model generation, evaluation (e.g., ROUGE score)\n",
        "        time.sleep(0.3)\n",
        "        rouge_score = random.uniform(0.4, 0.6)\n",
        "        print(f\"--- Task Complete. ROUGE Score: {rouge_score:.4f} ---\")\n",
        "        return {\"rouge_score\": rouge_score}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- CuriosityQRLAgent Modification ---\n",
        "class CuriosityQRLAgent(DFSNAgent):\n",
        "    \"\"\"QRL agent using classical policy network and Qiskit backend for env simulation.\"\"\"\n",
        "    def __init__(self, name: str, bridge: ZSGQuantumBridge, action_size: int, state_size: int = 4): # Default state_size = 4\n",
        "        super().__init__(name)\n",
        "        if not bridge or not bridge.simulator: raise ValueError(\"CuriosityQRLAgent requires Qiskit Bridge.\")\n",
        "        self.bridge = bridge\n",
        "        self.policy_network = QuantumPolicyNetwork(state_size, action_size)\n",
        "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=0.005)\n",
        "        self.action_size = action_size; self.state_size = state_size\n",
        "        self.gamma = 0.9; self.epsilon = 0.1\n",
        "        self.state_predictor = nn.Linear(state_size, state_size)\n",
        "        self.predictor_optimizer = optim.Adam(self.state_predictor.parameters(), lr=0.005)\n",
        "        self.memory = []; print(f\"CuriosityQRLAgent {self.name} initialized (Qiskit). Policy State Size: {state_size}, Actions: {action_size}\")\n",
        "\n",
        "    def _get_state_embedding(self, state: Any) -> torch.Tensor:\n",
        "         # Adapt this based on actual state representation!\n",
        "         state_size = self.state_size\n",
        "         if isinstance(state, (list, np.ndarray)):\n",
        "             state_np = np.array(state).flatten()\n",
        "             if len(state_np) >= state_size: return torch.tensor(state_np[:state_size], dtype=torch.float32)\n",
        "             else: padded = np.pad(state_np, (0, state_size - len(state_np))); return torch.tensor(padded, dtype=torch.float32)\n",
        "         else: # Fallback hash\n",
        "             state_str = str(state); state_hash = hashlib.sha256(state_str.encode()).digest()\n",
        "             embedding = [b / 255.0 for b in state_hash[:state_size]]\n",
        "             if len(embedding) < state_size: embedding.extend([0.0] * (state_size - len(embedding)))\n",
        "             return torch.tensor(embedding, dtype=torch.float32)\n",
        "    def select_action(self, state_embedding: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
        "        if random.random() < self.epsilon:\n",
        "            action = random.randrange(self.action_size)\n",
        "            with torch.no_grad(): action_probs = self.policy_network(state_embedding)\n",
        "        else:\n",
        "            action_probs = self.policy_network(state_embedding)\n",
        "            m = torch.distributions.Categorical(probs=action_probs)\n",
        "            action = m.sample().item()\n",
        "        return action, action_probs\n",
        "    def compute_curiosity_reward(self, state: Any, action: int, next_state: Any) -> float:\n",
        "        state_emb = self._get_state_embedding(state); next_state_emb = self._get_state_embedding(next_state)\n",
        "        predicted_next_emb = self.state_predictor(state_emb); loss = nn.MSELoss()(predicted_next_emb, next_state_emb)\n",
        "        self.predictor_optimizer.zero_grad(); loss.backward(); self.predictor_optimizer.step()\n",
        "        curiosity_reward = loss.item(); curiosity_reward = self.math_module.f0z_stabilize(torch.tensor(curiosity_reward)).item()\n",
        "        self.memory.append((state_emb.detach().numpy(), action, next_state_emb.detach().numpy()));\n",
        "        if len(self.memory) > 500: self.memory.pop(0)\n",
        "        return curiosity_reward * 0.1 # Scaled reward\n",
        "    def standard_reward(self, fidelity, noise_level): return fidelity - 0.5 * noise_level\n",
        "    def ffz_reward(self, fidelity, noise_level, action_entropy):\n",
        "        fidelity_t = torch.tensor(fidelity, dtype=torch.float32); noise_t = torch.tensor(noise_level, dtype=torch.float32); action_entropy_t = torch.tensor(action_entropy, dtype=torch.float32)\n",
        "        balance_factor = 1.0 - torch.abs(fidelity_t - noise_t)\n",
        "        reward = fidelity_t - 0.5 * noise_t + 0.1 * action_entropy_t * balance_factor\n",
        "        return reward.item()\n",
        "    def update(self, state: Any, action: int, action_probs: torch.Tensor, extrinsic_reward: float, next_state: Any, fidelity: float, noise_level: float, is_ffz: bool = False):\n",
        "        state_emb = self._get_state_embedding(state) # Need state emb for curiosity\n",
        "        curiosity_reward = self.compute_curiosity_reward(state, action, next_state)\n",
        "        action_entropy = torch.distributions.Categorical(probs=action_probs).entropy().item()\n",
        "        if is_ffz: total_reward = self.ffz_reward(fidelity, noise_level, action_entropy) + curiosity_reward\n",
        "        else: total_reward = self.standard_reward(fidelity, noise_level) + curiosity_reward\n",
        "        m = torch.distributions.Categorical(probs=action_probs); log_prob = m.log_prob(torch.tensor(action))\n",
        "        loss = -log_prob * total_reward # REINFORCE\n",
        "        self.policy_optimizer.zero_grad(); loss.backward(); self.policy_optimizer.step()\n",
        "        self.performance_history.append(total_reward)\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        action = task.get(\"action\")\n",
        "        if action == \"qrl_step_qiskit\":\n",
        "             error = self.check_task_requirements(task, [\"state\", \"qiskit_circuit_func\", \"qiskit_shots\"]) # Changed to circuit_func\n",
        "             if error: return error\n",
        "             current_state = task[\"state\"]; circuit_func = task[\"qiskit_circuit_func\"]; shots = task[\"qiskit_shots\"]\n",
        "             use_ffz_reward = task.get(\"use_ffz_reward\", False)\n",
        "             state_emb = self._get_state_embedding(current_state)\n",
        "             chosen_action, action_probs = self.select_action(state_emb)\n",
        "             # Get base circuit from function and apply action\n",
        "             sim_circuit = circuit_func() # Call the function to get a fresh circuit\n",
        "             if not isinstance(sim_circuit, QuantumCircuit) or sim_circuit.num_qubits != self.bridge.n_qubits: return {\"error\": \"Invalid circuit from qiskit_circuit_func\"}\n",
        "             qubit_to_act = 0 # Example action mapping\n",
        "             if chosen_action == 0: sim_circuit.h(qubit_to_act)\n",
        "             elif chosen_action == 1: sim_circuit.x(qubit_to_act)\n",
        "             elif chosen_action == 2: sim_circuit.cx(qubit_to_act, (qubit_to_act + 1) % self.bridge.n_qubits)\n",
        "             elif chosen_action == 3: pass # Identity\n",
        "             sim_results = self.bridge.run_circuit(sim_circuit, shots=shots)\n",
        "             if \"error\" in sim_results: return sim_results\n",
        "             # Determine reward/next_state (Environment Logic)\n",
        "             counts = sim_results.get(\"counts\", {}); fidelity = (counts.get('00', 0) + counts.get('11', 0)) / shots if shots > 0 else 0.0 # Example for Bell state\n",
        "             noise_level = max(0.0, 1.0 - fidelity); extrinsic_reward = fidelity\n",
        "             next_state = [fidelity, 1.0 - fidelity, noise_level, 1.0 - noise_level] # Example next state\n",
        "             self.update(current_state, chosen_action, action_probs, extrinsic_reward, next_state, fidelity, noise_level, is_ffz=use_ffz_reward)\n",
        "             return {\"result\": {\"action_taken\": chosen_action, \"fidelity\": fidelity, \"noise_level\": noise_level, \"extrinsic_reward\": extrinsic_reward, \"next_state\": next_state, \"qiskit_counts\": counts}, \"agent\": self.name}\n",
        "        return super()._execute_single_task_iteration(task)\n",
        "\n",
        "\n",
        "# --- Replace or Augment F0ZAgent/Create New LLMAgent ---\n",
        "# Option: Create a new LLMAgent inheriting from DFSNAgent\n",
        "class LLMAgent(DFSNAgent):\n",
        "    def __init__(self, name: str, model_name: str = \"llama3\", ollama_base_url: str = \"http://localhost:11434\"):\n",
        "        super().__init__(name)\n",
        "        self.model_name = model_name\n",
        "        self.base_url = ollama_base_url\n",
        "        self.api_endpoint = f\"{self.base_url}/api/generate\"\n",
        "        print(f\"LLMAgent {self.name} initialized to use Ollama model '{self.model_name}' at {self.base_url}\")\n",
        "        # Verify connection (optional)\n",
        "        self._check_connection()\n",
        "\n",
        "    def _check_connection(self):\n",
        "        try:\n",
        "            response = requests.get(self.base_url) # Check if base URL is reachable\n",
        "            response.raise_for_status() # Raise an exception for bad status codes\n",
        "            print(f\"  Ollama connection successful to {self.base_url}\")\n",
        "            # Check if model is available (optional, makes init slower)\n",
        "            # models_res = requests.get(f\"{self.base_url}/api/tags\")\n",
        "            # available_models = [m['name'] for m in models_res.json().get('models', [])]\n",
        "            # if self.model_name not in [m.split(':')[0] for m in available_models]: # Check base name\n",
        "            #     print(f\"  Warning: Model '{self.model_name}' not listed in Ollama tags. Ensure it's pulled.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  Warning: Could not connect to Ollama at {self.base_url}. Error: {e}\")\n",
        "            print(\"  Ensure the Ollama server is running.\")\n",
        "        except Exception as e:\n",
        "             print(f\" Error checking Ollama connection: {e}\")\n",
        "\n",
        "\n",
        "    # Implement the single iteration execution logic\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        action = task.get(\"action\", \"generate\") # Default action\n",
        "        data = task.get(\"data\", {})\n",
        "        prompt = data.get(\"prompt\", task.get(\"query\", \"Explain the ZSG Framework briefly.\")) # Get prompt/query\n",
        "        # Parameters for the Ollama API\n",
        "        ollama_params = data.get(\"ollama_params\", {\n",
        "            \"stream\": False, # Get full response at once\n",
        "            \"options\": { # Model-specific options\n",
        "                \"temperature\": 0.7,\n",
        "                \"num_predict\": 150 # Max tokens to generate (~max_new_tokens)\n",
        "            }\n",
        "        })\n",
        "\n",
        "        if action in [\"generate\", \"infer\", \"explain\", \"generate_text\", \"plan\", \"summarize\"]: # Actions this agent handles\n",
        "            payload = {\n",
        "                \"model\": self.model_name,\n",
        "                \"prompt\": prompt,\n",
        "                **ollama_params # Include stream and options\n",
        "            }\n",
        "\n",
        "            print(f\"  {self.name}: Sending prompt to Ollama model '{self.model_name}': '{prompt[:70]}...'\")\n",
        "\n",
        "            try:\n",
        "                response = requests.post(self.api_endpoint, json=payload, timeout=120) # 120s timeout\n",
        "                response.raise_for_status() # Check for HTTP errors\n",
        "\n",
        "                response_data = response.json()\n",
        "                llm_output = response_data.get(\"response\", \"Error: No response field in Ollama output.\")\n",
        "                # print(f\"  {self.name}: Received response: {llm_output[:100]}...\") # Debug\n",
        "                return {\"result\": {\"llm_response\": llm_output.strip()}, \"agent\": self.name}\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f\"  Error: Ollama request timed out for {self.name}.\")\n",
        "                return {\"error\": \"Ollama request timed out\", \"agent\": self.name}\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"  Error: Ollama request failed for {self.name}: {e}\")\n",
        "                return {\"error\": f\"Ollama request failed: {e}\", \"agent\": self.name}\n",
        "            except Exception as e:\n",
        "                 print(f\"  Error processing Ollama response for {self.name}: {e}\")\n",
        "                 return {\"error\": f\"Failed to process Ollama response: {e}\", \"agent\": self.name}\n",
        "\n",
        "        else:\n",
        "            # If action isn't for LLM, delegate to base DFSNAgent logic\n",
        "            return super()._execute_single_task_iteration(task)\n",
        "\n",
        "    # Optional: Keep specific overrides if needed\n",
        "    def get_engagement_state(self): return 1 # LLM agent might have a base level activity\n",
        "    def share_state(self, peer): pass\n",
        "    def receive_state(self, state): pass\n",
        "\n",
        "\n",
        "# --- ZSGManager Modifications --\n",
        "\n",
        "# --- Core ZSG Systems ---\n",
        "class MemorySystem:\n",
        "    \"\"\"Manages short-term and long-term memory for ZSG agents.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Simple dict-based short-term memory (e.g., last result per task type)\n",
        "        self.short_term_memory: Dict[str, Any] = {}\n",
        "        # List-based long-term memory (e.g., history of significant events or results)\n",
        "        self.long_term_memory: List[Any] = []\n",
        "        # Task-specific storage (from later iterations)\n",
        "        self.task_store: Dict[str, List[Dict]] = {} # Key: episode_iteration string\n",
        "        self.episode_memory: Dict[Tuple[int, int], Any] = {} # Key: (episode, iteration) tuple\n",
        "        self.max_long_term_size = 1000 # Configurable limit\n",
        "        print(\"MemorySystem initialized.\")\n",
        "\n",
        "\n",
        "    def store_memory(self, memory_data: Any, memory_type: str = 'short', key: Optional[str] = None):\n",
        "        \"\"\"Store data in the specified memory type.\"\"\"\n",
        "        if memory_type == 'short':\n",
        "            if key is None: key = f\"generic_{time.time()}\" # Generate a key if none provided\n",
        "            self.short_term_memory[key] = memory_data\n",
        "            # print(f\"Stored in short-term memory with key '{key}'.\")\n",
        "        elif memory_type == 'long':\n",
        "            self.long_term_memory.append(memory_data)\n",
        "            # Enforce size limit\n",
        "            if len(self.long_term_memory) > self.max_long_term_size:\n",
        "                self.long_term_memory.pop(0) # Remove the oldest item\n",
        "            # print(f\"Stored in long-term memory (size: {len(self.long_term_memory)}).\")\n",
        "        else:\n",
        "            print(f\"Warning: Unknown memory type '{memory_type}'. Data not stored.\")\n",
        "\n",
        "    def retrieve_memory(self, memory_type: str = 'short', key: Optional[str] = None, criteria: Optional[callable] = None) -> Any:\n",
        "        \"\"\"\n",
        "        Retrieve data from the specified memory type.\n",
        "        For long-term, criteria can be a function that takes a memory item and returns True if it matches.\n",
        "        \"\"\"\n",
        "        if memory_type == 'short':\n",
        "            if key:\n",
        "                return self.short_term_memory.get(key)\n",
        "            else:\n",
        "                # Return the most recent item or None if empty\n",
        "                return list(self.short_term_memory.values())[-1] if self.short_term_memory else None\n",
        "        elif memory_type == 'long':\n",
        "            if criteria:\n",
        "                # Search long-term memory backwards (most recent first) based on criteria\n",
        "                for item in reversed(self.long_term_memory):\n",
        "                    try:\n",
        "                        if criteria(item):\n",
        "                            return item\n",
        "                    except Exception as e:\n",
        "                         # Catch errors in criteria function evaluation\n",
        "                        print(f\"Error applying criteria function in retrieve_memory: {e}\")\n",
        "                        continue # Skip this item\n",
        "                return None # No item matched the criteria\n",
        "            else:\n",
        "                # Return the most recent long-term memory item or None if empty\n",
        "                return self.long_term_memory[-1] if self.long_term_memory else None\n",
        "        else:\n",
        "            print(f\"Warning: Unknown memory type '{memory_type}'. Cannot retrieve.\")\n",
        "            return None\n",
        "\n",
        "    def store_task(self, episode: int, iteration: int, todo): # Assuming todo has a .to_json() method\n",
        "        \"\"\"Stores a specific task identified by episode and iteration.\"\"\"\n",
        "        key = f\"{episode}_{iteration}\"\n",
        "        if key not in self.task_store:\n",
        "            self.task_store[key] = []\n",
        "        # Ensure todo is serializable or get its JSON representation\n",
        "        try:\n",
        "            todo_data = todo.to_json() if hasattr(todo, 'to_json') else todo\n",
        "            if not isinstance(todo_data, dict):\n",
        "                 raise ValueError(\"Stored task data must be a dictionary or have a to_json method.\")\n",
        "            self.task_store[key].append(todo_data)\n",
        "            # print(f\"Stored task for {key}. Total tasks for key: {len(self.task_store[key])}\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error storing task for {key}: {e}. Data: {todo}\")\n",
        "\n",
        "\n",
        "    def store_episode(self, episode: int, iteration: int, results: Any):\n",
        "        \"\"\"Stores the results associated with a specific episode and iteration.\"\"\"\n",
        "        self.episode_memory[(episode, iteration)] = results\n",
        "        # print(f\"Stored results for episode {episode}, iteration {iteration}.\")\n",
        "\n",
        "    def retrieve_task(self, episode: int, iteration: int) -> Optional[List[Dict]]:\n",
        "         \"\"\"Retrieves tasks for a given episode and iteration.\"\"\"\n",
        "         key = f\"{episode}_{iteration}\"\n",
        "         return self.task_store.get(key)\n",
        "\n",
        "    def retrieve_episode(self, episode: int, iteration: int) -> Optional[Any]:\n",
        "         \"\"\"Retrieves results for a given episode and iteration.\"\"\"\n",
        "         return self.episode_memory.get((episode, iteration))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Batch Processing & Science Fair Mode ---\n",
        "\n",
        "class DynamicBatchModeProcessor:\n",
        "    \"\"\"Handles dynamic batching of tasks for ZSG.\"\"\"\n",
        "    def __init__(self, manager: 'ZSGManager'): # Forward reference ZSGManager\n",
        "        self.manager = manager\n",
        "        self.batch: List[ZSGTodo] = []\n",
        "        print(\"DynamicBatchModeProcessor initialized.\")\n",
        "\n",
        "    def add_to_batch(self, prompt: str, complexity: float, task_type=\"generic_batch_task\"):\n",
        "        \"\"\"Adds a task described by a prompt to the current batch.\"\"\"\n",
        "        # Use hash for a simple task ID\n",
        "        task_id = f\"batch_{hashlib.md5(prompt.encode()).hexdigest()[:8]}\"\n",
        "        # Create ZSGTodo object\n",
        "        todo = ZSGTodo(\n",
        "            task_id=task_id,\n",
        "            description=prompt,\n",
        "            status=\"Pending\",\n",
        "            priority=complexity, # Use complexity as priority\n",
        "            mliw_step=\"BatchProcess\",\n",
        "            data_payload={\"prompt\": prompt, \"task_type\": task_type} # Include original prompt and type\n",
        "        )\n",
        "        self.batch.append(todo)\n",
        "        print(f\"Added task {task_id} to batch. Batch size: {len(self.batch)}\")\n",
        "\n",
        "    def process_batch(self) -> List[Dict]:\n",
        "        \"\"\"Processes all tasks currently in the batch using the ZSGManager.\"\"\"\n",
        "        print(f\"Processing batch of {len(self.batch)} tasks...\")\n",
        "        results = []\n",
        "        if not self.batch:\n",
        "             return results\n",
        "\n",
        "        # Get current MLIW state from manager\n",
        "        episode, iteration = self.manager.mliw.get_state()\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel processing (optional)\n",
        "        # Adjust max_workers based on system resources\n",
        "        # max_workers = min(4, os.cpu_count())\n",
        "        # with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        #      futures = [executor.submit(self._process_single_todo, todo, episode, iteration) for todo in self.batch]\n",
        "        #      results = [future.result() for future in futures]\n",
        "\n",
        "        # Sequential processing:\n",
        "        for todo in self.batch:\n",
        "             result_package = self._process_single_todo(todo, episode, iteration)\n",
        "             results.append(result_package)\n",
        "\n",
        "\n",
        "        # Clear the batch after processing\n",
        "        processed_count = len(self.batch)\n",
        "        self.batch = []\n",
        "        print(f\"Batch processing complete. Processed {processed_count} tasks.\")\n",
        "        return results # List of results from each task\n",
        "\n",
        "\n",
        "    def _process_single_todo(self, todo: ZSGTodo, episode: int, iteration: int) -> Dict:\n",
        "         \"\"\"Helper to process a single TODO item within the batch.\"\"\"\n",
        "         print(f\"  Processing batch task: {todo.task_id} ({todo.description[:30]}...)\")\n",
        "         todo.update_status(\"InProgress\", agent_name=\"BatchProcessor\")\n",
        "\n",
        "         # Construct the task dictionary for the manager\n",
        "         task_dict = {\n",
        "             \"type\": todo.data_payload.get(\"task_type\", \"generic_batch_task\"),\n",
        "             \"action\": todo.mliw_step, # Or derive action from description/type\n",
        "             \"complexity\": todo.priority,\n",
        "             \"data\": todo.data_payload, # Pass entire payload\n",
        "             \"task_id\": todo.task_id # Include ID for tracking\n",
        "         }\n",
        "\n",
        "         # Process task using the manager's main processing function\n",
        "         result = self.manager.process_task_with_zsg(task_dict) # Pass the constructed task\n",
        "\n",
        "         # Update TODO status based on result\n",
        "         if \"error\" in result:\n",
        "             todo.update_status(\"Failed\")\n",
        "             print(f\"    Task {todo.task_id} failed: {result['error']}\")\n",
        "         else:\n",
        "             todo.update_status(\"Completed\")\n",
        "             # Store the result back into the todo's payload (optional)\n",
        "             todo.data_payload[\"result\"] = result.get(\"result\")\n",
        "\n",
        "         # Store the completed/failed TODO using the MemorySystem\n",
        "         self.manager.memory_system.store_task(episode, iteration, todo)\n",
        "\n",
        "         # Return a package containing original task info and the result\n",
        "         return {\"task\": todo.to_json(), \"result_package\": result}\n",
        "\n",
        "\n",
        "\n",
        "class ZSGBatchModeScienceFair:\n",
        "    \"\"\"Orchestrates a 'Science Fair' simulation using batch processing and forecasting.\"\"\"\n",
        "    def __init__(self, manager: 'ZSGManager'):\n",
        "        self.manager = manager\n",
        "        self.processor = DynamicBatchModeProcessor(manager)\n",
        "        # Find required agents within the manager's list\n",
        "        self.temporal_agent = self._find_agent(TemporalPrimeAgent)\n",
        "        self.collab_agent = self._find_agent(CollaborativeAgent)\n",
        "        if not self.temporal_agent or not self.collab_agent:\n",
        "             raise RuntimeError(\"Required agents (TemporalPrimeAgent, CollaborativeAgent) not found in manager.\")\n",
        "        print(\"ZSGBatchModeScienceFair initialized.\")\n",
        "\n",
        "    def _find_agent(self, agent_class: type) -> Optional[DFSNAgent]:\n",
        "         \"\"\"Utility to find the first agent of a specific class.\"\"\"\n",
        "         for agent in self.manager.agents:\n",
        "             if isinstance(agent, agent_class):\n",
        "                 return agent\n",
        "         return None\n",
        "\n",
        "    def run_science_fair(self, experiments: List[str], forecast_horizon: int = 5) -> Dict:\n",
        "        \"\"\"Runs a series of experiments, processes them in batch, forecasts, and combines.\"\"\"\n",
        "        print(f\"\\n--- Running ZSG Science Fair with {len(experiments)} experiments ---\")\n",
        "\n",
        "        # 1. Add experiments to batch processor\n",
        "        print(\"Adding experiments to batch...\")\n",
        "        for i, exp_description in enumerate(experiments):\n",
        "             # Assign complexity based on experiment name length or index (example)\n",
        "             complexity = max(3.0, min(10.0, len(exp_description) / 5.0))\n",
        "             self.processor.add_to_batch(f\"Experiment {i}: {exp_description}\", complexity, task_type=\"science_fair_experiment\")\n",
        "\n",
        "        # 2. Process the batch of experiments\n",
        "        batch_results_packages = self.processor.process_batch()\n",
        "        # Extract just the results relevant for forecasting\n",
        "        batch_results_for_forecast = [pkg['result_package'].get('result', {}).get('experiment_outcome', 0) # Extract specific numeric outcome\n",
        "                                      for pkg in batch_results_packages if 'error' not in pkg['result_package']]\n",
        "\n",
        "        # 3. Forecast based on batch results using TemporalPrimeAgent\n",
        "        print(f\"\\nForecasting future trends (Horizon: {forecast_horizon})...\")\n",
        "        forecast_input_data = [res if isinstance(res, (int, float)) else (res.get('value', 0) if isinstance(res, dict) else 0) for res in batch_results_for_forecast] # Extract numeric values\n",
        "        forecast_task = {\n",
        "            \"type\": \"temporal_forecast\",\n",
        "            \"action\": \"forecast\", # Explicit action\n",
        "            \"data\": forecast_input_data,\n",
        "            \"horizon\": forecast_horizon,\n",
        "            \"complexity\": 6.0 # Assign complexity to the forecast task itself\n",
        "        }\n",
        "        forecast_result_package = self.temporal_agent._execute_single_task_iteration(forecast_task)\n",
        "        forecast_value = forecast_result_package.get(\"result\", {}).get(\"forecast\", \"Forecast Error\")\n",
        "        print(f\"Forecast Result: {forecast_value}\")\n",
        "\n",
        "        # 4. Combine results and forecast using CollaborativeAgent\n",
        "        print(\"\\nCombining batch results and forecast...\")\n",
        "        collab_task = {\n",
        "             \"type\": \"collaboration\",\n",
        "             \"action\": \"combine\", # Use the 'combine' action defined in CollaborativeAgent\n",
        "             \"data\": {\"batch_results\": batch_results_for_forecast, \"forecast\": forecast_value},\n",
        "             \"complexity\": 4.0\n",
        "        }\n",
        "        combined_result_package = self.collab_agent._execute_single_task_iteration(collab_task)\n",
        "        final_combined_data = combined_result_package.get(\"result\", {\"summary\": \"Combination Error\"})\n",
        "        print(\"Combination complete.\")\n",
        "\n",
        "        # 5. Return comprehensive results\n",
        "        print(\"--- ZSG Science Fair Run Complete ---\")\n",
        "        return {\n",
        "            \"batch_run_details\": batch_results_packages, # Full details of each batch task\n",
        "            \"forecast_package\": forecast_result_package,\n",
        "            \"combination_package\": combined_result_package,\n",
        "            \"final_summary\": final_combined_data # The data combined by the collab agent\n",
        "        }\n",
        "\n",
        "class ZSGTodo:\n",
        "    \"\"\"Represents a task item within the ZSG framework.\"\"\"\n",
        "    def __init__(self, task_id: str, description: str, status: str, priority: float, mliw_step: str, data_payload: Dict):\n",
        "        self.task_id = task_id # Unique ID (e.g., \"T001\", \"QuantumTask_abc\")\n",
        "        self.description = description # E.g., \"Optimize PNS sampling\", \"Run Grover Search\"\n",
        "        self.status = status # \"Pending\", \"In Progress\", \"Completed\", \"Failed\"\n",
        "        self.priority = priority # Numerical priority (higher is more important)\n",
        "        self.mliw_step = mliw_step # Which MLIW phase (e.g., \"Analyze\", \"Modulate\", \"Test\", \"Generate\", \"Validate\")\n",
        "        self.data_payload = data_payload # Input data, parameters, or results needed/produced (dict)\n",
        "\n",
        "        # Additional fields for tracking, if needed\n",
        "        self.creation_time = time.time()\n",
        "        self.assigned_agent: Optional[str] = None\n",
        "        self.completion_time: Optional[float] = None\n",
        "\n",
        "    def to_json(self) -> Dict:\n",
        "        \"\"\"Serializes the ZSGTodo object into a JSON-compatible dictionary.\"\"\"\n",
        "        return {\n",
        "            \"task_id\": self.task_id,\n",
        "            \"description\": self.description,\n",
        "            \"status\": self.status,\n",
        "            \"priority\": self.priority,\n",
        "            \"mliw_step\": self.mliw_step,\n",
        "            \"data\": self.data_payload, # Keep payload under 'data' key for consistency\n",
        "            \"creation_time\": self.creation_time,\n",
        "            \"assigned_agent\": self.assigned_agent,\n",
        "            \"completion_time\": self.completion_time\n",
        "        }\n",
        "\n",
        "    def update_status(self, new_status: str, agent_name: Optional[str] = None):\n",
        "        \"\"\"Updates the status of the TODO item.\"\"\"\n",
        "        self.status = new_status\n",
        "        if agent_name:\n",
        "            self.assigned_agent = agent_name\n",
        "        if new_status in [\"Completed\", \"Failed\"]:\n",
        "            self.completion_time = time.time()\n",
        "        print(f\"TODO {self.task_id} status updated to {new_status}\" + (f\" by {agent_name}\" if agent_name else \"\"))\n",
        "\n",
        "\n",
        "\n",
        "class ResourceMonitor:\n",
        "    \"\"\"Monitors system resources and can adjust parameters like batch size.\"\"\"\n",
        "    def __init__(self, batch_size_init=32):\n",
        "        self.batch_size = batch_size_init\n",
        "        self.base_cpu = 10 # Base allocation % per agent\n",
        "        self.base_memory = 10 # Base allocation % per agent\n",
        "        self.total_cpu = psutil.cpu_count() * 100 # Theoretical max %\n",
        "        self.total_memory = 100 # Percentage based\n",
        "        self.processes = {} # Track resource usage per process/agent if needed\n",
        "        print(f\"ResourceMonitor initialized. Initial batch size: {self.batch_size}\")\n",
        "\n",
        "\n",
        "    def check_usage(self) -> Tuple[float, float]:\n",
        "        \"\"\"Checks current overall CPU and virtual memory usage.\"\"\"\n",
        "        cpu_usage = psutil.cpu_percent(interval=0.1) # Non-blocking short interval\n",
        "        mem_usage = psutil.virtual_memory().percent\n",
        "        # print(f\"Resource Check: CPU={cpu_usage}%, Memory={mem_usage}%\")\n",
        "        return cpu_usage, mem_usage\n",
        "\n",
        "    def adjust_batch_size(self) -> int:\n",
        "        \"\"\"Adjusts batch size based on resource usage.\"\"\"\n",
        "        cpu, mem = self.check_usage()\n",
        "        if cpu > 85.0 or mem > 85.0:\n",
        "            print(f\"High resource usage (CPU: {cpu:.1f}%, Mem: {mem:.1f}%). Reducing batch size.\")\n",
        "            self.batch_size = max(8, self.batch_size // 2) # Halve batch size, min 8\n",
        "        elif cpu < 50.0 and mem < 50.0:\n",
        "             # Gradually increase batch size if resources are low\n",
        "             self.batch_size = min(128, self.batch_size + 4) # Increase slowly, max 128\n",
        "        print(f\"Adjusted batch size to: {self.batch_size}\")\n",
        "        return self.batch_size\n",
        "\n",
        "    def pre_allocate(self, agents: List[DFSNAgent], task_complexity: float) -> Dict[str, float]:\n",
        "        \"\"\"Estimate and pre-allocate resources based on agent count and task complexity.\"\"\"\n",
        "        num_agents = len(agents)\n",
        "        if num_agents == 0: return {\"cpu_total\": 0, \"memory_total\": 0}\n",
        "\n",
        "        # Estimate needed resources - simple scaling\n",
        "        cpu_needed_per_agent = min(self.base_cpu + 5 * task_complexity, 100 / num_agents) # Limit by available share\n",
        "        mem_needed_per_agent = min(self.base_memory + 4 * task_complexity, 100 / num_agents)\n",
        "\n",
        "        total_cpu_allocated = 0\n",
        "        total_memory_allocated = 0\n",
        "        for agent in agents:\n",
        "            agent.cpu_allocation = cpu_needed_per_agent\n",
        "            agent.memory_allocation = mem_needed_per_agent\n",
        "            total_cpu_allocated += cpu_needed_per_agent\n",
        "            total_memory_allocated += mem_needed_per_agent\n",
        "\n",
        "        print(f\"Pre-allocated resources: ~{cpu_needed_per_agent:.1f}% CPU, ~{mem_needed_per_agent:.1f}% Mem per agent. Total: {total_cpu_allocated:.1f}% CPU, {total_memory_allocated:.1f}% Mem\")\n",
        "        return {\"cpu_total\": total_cpu_allocated, \"memory_total\": total_memory_allocated}\n",
        "\n",
        "\n",
        "    def update_allocations(self, agents: List[DFSNAgent]):\n",
        "        \"\"\"Dynamically update resource allocations based on agent engagement and performance.\"\"\"\n",
        "        num_agents = len(agents)\n",
        "        if num_agents == 0: return\n",
        "\n",
        "        active_agents = [agent for agent in agents if agent.engagement_state > 0]\n",
        "        num_active = len(active_agents)\n",
        "\n",
        "        if num_active == 0: # No active agents, allocate base resources\n",
        "            for agent in agents:\n",
        "                agent.cpu_allocation = self.base_cpu\n",
        "                agent.memory_allocation = self.base_memory\n",
        "            # print(\"No active agents. Allocating base resources.\")\n",
        "            return\n",
        "\n",
        "        # Calculate available resources beyond base allocation for all agents\n",
        "        available_extra_cpu = self.total_cpu - (self.base_cpu * num_agents)\n",
        "        available_extra_memory = self.total_memory - (self.base_memory * num_agents)\n",
        "\n",
        "        # Distribute extra resources among active agents based on performance (simple weighting)\n",
        "        total_performance_score = sum(agent.performance_history[-1] if agent.performance_history else 0.1 for agent in active_agents) + 1e-8 # Avoid zero division\n",
        "        if total_performance_score <= 0: total_performance_score = 1e-8 # Ensure positive\n",
        "\n",
        "        for agent in agents:\n",
        "            if agent in active_agents:\n",
        "                perf_score = agent.performance_history[-1] if agent.performance_history else 0.1\n",
        "                perf_weight = (perf_score / total_performance_score) if total_performance_score > 0 else (1/num_active)\n",
        "\n",
        "                # Allocate proportional share of extra resources + base\n",
        "                agent.cpu_allocation = self.base_cpu + available_extra_cpu * perf_weight\n",
        "                agent.memory_allocation = self.base_memory + available_extra_memory * perf_weight\n",
        "            else:\n",
        "                # Inactive agents get base allocation\n",
        "                agent.cpu_allocation = self.base_cpu\n",
        "                agent.memory_allocation = self.base_memory\n",
        "\n",
        "        # Clamp allocations to reasonable bounds (e.g., max 80% per agent)\n",
        "        for agent in agents:\n",
        "             agent.cpu_allocation = max(5.0, min(agent.cpu_allocation, 80.0))\n",
        "             agent.memory_allocation = max(5.0, min(agent.memory_allocation, 80.0))\n",
        "             # print(f\"  {agent.name}: CPU={agent.cpu_allocation:.1f}%, Mem={agent.memory_allocation:.1f}%\")\n",
        "\n",
        "        # print(\"Updated resource allocations for active agents based on performance.\")\n",
        "\n",
        "    def start(self):\n",
        "         \"\"\"Start monitoring (if running in a separate thread/process).\"\"\"\n",
        "         print(\"Resource monitoring started (simulated).\")\n",
        "\n",
        "    def stop(self):\n",
        "         \"\"\"Stop monitoring.\"\"\"\n",
        "         print(\"Resource monitoring stopped (simulated).\")\n",
        "\n",
        "class DynamicFlowStateNetwork:\n",
        "    \"\"\"Manages the flow states of a collection of agents based on task complexity.\"\"\"\n",
        "    def __init__(self, agents: List[DFSNAgent], task_complexity_threshold: float = 5.0, max_agents: int = 15):\n",
        "        self.agents = agents # This should be a reference, updated by ZSGManager\n",
        "        self.task_complexity_threshold = task_complexity_threshold\n",
        "        self.agent_states: Dict[str, str] = {agent.name: agent.flow_state for agent in agents}\n",
        "        self.max_agents = max_agents\n",
        "        self.is_dynamic_enabled = False\n",
        "        print(f\"DFSN initialized. Threshold: {self.task_complexity_threshold}, Max Agents: {self.max_agents}\")\n",
        "\n",
        "\n",
        "    def adjust_flow_states(self, current_task_complexity: float, batch_info: Optional[List] = None):\n",
        "        \"\"\"Adjust agent flow states based on complexity and potentially batch info.\"\"\"\n",
        "        if not self.is_dynamic_enabled:\n",
        "            # print(\"DFSN is disabled. No flow state adjustments.\")\n",
        "            return\n",
        "\n",
        "        # Determine complexity measure (use batch avg if available)\n",
        "        complexity_measure = current_task_complexity\n",
        "        if batch_info and len(batch_info) > 0:\n",
        "            # Assuming batch_info is a list of ZSGTodo objects or similar dicts with 'priority'\n",
        "            try:\n",
        "                batch_avg_complexity = sum(item.priority if hasattr(item, 'priority') else item.get('priority', 5.0) for item in batch_info) / len(batch_info)\n",
        "                complexity_measure = (current_task_complexity + batch_avg_complexity) / 2 # Average task and batch complexity\n",
        "                print(f\"DFSN using combined complexity: {complexity_measure:.2f} (Task: {current_task_complexity:.2f}, Batch Avg: {batch_avg_complexity:.2f})\")\n",
        "            except Exception as e:\n",
        "                print(f\"DFSN Warning: Could not calculate batch average complexity: {e}\")\n",
        "\n",
        "\n",
        "        # print(f\"DFSN adjusting flow states based on complexity measure: {complexity_measure:.2f} (Threshold: {self.task_complexity_threshold})\")\n",
        "        num_flow = 0\n",
        "        num_idle = 0\n",
        "        for agent in self.agents:\n",
        "            # Agents decide their own state via adjust_flow_state called within AIW/_execute_single_task_iteration\n",
        "            # DFSN can provide global context or override based on system-wide needs\n",
        "            performance = np.mean(agent.performance_history[-5:]) if agent.performance_history else 0.0\n",
        "            # The agent's internal optimizer will handle the transition logic\n",
        "            agent.adjust_flow_state(complexity_measure, performance)\n",
        "            self.agent_states[agent.name] = agent.flow_state # Update tracked state\n",
        "\n",
        "            if agent.flow_state == 'flow':\n",
        "                num_flow += 1\n",
        "            else:\n",
        "                num_idle += 1\n",
        "\n",
        "        print(f\"DFSN status: {num_flow} agents in flow, {num_idle} agents idle.\")\n",
        "        self.scale_agents(complexity_measure) # Scale agents after adjustments\n",
        "\n",
        "    def enable_dynamic_states(self):\n",
        "        \"\"\"Enable dynamic adjustments.\"\"\"\n",
        "        self.is_dynamic_enabled = True\n",
        "        print(\"DFSN enabled for dynamic state adjustments.\")\n",
        "\n",
        "    def disable_dynamic_states(self):\n",
        "        \"\"\"Disable dynamic adjustments and reset agents to idle.\"\"\"\n",
        "        self.is_dynamic_enabled = False\n",
        "        for agent in self.agents:\n",
        "             if agent.flow_state == 'flow':\n",
        "                 agent.exit_flow_state() # Explicitly exit flow\n",
        "             self.agent_states[agent.name] = agent.flow_state\n",
        "        print(\"DFSN disabled. Agents reset towards idle states.\")\n",
        "\n",
        "    def scale_agents(self, task_complexity: float):\n",
        "        \"\"\"Dynamically scale the number of active agents (placeholder).\"\"\"\n",
        "        # Example scaling logic: more complex tasks require more agents active/instantiated\n",
        "        target_active_agents = max(2, min(self.max_agents, int(task_complexity / 2.0) + 1))\n",
        "        num_current_agents = len(self.agents)\n",
        "        num_currently_active = sum(1 for agent in self.agents if agent.engagement_state > 0)\n",
        "\n",
        "        print(f\"DFSN Scaling Check: Complexity={task_complexity:.2f}, TargetActive={target_active_agents}, CurrentTotal={num_current_agents}, CurrentActive={num_currently_active}\")\n",
        "\n",
        "        # Add new agents if needed and below max limit\n",
        "        if num_current_agents < target_active_agents and num_current_agents < self.max_agents:\n",
        "            num_to_add = min(target_active_agents - num_current_agents, self.max_agents - num_current_agents)\n",
        "            print(f\"  Scaling up: Adding {num_to_add} new DFSNAgent(s).\")\n",
        "            for i in range(num_to_add):\n",
        "                 new_agent_name = f\"DFSNAgent_{num_current_agents + i}\"\n",
        "                 # This needs interaction with the ZSGManager to actually add the agent\n",
        "                 # self.manager.add_agent(DFSNAgent, new_agent_name) # Conceptual\n",
        "                 print(f\"    (Conceptual) Added {new_agent_name}\")\n",
        "            # Note: Need a reference to the manager or a callback to add agents properly.\n",
        "\n",
        "        # Deactivate surplus agents if complexity is low (or handle via engagement states)\n",
        "        elif num_currently_active > target_active_agents and task_complexity < self.task_complexity_threshold * 0.8:\n",
        "             num_to_deactivate = num_currently_active - target_active_agents\n",
        "             print(f\"  Scaling down: Deactivating {num_to_deactivate} agent(s) (setting to low engagement).\")\n",
        "             # Find agents to deactivate (e.g., lowest performance or idle)\n",
        "             agents_to_consider = sorted(self.agents, key=lambda a: (a.engagement_state, np.mean(a.performance_history[-5:]) if a.performance_history else 0))\n",
        "             for i in range(num_to_deactivate):\n",
        "                 if i < len(agents_to_consider):\n",
        "                     agents_to_consider[i].exit_flow_state() # Force lower engagement\n",
        "                     print(f\"    Deactivated {agents_to_consider[i].name}\")\n",
        "\n",
        "    def handle_chaos(self, chaos_metrics: Dict):\n",
        "         \"\"\"Adjust DFSN parameters based on chaos metrics.\"\"\"\n",
        "         # --- CORRECTED KEY ---\n",
        "         lyapunov_exp = chaos_metrics.get(\"lyapunov_estimate\", 0.0)\n",
        "         # --- END CORRECTION ---\n",
        "         print(f\"DFSN received chaos metrics: Lyapunov={lyapunov_exp:.3f}\")\n",
        "         # Rest of the logic using lyapunov_exp remains the same...\n",
        "         if lyapunov_exp > 0.5: # System becoming more chaotic\n",
        "             print(\"  High chaos detected. Increasing stability preference (reducing threshold).\")\n",
        "             self.task_complexity_threshold *= 0.95\n",
        "             for agent in self.agents:\n",
        "                 # Check if optimizer exists before modifying lr\n",
        "                 if hasattr(agent, 'optimizer') and hasattr(agent.optimizer, 'learning_rate'):\n",
        "                     agent.optimizer.learning_rate = min(0.2, agent.optimizer.learning_rate * 1.1)\n",
        "         elif lyapunov_exp < 0.1: # System very stable\n",
        "             print(\"  Low chaos detected. Increasing complexity tolerance (increasing threshold).\")\n",
        "             self.task_complexity_threshold *= 1.05\n",
        "             for agent in self.agents:\n",
        "                  if hasattr(agent, 'optimizer') and hasattr(agent.optimizer, 'learning_rate'):\n",
        "                     agent.optimizer.learning_rate = max(0.05, agent.optimizer.learning_rate * 0.95)\n",
        "\n",
        "\n",
        "class MultiAgentCoordinator:\n",
        "    \"\"\"Coordinates task assignment and synchronization among multiple agents.\"\"\"\n",
        "    def __init__(self, agents: List[Agent]):\n",
        "        self.agents = agents # Reference to the list of agents managed by ZSGManager\n",
        "        self.task_queue: List[Tuple[int, Dict]] = [] # Priority queue (using heapq, neg priority)\n",
        "        # Basic resource pool tracking (can be enhanced by ResourceMonitor)\n",
        "        self.resource_pool = {\"cpu\": 100.0, \"memory\": 100.0}\n",
        "        self.agent_states: Dict[str, Dict] = {agent.name: {\"state\": \"idle\", \"load\": 0.0, \"engagement\": agent.get_engagement_state()} for agent in agents}\n",
        "        self.domain_map = self._build_domain_map()\n",
        "        print(f\"MultiAgentCoordinator initialized with {len(agents)} agents.\")\n",
        "\n",
        "    def _build_domain_map(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Builds a map from task types/domains to capable agent names.\"\"\"\n",
        "        # This should be dynamic based on agent capabilities\n",
        "        domain_map = {\n",
        "            \"physics_simulation\": [\"PhysicsAgent\"], # Can match parts of agent names\n",
        "            \"quantum\": [\"QuantumAgent\"], # Matches QuantumAgent_1, QuantumAgent_QA1 etc.\n",
        "            \"memory_task\": [\"MemoryAgent\"],\n",
        "            \"science_fair_experiment\": [\"CollaborativeAgent\"],\n",
        "            \"collaboration\": [\"CollaborativeAgent\"],\n",
        "            \"temporal_forecast\": [\"TemporalPrimeAgent\"],\n",
        "            \"organic_chemistry\": [\"OrganicChemistryAgent\"],\n",
        "            \"molecular_biology\": [\"MolecularBiologyAgent\"],\n",
        "            \"creative\": [\"CreativeAgent\"],\n",
        "            \"information_theory\": [\"InformationTheoryAgent\"],\n",
        "            \"data_science\": [\"DataScienceAgent\"],\n",
        "            \"astrophysics\": [\"AstrophysicsAgent\"],\n",
        "            \"robotics\": [\"RoboticsAgent\"],\n",
        "            \"environmental_science\": [\"EnvironmentalScienceAgent\"],\n",
        "            \"machine_learning\": [\"MachineLearningAgent\"],\n",
        "            \"validation\": [\"ValidationAgent\"],\n",
        "            \"chaos\": [\"PhysicsAgent\", \"FractalAgent\"], # Example: Multiple agents can handle\n",
        "            \"fractal_generate\": [\"FractalAgent\"],\n",
        "            \"hnn_update\": [\"HopfieldAgent\"],\n",
        "            \"temporal_scaling\": [\"TemporalPrimeAgent\"],\n",
        "            \"llada_task\": [\"LLaDATaskAgent\"],\n",
        "            \"quantum_poetry\": [\"QuantumAIMLLLM\"], # Handled by manager directly? Or a dedicated agent?\n",
        "            \"quantum_game\": [\"QuantumAgent\"],\n",
        "            \"quantum_field\": [\"QuantumAgent\"],\n",
        "            \"grover_search\": [\"QuantumAgent\"],\n",
        "            \"shor_factor\": [\"QuantumAgent\"],\n",
        "            \"quantum_circuit\": [\"QuantumAgent\"],\n",
        "            # Add more mappings as new agents/tasks are defined\n",
        "        }\n",
        "        print(\"Coordinator domain map built.\")\n",
        "        return domain_map\n",
        "\n",
        "    def find_capable_agents(self, task_type: str) -> List[Agent]:\n",
        "         \"\"\"Finds agents whose names or declared capabilities match the task type.\"\"\"\n",
        "         capable_agents = []\n",
        "         agent_dict = {agent.name: agent for agent in self.agents}\n",
        "\n",
        "         for domain_key, agent_name_patterns in self.domain_map.items():\n",
        "             if domain_key in task_type: # Simple substring matching for type\n",
        "                 for pattern in agent_name_patterns:\n",
        "                     for agent_name, agent in agent_dict.items():\n",
        "                         if pattern in agent_name and agent not in capable_agents:\n",
        "                             capable_agents.append(agent)\n",
        "\n",
        "         # Fallback if no specific agent found\n",
        "         if not capable_agents:\n",
        "             print(f\"No specific agent found for task type '{task_type}', assigning to general DFSNAgent.\")\n",
        "             # Find any generic DFSNAgent available\n",
        "             general_agents = [agent for agent in self.agents if isinstance(agent, DFSNAgent) and not any(pattern in agent.name for patterns in self.domain_map.values() for pattern in patterns)]\n",
        "             if general_agents:\n",
        "                 capable_agents.append(random.choice(general_agents)) # Assign to a random general agent\n",
        "             elif self.agents:\n",
        "                  capable_agents.append(random.choice(self.agents)) # Assign to any agent if no general one exists\n",
        "\n",
        "         # print(f\"Found {len(capable_agents)} capable agents for task '{task_type}': {[a.name for a in capable_agents]}\")\n",
        "         return capable_agents\n",
        "\n",
        "\n",
        "    def add_task(self, task: dict, priority: int):\n",
        "        \"\"\"Adds a task to the priority queue.\"\"\"\n",
        "        if 'type' not in task:\n",
        "            print(\"Warning: Task added without 'type'. Assigning low priority.\")\n",
        "            task['type'] = 'unknown'\n",
        "            priority = -10 # Use negative for min-heap\n",
        "\n",
        "        print(f\"Coordinator adding task: {task['type']} with priority {priority}\")\n",
        "        heappush(self.task_queue, (-priority, task)) # Use negative priority for max-heap behavior\n",
        "\n",
        "    def assign_tasks(self, task: Dict) -> Dict:\n",
        "         \"\"\"Assigns a single task to the most suitable agent(s).\"\"\"\n",
        "         task_type = task.get(\"type\", \"unknown\")\n",
        "         capable_agents = self.find_capable_agents(task_type)\n",
        "\n",
        "         if not capable_agents:\n",
        "             print(f\"Error: No capable agent found for task type '{task_type}'.\")\n",
        "             return {\"error\": \"No suitable agent found\", \"task_type\": task_type}\n",
        "\n",
        "         # Simple assignment: Assign to the least loaded capable agent\n",
        "         # More complex: Consider engagement state, specialization score, etc.\n",
        "         best_agent = min(capable_agents, key=lambda a: self.agent_states.get(a.name, {\"load\": 0.0})[\"load\"])\n",
        "\n",
        "         print(f\"Assigning task '{task_type}' to agent: {best_agent.name} (Load: {self.agent_states.get(best_agent.name, {}).get('load', 0):.2f})\")\n",
        "         self.agent_states[best_agent.name][\"load\"] += task.get(\"complexity\", 5.0) # Increment load estimate\n",
        "\n",
        "         # Execute the task via the agent's _execute_single_task_iteration method\n",
        "         result = best_agent._execute_single_task_iteration(task)\n",
        "\n",
        "         # Decrement load after execution (simplified)\n",
        "         self.agent_states[best_agent.name][\"load\"] -= task.get(\"complexity\", 5.0)\n",
        "         self.agent_states[best_agent.name][\"load\"] = max(0, self.agent_states[best_agent.name][\"load\"]) # Ensure load doesn't go negative\n",
        "\n",
        "         # Synchronize states after task completion\n",
        "         self.synchronize_flow_states({\"success\": \"error\" not in result})\n",
        "\n",
        "         return result\n",
        "\n",
        "    def execute_next_task(self) -> Optional[Dict]:\n",
        "        \"\"\"Retrieves and executes the highest priority task from the queue.\"\"\"\n",
        "        if not self.task_queue:\n",
        "            # print(\"Coordinator task queue is empty.\")\n",
        "            return None\n",
        "\n",
        "        neg_priority, task = heappop(self.task_queue)\n",
        "        priority = -neg_priority\n",
        "        print(f\"Coordinator executing next task (Priority: {priority}): {task.get('type', 'Unknown')}\")\n",
        "\n",
        "        return self.assign_tasks(task) # Use the assign_tasks logic\n",
        "\n",
        "\n",
        "    def synchronize_flow_states(self, task_feedback: Optional[Dict] = None):\n",
        "        \"\"\"Synchronizes agent states, potentially based on feedback or global state.\"\"\"\n",
        "        # print(\"Coordinator synchronizing flow states...\")\n",
        "        if not self.agents: return\n",
        "\n",
        "        engagement_levels = [agent.get_engagement_state() for agent in self.agents]\n",
        "        avg_engagement = np.mean(engagement_levels) if engagement_levels else 0\n",
        "        # print(f\"  Average engagement: {avg_engagement:.2f}\")\n",
        "\n",
        "        # Update internal tracking\n",
        "        for agent in self.agents:\n",
        "            self.agent_states[agent.name][\"engagement\"] = agent.get_engagement_state()\n",
        "            # Optional: Adjust load based on engagement?\n",
        "\n",
        "        # Basic synchronization: Agents adjust workload based on average (or peers)\n",
        "        for agent in self.agents:\n",
        "            # Pass average engagement, agent can decide how to react\n",
        "             agent.adjust_workload(avg_engagement)\n",
        "\n",
        "             # Optionally trigger DFSN adjustments based on global state\n",
        "             # if hasattr(self.manager, 'flow_state_network'): # Requires link to manager\n",
        "             #     self.manager.flow_state_network.adjust_flow_states(global_complexity_measure)\n",
        "             pass\n",
        "\n",
        "        # print(\"Flow state synchronization complete.\")\n",
        "\n",
        "class EntropyBotTurbo:\n",
        "    \"\"\"Monitors entropy of quantum states.\"\"\"\n",
        "    def __init__(self, epsilon=1e-10):\n",
        "        self.epsilon = epsilon # Small value to avoid log(0)\n",
        "        self.math_module = PyZeroMathTorch()\n",
        "\n",
        "    def compute_entropy(self, probs: np.ndarray) -> float:\n",
        "        \"\"\"Computes Shannon entropy for a probability distribution.\"\"\"\n",
        "        # Ensure probs are valid probabilities (non-negative, sum to 1)\n",
        "        probs = np.maximum(probs, 0) # Ensure non-negative\n",
        "        prob_sum = np.sum(probs)\n",
        "        if prob_sum <= 0: return 0.0 # Entropy is 0 if all probs are 0\n",
        "        probs = probs / prob_sum # Normalize\n",
        "\n",
        "        # Use F0Z stabilized log\n",
        "        probs_t = torch.tensor(probs, dtype=torch.float32)\n",
        "        log_probs = torch.log2(self.math_module.f0z_stabilize(probs_t) + self.epsilon)\n",
        "        entropy = -torch.sum(probs_t * log_probs)\n",
        "        return entropy.item()\n",
        "\n",
        "    def monitor_state(self, state_vector: np.ndarray) -> Tuple[float, np.ndarray]:\n",
        "        \"\"\"Calculates probabilities and entropy from a quantum state vector.\"\"\"\n",
        "        if not isinstance(state_vector, np.ndarray) or state_vector.ndim != 1:\n",
        "             print(\"Warning (EntropyBot): Invalid state vector input.\")\n",
        "             return 0.0, np.array([1.0]) # Return default entropy and probability\n",
        "\n",
        "        probabilities = np.abs(state_vector)**2\n",
        "        entropy = self.compute_entropy(probabilities)\n",
        "        # print(f\"EntropyBot Turbo: State entropy = {entropy:.4f}\")\n",
        "        return entropy, probabilities\n",
        "\n",
        "\n",
        "# --- LLaDA Agent (from Iteration 23) ---\n",
        "# Needs LLaDA model definition - using placeholder\n",
        "class LLaDA(nn.Module): # Placeholder\n",
        "     def __init__(self, vocab_size, seq_len):\n",
        "         super().__init__()\n",
        "         self.vocab_size = vocab_size\n",
        "         self.seq_len = seq_len\n",
        "         # Dummy layers\n",
        "         self.embedding = nn.Linear(vocab_size, 64)\n",
        "         self.output = nn.Linear(64, vocab_size)\n",
        "         print(\"LLaDA Model Placeholder Initialized.\")\n",
        "\n",
        "     def forward_process(self, x0, t): # Simulate diffusion process\n",
        "         noise = torch.randn_like(x0.float()) * t\n",
        "         xt = x0.float() + noise # Simplified noise addition\n",
        "         mask = (torch.rand_like(x0.float()) < t * 0.5) # Random masking based on time t\n",
        "         return xt, mask\n",
        "\n",
        "     def loss(self, x0, xt, mask, t): # Simulate loss calculation\n",
        "         # Dummy loss based on difference and mask\n",
        "         diff = torch.abs(x0.float() - xt) * mask.float() # Calculate difference where masked\n",
        "         return torch.mean(diff) * 10 # Scale loss for visibility\n",
        "\n",
        "\n",
        "class LLaDATaskAgent(DFSNAgent): # Inherit from DFSNAgent\n",
        "    \"\"\"Agent designed to execute tasks related to the LLaDA model.\"\"\"\n",
        "    def __init__(self, name: str, bridge: Optional[ZSGQuantumBridge] = None, llada_model: Optional[LLaDA] = None):\n",
        "        super().__init__(name)\n",
        "        self.model = llada_model if llada_model else LLaDA(100, 10) # Default placeholder model\n",
        "        self.bridge = bridge # Store bridge if provided\n",
        "        # Agent state to store current task info and results\n",
        "        self.state = {\"current_todo\": None, \"task_completed\": False, \"quantum_state_vector\": None}\n",
        "        print(f\"LLaDATaskAgent {self.name} initialized.\")\n",
        "\n",
        "    def _execute_single_task_iteration(self, task: Dict) -> Dict:\n",
        "        \"\"\"Executes LLaDA-specific tasks (diffusion process simulation).\"\"\"\n",
        "        task_type = task.get(\"type\")\n",
        "        action = task.get(\"action\")\n",
        "\n",
        "        if task_type == \"llada_diffusion\":\n",
        "             error = self.check_task_requirements(task, [\"sequence\", \"t\"])\n",
        "             if error: return error\n",
        "             # Store task in agent's state\n",
        "             self.state[\"current_todo\"] = task\n",
        "             self.state[\"task_completed\"] = False\n",
        "\n",
        "             try:\n",
        "                 # Prepare input tensor (ensure it's long type for embedding usually)\n",
        "                 # Using float here based on LLaDA placeholder model's forward_process\n",
        "                 seq_tensor = torch.tensor(task[\"sequence\"], dtype=torch.float32)\n",
        "                 time_t = task[\"t\"]\n",
        "\n",
        "                 # Simulate the forward (diffusion) process and loss calculation\n",
        "                 xt, mask = self.model.forward_process(seq_tensor, time_t)\n",
        "                 loss = self.model.loss(seq_tensor, xt, mask, time_t)\n",
        "\n",
        "                 # Determine completion status (example: low loss)\n",
        "                 completed = loss.item() < 5.0\n",
        "                 self.state[\"task_completed\"] = completed\n",
        "\n",
        "                 # Generate or update quantum state representation if bridge exists\n",
        "                 if self.bridge:\n",
        "                      encoded_state = self.bridge.encode({\"task_id\": task.get(\"task_id\"), \"loss\": loss.item()}, entangle=completed)\n",
        "                      self.state[\"quantum_state_vector\"] = encoded_state\n",
        "\n",
        "                 print(f\"  LLaDA Task: Sequence length={len(task['sequence'])}, t={time_t:.2f} -> Loss={loss.item():.4f}, Completed={completed}\")\n",
        "                 return {\"result\": {\"loss\": loss.item()}, \"status\": \"Completed\" if completed else \"InProgress\", \"agent\": self.name}\n",
        "\n",
        "             except Exception as e:\n",
        "                  print(f\"Error during LLaDA task execution: {e}\")\n",
        "                  self.state[\"task_completed\"] = False\n",
        "                  return {\"error\": f\"LLaDA execution failed: {e}\", \"status\": \"Failed\", \"agent\": self.name}\n",
        "\n",
        "        return super()._execute_single_task_iteration(task) # Delegate other tasks\n",
        "\n",
        "\n",
        "    # Add convergence methods from LLaDA example\n",
        "    def converge(self, peer: Agent, bridge: Optional[ZSGQuantumBridge] = None):\n",
        "        \"\"\"Simulates convergence towards a peer's state, potentially via entanglement.\"\"\"\n",
        "        if not bridge or not bridge.simulator:\n",
        "            print(f\"{self.name}: Cannot converge without quantum bridge.\")\n",
        "            return\n",
        "\n",
        "        print(f\"{self.name} attempting quantum convergence with {peer.name}...\")\n",
        "        if isinstance(peer, LLaDATaskAgent) and peer.bridge and peer.state.get(\"quantum_state_vector\"):\n",
        "             # Get own and peer's quantum state\n",
        "             my_state_vec = self.state.get(\"quantum_state_vector\")\n",
        "             peer_state_vec = peer.state.get(\"quantum_state_vector\")\n",
        "\n",
        "             if my_state_vec and peer_state_vec and len(my_state_vec) == len(peer_state_vec):\n",
        "                  # Simulate entanglement and projection (simplified: average states)\n",
        "                  my_q_state = np.array(my_state_vec, dtype=complex)\n",
        "                  peer_q_state = np.array(peer_state_vec, dtype=complex)\n",
        "                  converged_state = 0.5 * (my_q_state + peer_q_state)\n",
        "                  norm = LA.norm(converged_state)\n",
        "                  if norm > 1e-9: converged_state /= norm\n",
        "\n",
        "                  # Update own quantum state\n",
        "                  self.state[\"quantum_state_vector\"] = converged_state.tolist()\n",
        "                  # Optionally update bridge simulator state if this agent 'owns' it\n",
        "                  # bridge.simulator.state = converged_state\n",
        "                  print(f\"  {self.name} converged quantum state with {peer.name}.\")\n",
        "             else:\n",
        "                  print(f\"  Convergence failed: Invalid or incompatible quantum states.\")\n",
        "        else:\n",
        "             print(f\"  Convergence failed: Peer {peer.name} is not suitable or has no quantum state.\")\n",
        "\n",
        "\n",
        "    def deconverge(self, peer: Agent):\n",
        "         \"\"\"Simulates decoherence or moving away from a peer's state.\"\"\"\n",
        "         print(f\"{self.name} deconverging from {peer.name} (simulated by randomizing state slightly).\")\n",
        "         if self.state.get(\"quantum_state_vector\"):\n",
        "              # Add small random noise to quantum state (simulate decoherence)\n",
        "              q_state = np.array(self.state[\"quantum_state_vector\"], dtype=complex)\n",
        "              noise = (np.random.rand(*q_state.shape) + 1j * np.random.rand(*q_state.shape)) * 0.01 # Small complex noise\n",
        "              noisy_state = q_state + noise\n",
        "              norm = LA.norm(noisy_state)\n",
        "              if norm > 1e-9: noisy_state /= norm\n",
        "              self.state[\"quantum_state_vector\"] = noisy_state.tolist()\n",
        "\n",
        "\n",
        "class ZSGQueue:\n",
        "     \"\"\"Interface for interacting with a Redis queue for ZSG tasks.\"\"\"\n",
        "     def __init__(self, manager: Optional['ZSGManager'] = None, redis_host='localhost', redis_port=6379, redis_db=0):\n",
        "         self.manager = manager # Optional link to manager for getting TODOs\n",
        "         self.queue_name = \"zsg_tasks\"\n",
        "         try:\n",
        "             self.r = redis.Redis(host=redis_host, port=redis_port, db=redis_db, decode_responses=True)\n",
        "             self.r.ping() # Check connection\n",
        "             print(f\"ZSGQueue connected to Redis at {redis_host}:{redis_port}\")\n",
        "         except Exception as e:\n",
        "             print(f\"Error connecting to Redis for ZSGQueue: {e}. Queue functionality disabled.\")\n",
        "             self.r = None\n",
        "\n",
        "     def publish_todo(self, todo: ZSGTodo):\n",
        "         \"\"\"Publishes a single ZSGTodo item to the Redis queue.\"\"\"\n",
        "         if not self.r: return False\n",
        "         try:\n",
        "             self.r.lpush(self.queue_name, json.dumps(todo.to_json()))\n",
        "             print(f\"Published TODO {todo.task_id} to Redis queue '{self.queue_name}'.\")\n",
        "             return True\n",
        "         except Exception as e:\n",
        "             print(f\"Error publishing TODO {todo.task_id} to Redis: {e}\")\n",
        "             return False\n",
        "\n",
        "     def publish_todos_from_manager(self):\n",
        "         \"\"\"Publishes all TODOs currently held by a manager's component (e.g., processor batch).\"\"\"\n",
        "         if not self.manager or not hasattr(self.manager, 'processor') or not hasattr(self.manager.processor, 'batch'):\n",
        "              print(\"Error: Cannot publish TODOs, manager or batch processor not available.\")\n",
        "              return\n",
        "         if not self.r: return\n",
        "\n",
        "         todos_to_publish = self.manager.processor.batch # Publish tasks currently in the batch\n",
        "         print(f\"Publishing {len(todos_to_publish)} TODOs from manager batch to Redis...\")\n",
        "         count = 0\n",
        "         for todo in todos_to_publish:\n",
        "             if self.publish_todo(todo):\n",
        "                 count += 1\n",
        "         print(f\"Successfully published {count} TODOs.\")\n",
        "\n",
        "     def subscribe_and_process(self, endpoint_name: str, processing_callback: callable):\n",
        "         \"\"\"Subscribes to the queue and processes tasks using the provided callback.\"\"\"\n",
        "         if not self.r:\n",
        "             print(f\"{endpoint_name}: Cannot subscribe, Redis connection failed.\")\n",
        "             return\n",
        "\n",
        "         print(f\"{endpoint_name}: Subscribing to Redis queue '{self.queue_name}'...\")\n",
        "         while True:\n",
        "             try:\n",
        "                 # Blocking right pop (list treated as queue) with timeout\n",
        "                 task_json = self.r.brpop(self.queue_name, timeout=5)\n",
        "                 if task_json:\n",
        "                     # task_json is a tuple (queue_name, task_data_string)\n",
        "                     task_data_str = task_json[1]\n",
        "                     print(f\"\\n{endpoint_name} received task: {task_data_str[:100]}...\")\n",
        "                     try:\n",
        "                         task_dict = json.loads(task_data_str)\n",
        "                         # Call the processing function provided by the consumer\n",
        "                         processing_callback(task_dict)\n",
        "                     except json.JSONDecodeError:\n",
        "                         print(f\"  {endpoint_name} Error: Could not decode JSON task data.\")\n",
        "                     except Exception as e:\n",
        "                          print(f\"  {endpoint_name} Error processing task: {e}\")\n",
        "                 # else: print(f\"{endpoint_name}: No task received (timeout).\") # Optional: logging for timeout\n",
        "             except redis.exceptions.ConnectionError:\n",
        "                  print(f\"{endpoint_name}: Redis connection error. Retrying...\")\n",
        "                  time.sleep(5)\n",
        "             except KeyboardInterrupt:\n",
        "                  print(f\"{endpoint_name}: Subscription interrupted.\")\n",
        "                  break\n",
        "             except Exception as e:\n",
        "                  print(f\"{endpoint_name}: Unexpected error in subscription loop: {e}\")\n",
        "                  time.sleep(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MLIW: # Machine Learning Iterative Workflow\n",
        "    \"\"\"Manages the episode and iteration state for the MLIW.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.current_episode: int = 0\n",
        "        self.current_iteration: int = 0\n",
        "        print(\"MLIW Controller initialized.\")\n",
        "\n",
        "    def start_episode(self, episode: Optional[int] = None, iteration: Optional[int] = None):\n",
        "        \"\"\"Starts or advances the episode/iteration counter.\"\"\"\n",
        "        if episode is not None:\n",
        "            self.current_episode = episode\n",
        "        else:\n",
        "            self.current_episode += 1\n",
        "\n",
        "        if iteration is not None:\n",
        "            self.current_iteration = iteration\n",
        "        else:\n",
        "            self.current_iteration = 1 # Reset iteration for new episode\n",
        "\n",
        "        print(f\"MLIW starting Episode {self.current_episode}, Iteration {self.current_iteration}\")\n",
        "\n",
        "    def next_iteration(self):\n",
        "        \"\"\"Advances to the next iteration within the current episode.\"\"\"\n",
        "        self.current_iteration += 1\n",
        "        print(f\"MLIW advanced to Episode {self.current_episode}, Iteration {self.current_iteration}\")\n",
        "\n",
        "    def get_state(self) -> Tuple[int, int]:\n",
        "        \"\"\"Returns the current episode and iteration.\"\"\"\n",
        "        return self.current_episode, self.current_iteration\n",
        "\n",
        "\n",
        "class ZSGTodo:\n",
        "    \"\"\"Represents a task item within the ZSG framework.\"\"\"\n",
        "    def __init__(self, task_id: str, description: str, status: str, priority: float, mliw_step: str, data_payload: Dict):\n",
        "        self.task_id = task_id # Unique ID (e.g., \"T001\", \"QuantumTask_abc\")\n",
        "        self.description = description # E.g., \"Optimize PNS sampling\", \"Run Grover Search\"\n",
        "        self.status = status # \"Pending\", \"In Progress\", \"Completed\", \"Failed\"\n",
        "        self.priority = priority # Numerical priority (higher is more important)\n",
        "        self.mliw_step = mliw_step # Which MLIW phase (e.g., \"Analyze\", \"Modulate\", \"Test\", \"Generate\", \"Validate\")\n",
        "        self.data_payload = data_payload # Input data, parameters, or results needed/produced (dict)\n",
        "\n",
        "        # Additional fields for tracking, if needed\n",
        "        self.creation_time = time.time()\n",
        "        self.assigned_agent: Optional[str] = None\n",
        "        self.completion_time: Optional[float] = None\n",
        "\n",
        "    def to_json(self) -> Dict:\n",
        "        \"\"\"Serializes the ZSGTodo object into a JSON-compatible dictionary.\"\"\"\n",
        "        return {\n",
        "            \"task_id\": self.task_id,\n",
        "            \"description\": self.description,\n",
        "            \"status\": self.status,\n",
        "            \"priority\": self.priority,\n",
        "            \"mliw_step\": self.mliw_step,\n",
        "            \"data\": self.data_payload, # Keep payload under 'data' key for consistency\n",
        "            \"creation_time\": self.creation_time,\n",
        "            \"assigned_agent\": self.assigned_agent,\n",
        "            \"completion_time\": self.completion_time\n",
        "        }\n",
        "\n",
        "    def update_status(self, new_status: str, agent_name: Optional[str] = None):\n",
        "        \"\"\"Updates the status of the TODO item.\"\"\"\n",
        "        self.status = new_status\n",
        "        if agent_name:\n",
        "            self.assigned_agent = agent_name\n",
        "        if new_status in [\"Completed\", \"Failed\"]:\n",
        "            self.completion_time = time.time()\n",
        "        print(f\"TODO {self.task_id} status updated to {new_status}\" + (f\" by {agent_name}\" if agent_name else \"\"))\n",
        "\n",
        "\n",
        "class ChaosTheoryModule:\n",
        "    \"\"\"Module for simulating chaotic systems like Lorenz attractor.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.f0z_math = PyZeroMathTorch()\n",
        "        print(\"ChaosTheoryModule initialized.\")\n",
        "\n",
        "    def _lorenz_ode(self, state, t, sigma, rho, beta):\n",
        "        \"\"\"Defines the Lorenz system ODEs.\"\"\"\n",
        "        x, y, z = state\n",
        "        dx_dt = sigma * (y - x)\n",
        "        dy_dt = x * (rho - z) - y\n",
        "        dz_dt = x * y - beta * z\n",
        "        # Apply F0Z stabilization within the ODE calculation (optional, can affect dynamics)\n",
        "        # dx_dt = self.f0z_math.f0z_stabilize(torch.tensor(dx_dt)).item()\n",
        "        # dy_dt = self.f0z_math.f0z_stabilize(torch.tensor(dy_dt)).item()\n",
        "        # dz_dt = self.f0z_math.f0z_stabilize(torch.tensor(dz_dt)).item()\n",
        "        return [dx_dt, dy_dt, dz_dt]\n",
        "\n",
        "    def simulate(self, system: str, params: Dict) -> Dict:\n",
        "        \"\"\"Simulates a chaotic system.\"\"\"\n",
        "        print(f\"Simulating chaotic system: {system}\")\n",
        "        if system.lower() == \"lorenz\":\n",
        "            # Default Lorenz parameters\n",
        "            sigma = params.get(\"sigma\", 10.0)\n",
        "            rho = params.get(\"rho\", 28.0)\n",
        "            beta = params.get(\"beta\", 8.0/3.0)\n",
        "            x0 = params.get(\"x0\", 0.01)\n",
        "            y0 = params.get(\"y0\", 0.01)\n",
        "            z0 = params.get(\"z0\", 0.01)\n",
        "            t_max = params.get(\"t_max\", 10.0)\n",
        "            steps = params.get(\"steps\", 200) # Fewer steps for faster simulation\n",
        "\n",
        "            t_span = np.linspace(0, t_max, steps)\n",
        "            initial_state = [x0, y0, z0]\n",
        "\n",
        "            try:\n",
        "                 # Integrate the ODEs\n",
        "                 result = odeint(self._lorenz_ode, initial_state, t_span, args=(sigma, rho, beta))\n",
        "\n",
        "                 # Apply F0Z stabilization to the final trajectory\n",
        "                 stabilized_result = self.f0z_math.f0z_stabilize(torch.tensor(result, dtype=torch.float32), system_size=result.size).numpy()\n",
        "\n",
        "                 # Basic chaos metric: Estimate Lyapunov exponent (very simplified)\n",
        "                 # Calculate divergence of nearby trajectories (needs another simulation run)\n",
        "                 pert_initial = [x0 + 1e-5, y0, z0]\n",
        "                 pert_result = odeint(self._lorenz_ode, pert_initial, t_span, args=(sigma, rho, beta))\n",
        "                 log_divergence = np.log(np.maximum(1e-9, np.linalg.norm(result - pert_result, axis=1)))\n",
        "                 # Simple linear fit to estimate LE (not robust)\n",
        "                 lyapunov_estimate = np.polyfit(t_span[1:], log_divergence[1:] / np.maximum(1e-9, t_span[1:]), 1)[0] if len(t_span) > 1 else 0.0\n",
        "\n",
        "                 print(f\"  Lorenz simulation complete. Estimated LE: {lyapunov_estimate:.3f}\")\n",
        "                 return {\n",
        "                     \"trajectory\": stabilized_result.tolist(),\n",
        "                     \"chaos_metrics\": {\"lyapunov_estimate\": lyapunov_estimate}\n",
        "                 }\n",
        "            except Exception as e:\n",
        "                 print(f\"Error during Lorenz simulation: {e}\")\n",
        "                 return {\"trajectory\": None, \"chaos_metrics\": {}, \"error\": str(e)}\n",
        "\n",
        "        else:\n",
        "            print(f\"  Unsupported chaotic system: {system}\")\n",
        "            return {\"trajectory\": None, \"chaos_metrics\": {}, \"error\": \"Unsupported system\"}\n",
        "\n",
        "\n",
        "\n",
        "    def sensitivity_analysis(self, system: str, params: Dict) -> Dict:\n",
        "        \"\"\"Analyzes sensitivity to initial conditions (placeholder).\"\"\"\n",
        "        # This would typically involve running multiple simulations with perturbed initial conditions\n",
        "        print(f\"Performing sensitivity analysis for {system} (placeholder)...\")\n",
        "        # Simulate divergence based on params (e.g., higher rho in Lorenz -> more sensitive)\n",
        "        simulated_divergence = 0.05 + params.get(\"rho\", 28.0) * 0.001\n",
        "        print(f\"  Estimated divergence: {simulated_divergence:.3f}\")\n",
        "        return {\"divergence_estimate\": simulated_divergence}\n",
        "\n",
        "\n",
        "class SparseLSTMModel(nn.Module):\n",
        "    \"\"\"LSTM Model with optional weight sparsity.\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, sparsity=0.7):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sparsity = sparsity\n",
        "        self._apply_sparsity()\n",
        "        print(f\"SparseLSTMModel initialized: Input={input_size}, Hidden={hidden_size}, Sparsity={sparsity}\")\n",
        "\n",
        "    def _apply_sparsity(self):\n",
        "        \"\"\"Applies sparsity mask to LSTM weights.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.lstm.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    mask = torch.rand_like(param) > self.sparsity\n",
        "                    param.data *= mask.float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the LSTM.\"\"\"\n",
        "        # Initialize hidden and cell states\n",
        "        # Ensure batch size matches input x.size(0)\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size, device=x.device)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size, device=x.device)\n",
        "        # Pass through LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        # Return the output of the last time step\n",
        "        return out[:, -1, :]\n",
        "\n",
        "\n",
        "class TemporalPrimeScalingModule:\n",
        "    \"\"\"Manages components for Temporal Prime Scaling forecasting.\"\"\"\n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        # Ensure required keys are in config with defaults\n",
        "        self.config.setdefault(\"prime_limit\", 100)\n",
        "        self.config.setdefault(\"memory_horizon\", 20)\n",
        "        self.config.setdefault(\"tau\", 5.0)\n",
        "        self.config.setdefault(\"use_deep_learning\", False)\n",
        "        self.config.setdefault(\"hidden_size\", 10)\n",
        "        self.config.setdefault(\"sparsity\", 0.7)\n",
        "        self.config.setdefault(\"hnn_size\", 100) # For Hopfield agent config\n",
        "\n",
        "        self.primes = self._generate_primes(self.config[\"prime_limit\"])\n",
        "        print(\"TemporalPrimeScalingModule initialized.\")\n",
        "\n",
        "    def _generate_primes(self, n): # Duplicated code\n",
        "        primes = []\n",
        "        is_prime = [True] * (n + 1)\n",
        "        if n >= 0: is_prime[0] = False\n",
        "        if n >= 1: is_prime[1] = False\n",
        "        for p in range(2, int(np.sqrt(n)) + 1):\n",
        "            if is_prime[p]:\n",
        "                for i in range(p * p, n + 1, p):\n",
        "                    is_prime[i] = False\n",
        "        for p in range(2, n + 1):\n",
        "            if is_prime[p]:\n",
        "                primes.append(p)\n",
        "        return primes if primes else [2, 3, 5]\n",
        "\n",
        "    def initialize_agents(self) -> List[DFSNAgent]:\n",
        "        \"\"\"Initializes the specialized agents required for TPS.\"\"\"\n",
        "        print(\"Initializing Temporal Prime Scaling Agents...\")\n",
        "        agents = [\n",
        "            TemporalPrimeAgent(\"TemporalAgent_TPS\", self.config, self.primes),\n",
        "            HopfieldAgent(\"HopfieldAgent_TPS\", self.config, self.primes),\n",
        "            FractalAgent(\"FractalAgent_TPS\", self.config, self.primes)\n",
        "        ]\n",
        "        print(\"TPS Agents initialized.\")\n",
        "        return agents\n",
        "\n",
        "class ZSGTodo:\n",
        "    \"\"\"Represents a task item within the ZSG framework.\"\"\"\n",
        "    def __init__(self, task_id: str, description: str, status: str, priority: float, mliw_step: str, data_payload: Dict):\n",
        "        self.task_id = task_id # Unique ID (e.g., \"T001\", \"QuantumTask_abc\")\n",
        "        self.description = description # E.g., \"Optimize PNS sampling\", \"Run Grover Search\"\n",
        "        self.status = status # \"Pending\", \"In Progress\", \"Completed\", \"Failed\"\n",
        "        self.priority = priority # Numerical priority (higher is more important)\n",
        "        self.mliw_step = mliw_step # Which MLIW phase (e.g., \"Analyze\", \"Modulate\", \"Test\", \"Generate\", \"Validate\")\n",
        "        self.data_payload = data_payload # Input data, parameters, or results needed/produced (dict)\n",
        "\n",
        "        # Additional fields for tracking, if needed\n",
        "        self.creation_time = time.time()\n",
        "        self.assigned_agent: Optional[str] = None\n",
        "        self.completion_time: Optional[float] = None\n",
        "\n",
        "    def to_json(self) -> Dict:\n",
        "        \"\"\"Serializes the ZSGTodo object into a JSON-compatible dictionary.\"\"\"\n",
        "        return {\n",
        "            \"task_id\": self.task_id,\n",
        "            \"description\": self.description,\n",
        "            \"status\": self.status,\n",
        "            \"priority\": self.priority,\n",
        "            \"mliw_step\": self.mliw_step,\n",
        "            \"data\": self.data_payload, # Keep payload under 'data' key for consistency\n",
        "            \"creation_time\": self.creation_time,\n",
        "            \"assigned_agent\": self.assigned_agent,\n",
        "            \"completion_time\": self.completion_time\n",
        "        }\n",
        "\n",
        "    def update_status(self, new_status: str, agent_name: Optional[str] = None):\n",
        "        \"\"\"Updates the status of the TODO item.\"\"\"\n",
        "        self.status = new_status\n",
        "        if agent_name:\n",
        "            self.assigned_agent = agent_name\n",
        "        if new_status in [\"Completed\", \"Failed\"]:\n",
        "            self.completion_time = time.time()\n",
        "        print(f\"TODO {self.task_id} status updated to {new_status}\" + (f\" by {agent_name}\" if agent_name else \"\"))\n",
        "\n",
        "# Real-Time Forecasting Manager (Adapted from prompt)\n",
        "class RealTimeForecastingManager:\n",
        "    \"\"\"Manager for real-time forecasting using Temporal Prime Scaling agents.\"\"\"\n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        # Initialize the TPS module which creates the agents\n",
        "        self.module = TemporalPrimeScalingModule(config)\n",
        "        self.agents = self.module.initialize_agents() # Get agents from module\n",
        "        # Create DFSN with these specific agents\n",
        "        self.flow_network = DynamicFlowStateNetwork(self.agents, task_complexity_threshold=13, max_agents=10)\n",
        "        self.memory = MemorySystem() # Use a shared memory system\n",
        "        # Assign agents for easier access\n",
        "        self.temporal_agent = self._find_agent(TemporalPrimeAgent)\n",
        "        self.hopfield_agent = self._find_agent(HopfieldAgent)\n",
        "        self.fractal_agent = self._find_agent(FractalAgent)\n",
        "        if not all([self.temporal_agent, self.hopfield_agent, self.fractal_agent]):\n",
        "            raise RuntimeError(\"Failed to initialize all required TPS agents.\")\n",
        "        print(\"RealTimeForecastingManager initialized.\")\n",
        "\n",
        "    def _find_agent(self, agent_class: type) -> Optional[DFSNAgent]:\n",
        "         \"\"\"Utility to find the first agent of a specific class.\"\"\"\n",
        "         for agent in self.agents:\n",
        "             if isinstance(agent, agent_class):\n",
        "                 return agent\n",
        "         return None\n",
        "\n",
        "    def process_real_time_data(self, time_series_stream: List[float], t: int, iterations: int = 80):\n",
        "        \"\"\"Processes a single time step of data using the TPS agents (synchronous).\"\"\"\n",
        "        if t < 0 or t >= len(time_series_stream):\n",
        "            print(f\"Error: Time index {t} out of bounds for time series length {len(time_series_stream)}.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\n--- Processing Real-Time Data at t={t} ---\")\n",
        "        current_complexity = 13.0 # Fixed complexity for this manager example\n",
        "        # Adjust flow states and scale agents based on complexity\n",
        "        self.flow_network.adjust_flow_states(current_complexity) # Let DFSN handle adjustments\n",
        "\n",
        "        # Extract current data point and relevant history window\n",
        "        x_t = time_series_stream[t]\n",
        "        # Use a history window relevant for agents (e.g., memory horizon, HNN input)\n",
        "        history_window = time_series_stream[max(0, t - self.config.get(\"memory_horizon\", 20)) : t+1] # Includes current x_t\n",
        "        hnn_input_window = time_series_stream[max(0, t - self.config.get(\"hnn_size\", 100)) : t+1] # Window for HNN/Fractal\n",
        "\n",
        "\n",
        "        # --- Agent Execution Sequence (Synchronous) ---\n",
        "\n",
        "        # 1. Temporal Prime Agent: Calculate internal state z_t, predict next value\n",
        "        # Parameters for temporal scaling task\n",
        "        temp_params = {\"beta\": 0.1, \"eta\": 0.05, \"lambda\": 0.2, \"delta\": 0.15, \"gamma\": 0.1}\n",
        "        temp_task = {\n",
        "            \"type\": \"temporal_scaling\", \"action\": \"temporal_scaling\",\n",
        "            **temp_params, \"t\": t, \"x_t\": x_t,\n",
        "            \"complexity\": 7.0 # Assign complexity\n",
        "        }\n",
        "        print(f\"Executing Temporal Agent Task...\")\n",
        "        temp_result_pkg = self.temporal_agent._execute_single_task_iteration(temp_task)\n",
        "        if \"error\" in temp_result_pkg: print(f\"  Error: {temp_result_pkg['error']}\")\n",
        "        temp_result = temp_result_pkg.get(\"result\", {})\n",
        "        z_t = temp_result.get(\"z_t\", 0.0)\n",
        "        pred = temp_result.get(\"pred\", x_t) # Default prediction is current value\n",
        "        error = temp_result.get(\"error\", 0.0)\n",
        "        # Get hidden state if DL was used\n",
        "        h_t_from_temporal = self.temporal_agent.lstm(torch.tensor([[x_t]], dtype=torch.float32)).detach().numpy().flatten() if self.temporal_agent.use_dl and self.temporal_agent.lstm else np.zeros(self.config[\"hidden_size\"])\n",
        "\n",
        "\n",
        "        # 2. Hopfield Agent: Update state based on context and temporal results\n",
        "        # Generate context vector (e.g., based on z_t or recent history)\n",
        "        context_vector = np.random.rand(self.config[\"hnn_size\"]) * z_t # Example context\n",
        "        hnn_task = {\n",
        "            \"type\": \"hnn_update\", \"action\": \"hnn_update\",\n",
        "            \"state\": np.random.choice([-1, 1], self.config[\"hnn_size\"]), # Needs previous state ideally\n",
        "            \"context_vector\": context_vector.tolist(),\n",
        "            \"zero_equilibrium\": 0.5, \"alpha\": 0.1, \"t\": t,\n",
        "            \"x_t\": hnn_input_window, # Pass relevant window\n",
        "            \"delta\": temp_params[\"delta\"], \"gamma\": temp_params[\"gamma\"],\n",
        "            \"h_t\": h_t_from_temporal.tolist(), # Use hidden state from temporal agent\n",
        "            \"use_dl\": self.config[\"use_deep_learning\"],\n",
        "            \"complexity\": 6.0\n",
        "        }\n",
        "        print(f\"Executing Hopfield Agent Task...\")\n",
        "        hopfield_result_pkg = self.hopfield_agent._execute_single_task_iteration(hnn_task)\n",
        "        if \"error\" in hopfield_result_pkg: print(f\"  Error: {hopfield_result_pkg['error']}\")\n",
        "        hopfield_result = hopfield_result_pkg.get(\"result\", {})\n",
        "        hnn_state = hopfield_result.get(\"hnn_state\", np.zeros(self.config[\"hnn_size\"]))\n",
        "\n",
        "\n",
        "        # 3. Fractal Agent: Generate fractal sequence based on HNN state and temporal context\n",
        "        fractal_task = {\n",
        "            \"type\": \"fractal_generate\", \"action\": \"fractal_generate\",\n",
        "            \"initial_c\": z_t + 0.1j, # Use temporal state as part of initial complex value\n",
        "            \"iterations\": iterations,\n",
        "            \"embeddings\": np.random.rand(self.config[\"hnn_size\"]), # Example embeddings\n",
        "            \"gamma_factor\": 0.05,\n",
        "            \"hnn_state\": hnn_state, # Use state from Hopfield agent\n",
        "            \"x_t\": hnn_input_window, # Pass relevant window\n",
        "            \"delta\": temp_params[\"delta\"], \"gamma\": temp_params[\"gamma\"],\n",
        "            \"h_t\": h_t_from_temporal.tolist(), # Use hidden state from temporal agent\n",
        "            \"use_dl\": self.config[\"use_deep_learning\"],\n",
        "            \"complexity\": 5.0\n",
        "        }\n",
        "        print(f\"Executing Fractal Agent Task...\")\n",
        "        fractal_result_pkg = self.fractal_agent._execute_single_task_iteration(fractal_task)\n",
        "        if \"error\" in fractal_result_pkg: print(f\"  Error: {fractal_result_pkg['error']}\")\n",
        "        fractal_result = fractal_result_pkg.get(\"result\", {})\n",
        "        fractal_sequence = fractal_result.get(\"fractal_sequence\", [])\n",
        "        fractal_dimension = fractal_result.get(\"fractal_dimension\", 0.0)\n",
        "\n",
        "        # --- Store and Return Combined Results ---\n",
        "        combined_results = {\n",
        "            \"time_step\": t,\n",
        "            \"input_value\": x_t,\n",
        "            \"temporal_state_z\": z_t,\n",
        "            \"prediction\": pred,\n",
        "            \"prediction_error\": error,\n",
        "            \"hopfield_state_final_norm\": float(np.linalg.norm(hnn_state)), # Example summary\n",
        "            \"fractal_dimension\": fractal_dimension,\n",
        "            # Optionally include full sequences if needed\n",
        "            # \"fractal_sequence\": fractal_sequence,\n",
        "        }\n",
        "\n",
        "        # Store results in memory\n",
        "        self.memory.store_episode(episode=1, iteration=t, results=combined_results) # Use t as iteration for simplicity\n",
        "\n",
        "        print(f\"--- Real-Time Processing Complete for t={t} ---\")\n",
        "        return combined_results\n",
        "\n",
        "\n",
        "\n",
        "class ZSGManager:\n",
        "    \"\"\"Orchestrates the entire ZeroSumGame Framework.\"\"\"\n",
        "    def __init__(self, complexity: float = 6.0, vocab_size: int = 1000):\n",
        "        print(\"Initializing ZSGManager...\")\n",
        "        self.complexity = complexity\n",
        "        self.math_module = PyZeroMathTorch()\n",
        "        self.f0z_algebra = F0ZAlgebra()\n",
        "\n",
        "        # Core systems - Initialize containers first\n",
        "        self.agents: List[DFSNAgent] = []\n",
        "        self.agent_registry: Dict[str, Agent] = {}\n",
        "        self.memory_system = MemorySystem()\n",
        "        self.resource_monitor = ResourceMonitor()\n",
        "        self.mliw = MLIW()\n",
        "        self.chaos_module = ChaosTheoryModule()\n",
        "        self.quantum_bridge = ZSGQuantumBridge(n_logical_qubits=4, n_physical_per_logical=5)\n",
        "\n",
        "        # Initialize components that depend on the containers but not specific agents yet\n",
        "        self.flow_state_network = DynamicFlowStateNetwork(self.agents, task_complexity_threshold=5.0, max_agents=15)\n",
        "        self.multi_agent_coordinator = MultiAgentCoordinator(self.agents)\n",
        "\n",
        "        # Initialize LLM interface Agent (doesn't go in self.agents by default)\n",
        "        #self.f0z_llm_agent = F0ZAgent(\"F0ZAgent_LLM\")\n",
        "        self.f0z_llm_agent = F0ZAgent()\n",
        "\n",
        "        # Initialize LLM for NLP parsing\n",
        "        try:\n",
        "             model_name = \"gpt2\"\n",
        "             device = 0 if torch.cuda.is_available() else -1\n",
        "             self.nlp_pipeline = pipeline(\"text-generation\", model=model_name, device=device, max_new_tokens=150)\n",
        "             print(f\"ZSGManager NLP pipeline initialized with {model_name}.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: Failed to load NLP pipeline for ZSGManager: {e}. Using basic parsing.\")\n",
        "             self.nlp_pipeline = None\n",
        "\n",
        "        self.episode_history = {}\n",
        "        self.is_active = False\n",
        "\n",
        "        # --- Initialize Agents SECTION ---\n",
        "        # Initialize QRL Agent first as it uses the bridge\n",
        "        self.qrl_agent = CuriosityQRLAgent(\"QRL_Agent_1\", self.quantum_bridge, vocab_size)\n",
        "        self._add_agent_instance(self.qrl_agent) # Add QRL agent to registry and agents list\n",
        "\n",
        "        # Populate initial *other* default agents (calls _add_agent_instance)\n",
        "        self._initialize_default_agents(vocab_size) # <<< MOVED EARLIER\n",
        "\n",
        "        # --- Initialize Components Dependent on Agents ---\n",
        "        # Now that agents exist, initialize components that need them\n",
        "        self.batch_processor = DynamicBatchModeProcessor(self)\n",
        "        # Science fair mode initialization now happens AFTER agents are created\n",
        "        try:\n",
        "            self.science_fair_mode = ZSGBatchModeScienceFair(self) # <<< MOVED HERE\n",
        "        except RuntimeError as e:\n",
        "             print(f\"Critical Error initializing Science Fair Mode: {e}. Check agent dependencies.\")\n",
        "             # Decide how to handle this - maybe disable science fair mode?\n",
        "             self.science_fair_mode = None # Disable if init fails\n",
        "\n",
        "        print(f\"ZSGManager initialized successfully with {len(self.agents)} agents.\")\n",
        "\n",
        "\n",
        "    def _initialize_default_agents(self, vocab_size):\n",
        "         \"\"\"Creates and adds the default set of specialized agents.\"\"\"\n",
        "         print(\"Initializing default ZSG agents...\")\n",
        "         # NOTE: QRL Agent is already added before this call in __init__\n",
        "         default_agents_to_add = [\n",
        "             PhysicsAgent(\"PhysicsAgent_1\"),\n",
        "             QuantumAgent(\"QuantumAgent_1\"),\n",
        "             MemoryAgent(\"MemoryAgent_1\"),\n",
        "             CollaborativeAgent(\"CollaborativeAgent_1\"),\n",
        "             TemporalPrimeAgent(\"TemporalPrimeAgent_1\"),\n",
        "             OrganicChemistryAgent(\"OrganicChemistryAgent_1\"),\n",
        "             MolecularBiologyAgent(\"MolecularBiologyAgent_1\"),\n",
        "             CreativeAgent(\"CreativeAgent_1\"),\n",
        "             InformationTheoryAgent(\"InformationTheoryAgent_1\"),\n",
        "             HypothesisAgent(\"HypothesisAgent_1\"),\n",
        "             DataScienceAgent(\"DataScienceAgent_1\"),\n",
        "             AstrophysicsAgent(\"AstrophysicsAgent_1\"),\n",
        "             RoboticsAgent(\"RoboticsAgent_1\"),\n",
        "             EnvironmentalScienceAgent(\"EnvironmentalScienceAgent_1\"),\n",
        "             MachineLearningAgent(\"MachineLearningAgent_1\"),\n",
        "             ValidationAgent(\"ValidationAgent_1\"),\n",
        "             FractalAgent(\"FractalAgent_1\"),\n",
        "             HopfieldAgent(\"HopfieldAgent_1\"),\n",
        "             LLaDATaskAgent(\"LLaDA_Agent_1\", self.quantum_bridge)\n",
        "         ]\n",
        "         for agent in default_agents_to_add:\n",
        "              # Check if agent already exists (e.g., if QRL was listed here too)\n",
        "              if agent.name not in self.agent_registry:\n",
        "                  self._add_agent_instance(agent)\n",
        "\n",
        "         # Link collaborative peers (this logic is now safe)\n",
        "         collab_agent = self.agent_registry.get(\"CollaborativeAgent_1\")\n",
        "         if collab_agent and isinstance(collab_agent, CollaborativeAgent):\n",
        "             collab_agent.set_agent_registry(self.agent_registry)\n",
        "             for agent_name in self.agent_registry:\n",
        "                  if agent_name != collab_agent.name:\n",
        "                      collab_agent.add_peer(agent_name)\n",
        "\n",
        "         # Provide bridge to QuantumAgent (this logic is now safe)\n",
        "         q_agent = self.agent_registry.get(\"QuantumAgent_1\")\n",
        "         if q_agent and isinstance(q_agent, QuantumAgent):\n",
        "              q_agent.set_bridge(self.quantum_bridge)\n",
        "\n",
        "         print(\"Default agents initialized and linked.\")\n",
        "\n",
        "\n",
        "    def _add_agent_instance(self, agent: Agent):\n",
        "         \"\"\"Adds an initialized agent instance to the manager.\"\"\"\n",
        "         if agent.name in self.agent_registry:\n",
        "              print(f\"Warning: Agent with name {agent.name} already exists. Skipping.\")\n",
        "              return\n",
        "         if isinstance(agent, DFSNAgent): # Only add DFSNAgents to lists managed by DFSN/Coordinator\n",
        "             self.agents.append(agent) # Add to the shared list\n",
        "         self.agent_registry[agent.name] = agent\n",
        "         # Update coordinator's view of agents (it uses the shared list reference)\n",
        "         self.multi_agent_coordinator.agent_states[agent.name] = {\"state\": \"idle\", \"load\": 0.0, \"engagement\": agent.get_engagement_state()}\n",
        "         # Update DFSN's view\n",
        "         self.flow_state_network.agent_states[agent.name] = agent.flow_state if hasattr(agent, 'flow_state') else 'idle'\n",
        "         # print(f\"Agent {agent.name} added to ZSGManager.\")\n",
        "\n",
        "\n",
        "    def activate(self, episode: int, iteration: int):\n",
        "        \"\"\"Activates the ZSG framework components for an episode.\"\"\"\n",
        "        if self.is_active:\n",
        "            print(\"ZSG Framework is already active.\")\n",
        "            return\n",
        "        print(f\"\\n--- Activating ZSG Framework ---\")\n",
        "        self.mliw.start_episode(episode, iteration)\n",
        "        # Start monitoring, enable dynamic states\n",
        "        self.resource_monitor.start()\n",
        "        self.flow_state_network.enable_dynamic_states()\n",
        "        # Perform initial resource allocation based on current complexity\n",
        "        self.calibrate_episode(f\"Activation for Episode {episode}\", self.complexity)\n",
        "        self.is_active = True\n",
        "        print(\"--- ZSG Framework Activated ---\")\n",
        "\n",
        "    def deactivate(self):\n",
        "        \"\"\"Deactivates the ZSG framework components.\"\"\"\n",
        "        if not self.is_active:\n",
        "            print(\"ZSG Framework is already inactive.\")\n",
        "            return\n",
        "        print(f\"\\n--- Deactivating ZSG Framework ---\")\n",
        "        self.resource_monitor.stop()\n",
        "        self.flow_state_network.disable_dynamic_states()\n",
        "        self.is_active = False\n",
        "        # Clear task queue? Reset agent states?\n",
        "        self.multi_agent_coordinator.task_queue = []\n",
        "        print(\"--- ZSG Framework Deactivated ---\")\n",
        "\n",
        "    def calibrate_episode(self, prompt: str, complexity: float):\n",
        "        \"\"\"Calibrates system parameters for a new episode or complexity level.\"\"\"\n",
        "        print(f\"Calibrating ZSG for complexity {complexity:.2f}. Prompt: '{prompt}'\")\n",
        "        self.complexity = complexity\n",
        "        # Adjust DFSN threshold based on overall complexity\n",
        "        self.flow_state_network.task_complexity_threshold = complexity * 0.8 # e.g., 80% of overall\n",
        "        print(f\"  DFSN Threshold set to: {self.flow_state_network.task_complexity_threshold:.2f}\")\n",
        "        # Pre-allocate resources\n",
        "        self.resource_monitor.pre_allocate(self.agents, complexity)\n",
        "        # Scale agents based on new complexity\n",
        "        self.flow_state_network.scale_agents(complexity) # Note: Scaling might add conceptual agents\n",
        "\n",
        "    def enforce_non_repeat(self, task: Dict, episode: int) -> Dict:\n",
        "        \"\"\"Checks if a similar task was done and modifies it slightly if needed.\"\"\"\n",
        "        # Simple check based on task type and key parameters\n",
        "        task_signature = f\"{task.get('type', '')}_{task.get('action', '')}\"\n",
        "        # Check history for similar signatures (this could be more sophisticated)\n",
        "        for past_episode, past_result in self.episode_history.items():\n",
        "             past_task = past_result.get(\"task_input\") # Assuming task is stored in result\n",
        "             if past_task:\n",
        "                  past_signature = f\"{past_task.get('type', '')}_{past_task.get('action', '')}\"\n",
        "                  if task_signature == past_signature:\n",
        "                       # Found similar task type/action, modify current task slightly\n",
        "                       print(f\"Non-repeat: Modifying task {task_signature} based on past execution in {past_episode}\")\n",
        "                       task['complexity'] = task.get('complexity', 5.0) * 1.05 # Slightly increase complexity\n",
        "                       # Modify a data parameter if possible\n",
        "                       if 'data' in task and isinstance(task['data'], dict):\n",
        "                            for key, val in task['data'].items():\n",
        "                                 if isinstance(val, (int, float)):\n",
        "                                      task['data'][key] = val * (1 + random.uniform(-0.05, 0.05)) # Perturb numeric data\n",
        "                                      break # Modify one parameter and stop\n",
        "                       # Mark task as modified\n",
        "                       task['modified_for_non_repeat'] = True\n",
        "                       return task # Return modified task\n",
        "        return task # Return original task if no similar one found\n",
        "\n",
        "    def process_task_with_zsg(self, task: Dict) -> Dict:\n",
        "        \"\"\"Main processing function for a single task within the ZSG framework.\"\"\"\n",
        "        if not self.is_active:\n",
        "             print(\"Warning: ZSG Framework is not active. Processing task with defaults.\")\n",
        "             # Fallback: Execute directly via coordinator without full framework features\n",
        "             agent = self.agent_registry.get(task.get(\"agent_name\")) # Requires agent name in task\n",
        "             if agent: return agent._execute_single_task_iteration(task)\n",
        "             else: return {\"error\": \"ZSG inactive and no specific agent found.\"}\n",
        "\n",
        "\n",
        "        episode, iteration = self.mliw.get_state()\n",
        "        print(f\"\\n=== ZSG Processing Task (E{episode}, I{iteration}) ===\")\n",
        "        print(f\"Input Task: Type='{task.get('type', 'N/A')}', Action='{task.get('action', 'N/A')}', Complexity={task.get('complexity', self.complexity):.1f}\")\n",
        "\n",
        "        # 1. Enforce Non-Repetition\n",
        "        task = self.enforce_non_repeat(task, episode)\n",
        "\n",
        "        # 2. Assign task via Coordinator\n",
        "        # The coordinator finds the right agent(s) and calls their _execute_single_task_iteration (which uses AIW internally)\n",
        "        result_package = self.multi_agent_coordinator.assign_tasks(task)\n",
        "\n",
        "        # 3. Update Resource Allocations (based on current agent states)\n",
        "        self.resource_monitor.update_allocations(self.agents)\n",
        "\n",
        "        # 4. Store result in Memory System\n",
        "        self.memory_system.store_episode(episode, iteration, {\"task_input\": task, \"result_package\": result_package})\n",
        "        # Store in history for non-repeat check\n",
        "        self.episode_history[f\"E{episode}_I{iteration}\"] = {\"task_input\": task, \"result_package\": result_package}\n",
        "        if len(self.episode_history) > 500: # Limit history size\n",
        "             self.episode_history.pop(list(self.episode_history.keys())[0])\n",
        "\n",
        "\n",
        "        # 5. Advance MLIW iteration\n",
        "        self.mliw.next_iteration()\n",
        "\n",
        "        print(f\"=== ZSG Task Processing Complete ===\\n\")\n",
        "        return result_package\n",
        "\n",
        "\n",
        "    def process_nlp_command(self, command: str) -> Dict:\n",
        "         \"\"\"Parses an NLP command and executes the corresponding ZSG task.\"\"\"\n",
        "         if not self.is_active:\n",
        "             self.activate(1, 1) # Auto-activate if command received while dormant\n",
        "\n",
        "         print(f\"\\n--- Processing NLP Command: '{command}' ---\")\n",
        "         # Use LLM for parsing if available, otherwise use simple regex/keyword matching\n",
        "         task_dict = self.nlp_parse(command)\n",
        "\n",
        "         result = {}\n",
        "         explanation = \"Could not generate explanation.\"\n",
        "\n",
        "         if task_dict.get(\"type\") == \"unknown\":\n",
        "              result = {\"error\": \"Could not understand command.\"}\n",
        "         elif task_dict.get(\"type\") == \"batch_science_fair\":\n",
        "             # Trigger science fair mode\n",
        "              experiments = task_dict.get(\"experiments\", [\"Default Experiment 1\", \"Default Experiment 2\"])\n",
        "              sf_results = self.science_fair_mode.run_science_fair(experiments)\n",
        "              result = {\"science_fair_summary\": sf_results.get(\"final_summary\", \"Error in summary\")}\n",
        "         elif task_dict.get(\"type\") == \"chaos\":\n",
        "              result = self.chaos_module.simulate(task_dict[\"system\"], task_dict[\"params\"])\n",
        "              # Optionally trigger DFSN adjustment based on chaos result\n",
        "              if \"chaos_metrics\" in result:\n",
        "                   self.flow_state_network.handle_chaos(result[\"chaos_metrics\"])\n",
        "         elif task_dict.get(\"type\") == \"quantum_gate\":\n",
        "              try:\n",
        "                  gate_func = getattr(self.quantum_bridge.simulator, f\"{task_dict['gate'].lower()}_gate\")\n",
        "                  gate_func(*task_dict['qubits']) # Apply gate\n",
        "                  result = {\"result\": {\"quantum_state_vector\": self.quantum_bridge.simulator.state.tolist()}, \"agent\": \"QuantumBridge\"}\n",
        "              except Exception as e:\n",
        "                   result = {\"error\": f\"Failed to apply quantum gate: {e}\"}\n",
        "         elif task_dict.get(\"type\") == \"llm_inference\":\n",
        "               result = self.f0z_llm_agent._execute_single_task_iteration(task_dict)\n",
        "         elif task_dict.get(\"type\") == \"f0z_simulation\":\n",
        "              result = self.f0z_llm_agent._execute_single_task_iteration(task_dict)\n",
        "         else:\n",
        "              # Assign as a standard task to the coordinator\n",
        "              self.multi_agent_coordinator.add_task(task_dict, priority=int(task_dict.get(\"complexity\", 5)))\n",
        "              result = self.multi_agent_coordinator.execute_next_task()\n",
        "              if result is None: result = {\"status\": \"No task executed (queue might be empty or assignment failed)\"}\n",
        "\n",
        "\n",
        "         # Generate explanation using LLM if available\n",
        "         if self.nlp_pipeline:\n",
        "             try:\n",
        "                  explanation_prompt = f\"Explain the following ZSG framework result for command '{command}'. Result: {json.dumps(result, default=lambda o: '<non-serializable>')[:200]}...\"\n",
        "                  explanation_result = self.nlp_pipeline(explanation_prompt, max_length=100)\n",
        "                  explanation = explanation_result[0]['generated_text']\n",
        "                  # Clean explanation\n",
        "                  if explanation.startswith(explanation_prompt):\n",
        "                       explanation = explanation[len(explanation_prompt):].strip()\n",
        "             except Exception as e:\n",
        "                  explanation = f\"Error generating explanation: {e}\"\n",
        "         else:\n",
        "             explanation = f\"NLP explanation disabled. Task Type: {task_dict.get('type')}, Result Status: {'error' if 'error' in result else 'success'}\"\n",
        "\n",
        "         print(f\"--- NLP Command Processing Complete ---\")\n",
        "         return {\"result\": result, \"explanation\": explanation}\n",
        "\n",
        "\n",
        "    def nlp_parse(self, command: str) -> Dict:\n",
        "         \"\"\"Parses natural language command into a task dictionary (improved).\"\"\"\n",
        "         cmd = command.lower().strip()\n",
        "         task = {\"type\": \"unknown\", \"complexity\": 5.0, \"action\": None, \"data\": {}} # Default structure\n",
        "\n",
        "         # Keyword/Regex based parsing (example rules)\n",
        "         if re.search(r\"(run|start) science fair\", cmd):\n",
        "              task[\"type\"] = \"batch_science_fair\"\n",
        "              task[\"complexity\"] = 10.0\n",
        "              # Extract experiment descriptions if possible\n",
        "              exps = re.findall(r'experiments? \"?(.*?)\"? (and|with|featuring) \"?(.*?)\"?', cmd)\n",
        "              if exps: task[\"experiments\"] = [e.strip() for group in exps for e in group if e not in ['and', 'with', 'featuring'] and e]\n",
        "              elif \"experiments\" in cmd: task[\"experiments\"] = [s.strip() for s in cmd.split(\"experiments\")[-1].split(',') if s.strip()]\n",
        "              else: task[\"experiments\"] = [\"Quantum Entanglement Effects\", \"F0Z Stability Analysis\"]\n",
        "\n",
        "         elif re.search(r\"simulate lorenz\", cmd):\n",
        "              task[\"type\"] = \"chaos\"\n",
        "              task[\"system\"] = \"lorenz\"\n",
        "              task[\"params\"] = {\"t_max\": 15.0, \"steps\": 300} # Default params\n",
        "              # Try extracting params\n",
        "              if \"rho=\" in cmd: task[\"params\"][\"rho\"] = float(cmd.split(\"rho=\")[-1].split()[0])\n",
        "              task[\"complexity\"] = 7.0\n",
        "\n",
        "         elif re.search(r\"(apply|run) (h|hadamard|x|y|z|rz|ry|cnot|cx|cz) gate\", cmd):\n",
        "              task[\"type\"] = \"quantum_gate\"\n",
        "              gate_match = re.search(r\"(h|hadamard|x|y|z|rz|ry|cnot|cx|cz) gate\", cmd)\n",
        "              task[\"gate\"] = gate_match.group(1).upper() if gate_match else \"H\"\n",
        "              if task[\"gate\"] == \"HADAMARD\": task[\"gate\"] = \"H\"\n",
        "              if task[\"gate\"] == \"CX\": task[\"gate\"] = \"CNOT\"\n",
        "\n",
        "              qubit_matches = re.findall(r\"qubit (\\d+)\", cmd)\n",
        "              qubits = [int(q) for q in qubit_matches] if qubit_matches else [0] # Default qubit 0\n",
        "              task[\"qubits\"] = qubits\n",
        "              # Adjust structure for specific gates\n",
        "              if task[\"gate\"] in [\"RZ\", \"RY\"]:\n",
        "                    task[\"qubit\"] = qubits[0] # Single target\n",
        "                    angle_match = re.search(r\"angle ([0-9.]+)\", cmd)\n",
        "                    task[\"angle\"] = float(angle_match.group(1)) if angle_match else np.pi/2\n",
        "              elif task[\"gate\"] in [\"CNOT\", \"CZ\"]:\n",
        "                    if len(qubits) < 2: qubits = [0, 1] # Default control/target\n",
        "                    task[\"control\"] = qubits[0]\n",
        "                    task[\"target\"] = qubits[1]\n",
        "              task[\"complexity\"] = 4.0\n",
        "\n",
        "         elif re.search(r\"ask llm|infer|explain f0z\", cmd):\n",
        "              task[\"type\"] = \"llm_inference\"\n",
        "              task[\"action\"] = \"infer\"\n",
        "              query_match = re.search(r\"(?:ask llm about|infer|explain f0z) (.*)\", command, re.IGNORECASE) # Capture query after keyword\n",
        "              task[\"query\"] = query_match.group(1).strip() if query_match else \"What is the Formula for Zero?\"\n",
        "              task[\"complexity\"] = 3.0\n",
        "\n",
        "         elif re.search(r\"simulate f0z reaction\", cmd):\n",
        "               task[\"type\"] = \"f0z_simulation\"\n",
        "               task[\"action\"] = \"simulate_f0z_reaction\"\n",
        "               dpd_match = re.search(r\"d/pd ratio of ([0-9.]+)\", cmd)\n",
        "               task[\"D_Pd_ratio\"] = float(dpd_match.group(1)) if dpd_match else 0.8\n",
        "               task[\"complexity\"] = 6.0\n",
        "\n",
        "         elif re.search(r\"train ml model\", cmd):\n",
        "              task[\"type\"] = \"machine_learning\"\n",
        "              task[\"action\"] = \"train\"\n",
        "              # Requires data source definition - simplified: use dummy data\n",
        "              task[\"X\"] = [[1,1], [2,1], [3,2], [4,2]]\n",
        "              task[\"Y\"] = [[2], [3], [5], [6]]\n",
        "              task[\"complexity\"] = 8.0\n",
        "\n",
        "         elif re.search(r\"predict with ml model\", cmd):\n",
        "               task[\"type\"] = \"machine_learning\"\n",
        "               task[\"action\"] = \"predict\"\n",
        "               task[\"X\"] = [[5,3], [6,3]]\n",
        "               task[\"complexity\"] = 3.0\n",
        "\n",
        "         elif re.search(r\"validate last result\", cmd):\n",
        "               task[\"type\"] = \"validation\"\n",
        "               task[\"action\"] = \"verify_result\"\n",
        "               # Requires mechanism to get last result and task data\n",
        "               last_hist_key = list(self.episode_history.keys())[-1] if self.episode_history else None\n",
        "               if last_hist_key and \"result_package\" in self.episode_history[last_hist_key] and \"task_input\" in self.episode_history[last_hist_key]:\n",
        "                    task[\"original_result\"] = self.episode_history[last_hist_key][\"result_package\"].get(\"result\")\n",
        "                    task[\"task_data\"] = self.episode_history[last_hist_key][\"task_input\"]\n",
        "                    task[\"complexity\"] = 4.0\n",
        "               else:\n",
        "                    task[\"error\"] = \"Cannot validate: No previous result found in history.\"\n",
        "                    task[\"type\"] = \"error_task\"\n",
        "\n",
        "\n",
        "         # Add more parsing rules here...\n",
        "\n",
        "         print(f\"NLP Parsed Task: {task}\")\n",
        "         return task\n",
        "\n",
        "# --- CLI Shell ---\n",
        "class ScienceFairShell:\n",
        "    \"\"\"Command Line Interface for interacting with the ZSG Manager.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.manager = ZSGManager()\n",
        "        # self.session = PromptSession(history=None) # Keep using input for now\n",
        "        print(\"\\n--- ZSG Science Fair Shell Initialized ---\")\n",
        "        print(\"Type NLP commands (e.g., 'simulate lorenz', 'apply h gate qubit 0', 'ask llm about f0z', 'run science fair') or 'exit'.\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Starts the interactive shell.\"\"\"\n",
        "        self.manager.activate(episode=1, iteration=1)\n",
        "\n",
        "        while True:\n",
        "            try: # Main try block for input and processing\n",
        "                command = input(\"Science Fair (basic)> \")\n",
        "                command = command.strip()\n",
        "\n",
        "                if not command: continue\n",
        "                if command.lower() == \"exit\":\n",
        "                    print(\"Exiting Science Fair Shell.\")\n",
        "                    break # Correctly exits the loop\n",
        "\n",
        "                # Process the command using the manager's NLP handler\n",
        "                output = self.manager.process_nlp_command(command)\n",
        "\n",
        "                # Print results and explanation nicely\n",
        "                print(\"\\n--- Result ---\")\n",
        "                try: # Inner try specifically for JSON dumping\n",
        "                    print(json.dumps(output.get('result', {}), indent=2, default=lambda o: '<non-serializable>'))\n",
        "                except Exception as json_e:\n",
        "                     print(f\"Could not format result as JSON: {json_e}\")\n",
        "                     print(output.get('result', {})) # Print raw result if JSON fails\n",
        "\n",
        "                print(\"\\n--- Explanation ---\")\n",
        "                print(output.get('explanation', \"No explanation available.\"))\n",
        "                print(\"-\" * 17 + \"\\n\") # Separator\n",
        "\n",
        "\n",
        "            except EOFError:\n",
        "                 print(\"\\nEOF detected (input stream ended). Exiting.\")\n",
        "                 break # Exit loop\n",
        "            except KeyboardInterrupt:\n",
        "                 print(\"\\nKeyboard interrupt detected. Exiting.\")\n",
        "                 break # Exit loop\n",
        "            except Exception as e:\n",
        "                 print(f\"\\nShell Error: An unexpected error occurred during command processing: {e}\")\n",
        "                 # Optionally add more detailed logging or decide whether to continue/break\n",
        "                 # traceback.print_exc() # For debugging\n",
        "\n",
        "\n",
        "        # Deactivate manager AFTER the while loop finishes\n",
        "        self.manager.deactivate()\n",
        "\n",
        "# --- CLI Shell ---\n",
        "# (ScienceFairShell class definition - use corrected version from previous step)\n",
        "# ...\n",
        "\n",
        "# --- Example Usage & Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"*\"*70); print(\" ZSG Framework - Science Fair Edition (Qiskit Integrated) \"); print(\"*\"*70)\n",
        "    try:\n",
        "        shell = ScienceFairShell()\n",
        "        shell.run()\n",
        "    except ImportError as e:\n",
        "         print(f\"\\nCRITICAL ERROR DURING STARTUP: {e}\")\n",
        "         print(\"Please ensure all required libraries, including Qiskit and its dependencies, are installed.\")\n",
        "    except Exception as e:\n",
        "         print(f\"\\nUNEXPECTED ERROR DURING STARTUP OR EXECUTION: {e}\")\n",
        "         import traceback\n",
        "         traceback.print_exc()\n",
        "    finally:\n",
        "        print(\"\\nZSG Framework execution finished.\")\n",
        "\n",
        "# --- End of Merged File ---\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SghafrEW19m",
        "outputId": "efb5bbe4-0a3c-4d8b-c1d5-16c23b902efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************************************************\n",
            " ZSG Framework - Science Fair Edition (Qiskit Integrated) \n",
            "**********************************************************************\n",
            "Initializing ZSGManager...\n",
            "MemorySystem initialized.\n",
            "ResourceMonitor initialized. Initial batch size: 32\n",
            "MLIW Controller initialized.\n",
            "ChaosTheoryModule initialized.\n",
            "Setting up Qiskit noise model (Depolarizing p=0.01)...\n",
            "Qiskit AerSimulator configured with noise model.\n",
            "ZSGQuantumBridge initialized with Qiskit backend: 20 qubits.\n",
            "DFSN initialized. Threshold: 5.0, Max Agents: 15\n",
            "Coordinator domain map built.\n",
            "MultiAgentCoordinator initialized with 0 agents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F0ZAgent LLM interface initialized with: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZSGManager NLP pipeline initialized with gpt2.\n",
            "ZSGAgent QRL_Agent_1 initialized.\n",
            "CuriosityQRLAgent QRL_Agent_1 initialized (Qiskit). Policy State Size: 4, Actions: 1000\n",
            "Initializing default ZSG agents...\n",
            "ZSGAgent PhysicsAgent_1 initialized.\n",
            "PhysicsAgent PhysicsAgent_1 initialized.\n",
            "ZSGAgent QuantumAgent_1 initialized.\n",
            "QuantumAgent QuantumAgent_1 initialized.\n",
            "ZSGAgent MemoryAgent_1 initialized.\n",
            "MemorySystem initialized.\n",
            "MemoryAgent MemoryAgent_1 initialized.\n",
            "ZSGAgent CollaborativeAgent_1 initialized.\n",
            "CollaborativeAgent CollaborativeAgent_1 initialized.\n",
            "ZSGAgent TemporalPrimeAgent_1 initialized.\n",
            "TemporalPrimeAgent TemporalPrimeAgent_1 initialized (No DL). Primes up to 100.\n",
            "ZSGAgent OrganicChemistryAgent_1 initialized.\n",
            "OrganicChemistryAgent OrganicChemistryAgent_1 initialized.\n",
            "ZSGAgent MolecularBiologyAgent_1 initialized.\n",
            "MolecularBiologyAgent MolecularBiologyAgent_1 initialized.\n",
            "ZSGAgent CreativeAgent_1 initialized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CreativeAgent CreativeAgent_1 initialized with gpt2 pipeline.\n",
            "ZSGAgent InformationTheoryAgent_1 initialized.\n",
            "InformationTheoryAgent InformationTheoryAgent_1 initialized.\n",
            "ZSGAgent HypothesisAgent_1 initialized.\n",
            "HypothesisAgent HypothesisAgent_1 initialized.\n",
            "ZSGAgent DataScienceAgent_1 initialized.\n",
            "DataScienceAgent DataScienceAgent_1 initialized.\n",
            "ZSGAgent AstrophysicsAgent_1 initialized.\n",
            "AstrophysicsAgent AstrophysicsAgent_1 initialized.\n",
            "ZSGAgent RoboticsAgent_1 initialized.\n",
            "RoboticsAgent RoboticsAgent_1 initialized.\n",
            "ZSGAgent EnvironmentalScienceAgent_1 initialized.\n",
            "EnvironmentalScienceAgent EnvironmentalScienceAgent_1 initialized.\n",
            "ZSGAgent MachineLearningAgent_1 initialized.\n",
            "MachineLearningAgent MachineLearningAgent_1 initialized.\n",
            "ZSGAgent ValidationAgent_1 initialized.\n",
            "ValidationAgent ValidationAgent_1 initialized.\n",
            "ZSGAgent FractalAgent_1 initialized.\n",
            "FractalAgent FractalAgent_1 initialized.\n",
            "ZSGAgent HopfieldAgent_1 initialized.\n",
            "  Applied sparsity (0.7) to Hopfield weights.\n",
            "HopfieldAgent HopfieldAgent_1 initialized (Size: 100, Sparsity: 0.7).\n",
            "ZSGAgent LLaDA_Agent_1 initialized.\n",
            "LLaDA Model Placeholder Initialized.\n",
            "LLaDATaskAgent LLaDA_Agent_1 initialized.\n",
            "CollaborativeAgent_1 added peer: QRL_Agent_1\n",
            "CollaborativeAgent_1 added peer: PhysicsAgent_1\n",
            "CollaborativeAgent_1 added peer: QuantumAgent_1\n",
            "CollaborativeAgent_1 added peer: MemoryAgent_1\n",
            "CollaborativeAgent_1 added peer: TemporalPrimeAgent_1\n",
            "CollaborativeAgent_1 added peer: OrganicChemistryAgent_1\n",
            "CollaborativeAgent_1 added peer: MolecularBiologyAgent_1\n",
            "CollaborativeAgent_1 added peer: CreativeAgent_1\n",
            "CollaborativeAgent_1 added peer: InformationTheoryAgent_1\n",
            "CollaborativeAgent_1 added peer: HypothesisAgent_1\n",
            "CollaborativeAgent_1 added peer: DataScienceAgent_1\n",
            "CollaborativeAgent_1 added peer: AstrophysicsAgent_1\n",
            "CollaborativeAgent_1 added peer: RoboticsAgent_1\n",
            "CollaborativeAgent_1 added peer: EnvironmentalScienceAgent_1\n",
            "CollaborativeAgent_1 added peer: MachineLearningAgent_1\n",
            "CollaborativeAgent_1 added peer: ValidationAgent_1\n",
            "CollaborativeAgent_1 added peer: FractalAgent_1\n",
            "CollaborativeAgent_1 added peer: HopfieldAgent_1\n",
            "CollaborativeAgent_1 added peer: LLaDA_Agent_1\n",
            "QuantumAgent_1 received quantum bridge.\n",
            "Default agents initialized and linked.\n",
            "DynamicBatchModeProcessor initialized.\n",
            "DynamicBatchModeProcessor initialized.\n",
            "ZSGBatchModeScienceFair initialized.\n",
            "ZSGManager initialized successfully with 20 agents.\n",
            "\n",
            "--- ZSG Science Fair Shell Initialized ---\n",
            "Type NLP commands (e.g., 'simulate lorenz', 'apply h gate qubit 0', 'ask llm about f0z', 'run science fair') or 'exit'.\n",
            "\n",
            "--- Activating ZSG Framework ---\n",
            "MLIW starting Episode 1, Iteration 1\n",
            "Resource monitoring started (simulated).\n",
            "DFSN enabled for dynamic state adjustments.\n",
            "Calibrating ZSG for complexity 6.00. Prompt: 'Activation for Episode 1'\n",
            "  DFSN Threshold set to: 4.80\n",
            "Pre-allocated resources: ~5.0% CPU, ~5.0% Mem per agent. Total: 100.0% CPU, 100.0% Mem\n",
            "DFSN Scaling Check: Complexity=6.00, TargetActive=4, CurrentTotal=20, CurrentActive=0\n",
            "--- ZSG Framework Activated ---\n",
            "Science Fair (basic)> run science fair\n",
            "\n",
            "--- Processing NLP Command: 'run science fair' ---\n",
            "NLP Parsed Task: {'type': 'batch_science_fair', 'complexity': 10.0, 'action': None, 'data': {}, 'experiments': ['Quantum Entanglement Effects', 'F0Z Stability Analysis']}\n",
            "\n",
            "--- Running ZSG Science Fair with 2 experiments ---\n",
            "Adding experiments to batch...\n",
            "Added task batch_7b24ba55 to batch. Batch size: 1\n",
            "Added task batch_f1ea9460 to batch. Batch size: 2\n",
            "Processing batch of 2 tasks...\n",
            "  Processing batch task: batch_7b24ba55 (Experiment 0: Quantum Entangle...)\n",
            "TODO batch_7b24ba55 status updated to InProgress by BatchProcessor\n",
            "\n",
            "=== ZSG Processing Task (E1, I1) ===\n",
            "Input Task: Type='science_fair_experiment', Action='BatchProcess', Complexity=5.6\n",
            "Assigning task 'science_fair_experiment' to agent: CollaborativeAgent_1 (Load: 0.00)\n",
            "CollaborativeAgent_1 running science fair experiment: Experiment 0: Quantum Entanglement Effects\n",
            "MLIW advanced to Episode 1, Iteration 2\n",
            "=== ZSG Task Processing Complete ===\n",
            "\n",
            "TODO batch_7b24ba55 status updated to Completed\n",
            "  Processing batch task: batch_f1ea9460 (Experiment 1: F0Z Stability An...)\n",
            "TODO batch_f1ea9460 status updated to InProgress by BatchProcessor\n",
            "\n",
            "=== ZSG Processing Task (E1, I2) ===\n",
            "Input Task: Type='science_fair_experiment', Action='BatchProcess', Complexity=4.4\n",
            "Non-repeat: Modifying task science_fair_experiment_BatchProcess based on past execution in E1_I1\n",
            "Assigning task 'science_fair_experiment' to agent: CollaborativeAgent_1 (Load: 0.00)\n",
            "CollaborativeAgent_1 running science fair experiment: Experiment 1: F0Z Stability Analysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLIW advanced to Episode 1, Iteration 3\n",
            "=== ZSG Task Processing Complete ===\n",
            "\n",
            "TODO batch_f1ea9460 status updated to Completed\n",
            "Batch processing complete. Processed 2 tasks.\n",
            "\n",
            "Forecasting future trends (Horizon: 5)...\n",
            "TemporalPrimeAgent_1 forecast: Avg=0.800, Horizon=5, Forecast=4.000\n",
            "Forecast Result: 4.0003437995910645\n",
            "\n",
            "Combining batch results and forecast...\n",
            "Warning: CollaborativeAgent_1 using base DFSNAgent _execute_single_task_iteration. Task type 'collaboration' likely not handled.\n",
            "Combination complete.\n",
            "--- ZSG Science Fair Run Complete ---\n",
            "--- NLP Command Processing Complete ---\n",
            "\n",
            "--- Result ---\n",
            "{\n",
            "  \"science_fair_summary\": \"Base execution for collaboration\"\n",
            "}\n",
            "\n",
            "--- Explanation ---\n",
            "...\n",
            "\n",
            "Code\n",
            "\n",
            "C# 6.6.NET CLR 8\n",
            "\n",
            "A simple example of using a C# compiler to generate code by compiling a binary package.\n",
            "\n",
            "Code\n",
            "\n",
            "import com.maggart.microsoft.core.opengl.compiler.Properties; public class ICompilerTest { public int TestNum: int; public virtual void Run () { super(TestNum); System.out.println(\"hello world\".~(TestNum)); } }\n",
            "\n",
            "This test program generates the following C# code. Please note:\n",
            "\n",
            "A class with the following CXX parameters:\n",
            "\n",
            "void Test_Num() { }\n",
            "\n",
            "I used the following code:\n",
            "\n",
            "class\n",
            "-----------------\n",
            "\n",
            "Science Fair (basic)> run science fair\n",
            "\n",
            "--- Processing NLP Command: 'run science fair' ---\n",
            "NLP Parsed Task: {'type': 'batch_science_fair', 'complexity': 10.0, 'action': None, 'data': {}, 'experiments': ['Quantum Entanglement Effects', 'F0Z Stability Analysis']}\n",
            "\n",
            "--- Running ZSG Science Fair with 2 experiments ---\n",
            "Adding experiments to batch...\n",
            "Added task batch_7b24ba55 to batch. Batch size: 1\n",
            "Added task batch_f1ea9460 to batch. Batch size: 2\n",
            "Processing batch of 2 tasks...\n",
            "  Processing batch task: batch_7b24ba55 (Experiment 0: Quantum Entangle...)\n",
            "TODO batch_7b24ba55 status updated to InProgress by BatchProcessor\n",
            "\n",
            "=== ZSG Processing Task (E1, I3) ===\n",
            "Input Task: Type='science_fair_experiment', Action='BatchProcess', Complexity=5.6\n",
            "Non-repeat: Modifying task science_fair_experiment_BatchProcess based on past execution in E1_I1\n",
            "Assigning task 'science_fair_experiment' to agent: CollaborativeAgent_1 (Load: 0.00)\n",
            "CollaborativeAgent_1 running science fair experiment: Experiment 0: Quantum Entanglement Effects\n",
            "MLIW advanced to Episode 1, Iteration 4\n",
            "=== ZSG Task Processing Complete ===\n",
            "\n",
            "TODO batch_7b24ba55 status updated to Completed\n",
            "  Processing batch task: batch_f1ea9460 (Experiment 1: F0Z Stability An...)\n",
            "TODO batch_f1ea9460 status updated to InProgress by BatchProcessor\n",
            "\n",
            "=== ZSG Processing Task (E1, I4) ===\n",
            "Input Task: Type='science_fair_experiment', Action='BatchProcess', Complexity=4.4\n",
            "Non-repeat: Modifying task science_fair_experiment_BatchProcess based on past execution in E1_I1\n",
            "Assigning task 'science_fair_experiment' to agent: CollaborativeAgent_1 (Load: 0.00)\n",
            "CollaborativeAgent_1 running science fair experiment: Experiment 1: F0Z Stability Analysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLIW advanced to Episode 1, Iteration 5\n",
            "=== ZSG Task Processing Complete ===\n",
            "\n",
            "TODO batch_f1ea9460 status updated to Completed\n",
            "Batch processing complete. Processed 2 tasks.\n",
            "\n",
            "Forecasting future trends (Horizon: 5)...\n",
            "TemporalPrimeAgent_1 forecast: Avg=0.762, Horizon=5, Forecast=3.808\n",
            "Forecast Result: 3.808239221572876\n",
            "\n",
            "Combining batch results and forecast...\n",
            "Warning: CollaborativeAgent_1 using base DFSNAgent _execute_single_task_iteration. Task type 'collaboration' likely not handled.\n",
            "Combination complete.\n",
            "--- ZSG Science Fair Run Complete ---\n",
            "--- NLP Command Processing Complete ---\n",
            "\n",
            "--- Result ---\n",
            "{\n",
            "  \"science_fair_summary\": \"Base execution for collaboration\"\n",
            "}\n",
            "\n",
            "--- Explanation ---\n",
            "Example\n",
            "\n",
            "$ get github.com/myjak/zsg.framework/repos/github.com.myjak/zsg.base $ wget https://github.com/myjak/zsg.framework/repos/github.com.myjak/zsg-base/master $ curl https://get.zsg.com/releases/download/latest $ cd https://get.zsg.com/releases/download/latest $ git clone $ git://github.com/myjak/zsg.framework/repos/github.com.myjak/zsg.base.git $ cd zsg $ ls -l $ z\n",
            "-----------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTDuUrMJlUrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}